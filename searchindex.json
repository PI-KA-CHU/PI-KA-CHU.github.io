{"categories":[{"title":"it","uri":"https://PI-KA-CHU.github.io/categories/it/"},{"title":"java","uri":"https://PI-KA-CHU.github.io/categories/java/"},{"title":"spring","uri":"https://PI-KA-CHU.github.io/categories/spring/"},{"title":"中间件","uri":"https://PI-KA-CHU.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"title":"其他","uri":"https://PI-KA-CHU.github.io/categories/%E5%85%B6%E4%BB%96/"},{"title":"基础理论","uri":"https://PI-KA-CHU.github.io/categories/%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA/"},{"title":"开发运维","uri":"https://PI-KA-CHU.github.io/categories/%E5%BC%80%E5%8F%91%E8%BF%90%E7%BB%B4/"},{"title":"数据库","uri":"https://PI-KA-CHU.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"title":"算法","uri":"https://PI-KA-CHU.github.io/categories/%E7%AE%97%E6%B3%95/"}],"posts":[{"content":"hugo + github编写个人博客  https://blog.csdn.net/qq_41136216/article/details/112674327  生成流程如下：\n  下载hugo，并配置环境变量\n mac下载通过brew install hugo命令下载，如果失败，则直接到Hugo github下载extend版本的包 下载完配置环境变量，mac在/etc/paths.d文件夹下添加文件并输入下载的hugo路径 source [新建文件名]会有权限问题，需要su - root在执行该命令    hugo new site myNewSite 创建新项目\n  在/content/posts 文件夹中编写md文件\n  在/themes 文件夹中存入主题文件，并复制/themes/xxx/exampleSite/config.yaml 文件到/下\n  在根目录下执行 hugo ，/public 文件夹中会生成静态文件，将该文件上传到我们在github中创建的.github.io项目中，即可在打开博客。\n  本地打开：\n 执行命令根目录下执行 hugo server -D 进入：localhost:1313    配置记录：\n 头部加入：draft = true可以隐藏该md，即不被hugo构建  github + mac 配置个人密钥  在github的以下位置，生产token  mac搜索Keychain，进入以下界面，并搜索github  进入相关的密钥界面，在以下红框内填入第一步生成的密钥并保存即可  ","id":0,"section":"posts","summary":"hugo + github编写个人博客 https://blog.csdn.net/qq_41136216/article/details/112674327 生成流程如下： 下载hugo，并配置环境变量 mac下载通过brew install hugo命令下载，如果失败，则直接到Hugo","tags":["开发工具"],"title":"hugo + github编写个人博客","uri":"https://PI-KA-CHU.github.io/2022/05/hugo--github%E7%BC%96%E5%86%99%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","year":"2022"},{"content":"  https://www.pdai.tech/md/db/sql-mysql/sql-mysql-execute.html  bin log三种模式的区别  https://blog.csdn.net/keda8997110/article/details/50895171   row：记录具体的行被修改的内容。优点是记录清晰，复制完整；缺点是日志量较大。 statement：记录的是执行的SQL语句。优点是日志量少；缺点是对于某些特定的函数（sleep()、last_insert_id())，可能由于版本问题导致复制失败。 mixed：混合模式，对于正常的情况采用statement模式，对于statement模式无法复制的操作则采用row模式。  ","id":1,"section":"posts","summary":"https://www.pdai.tech/md/db/sql-mysql/sql-mysql-execute.html bin log三种模式的区别 https://blog.csdn.net/keda8997110/article/details/50895171 row：记录具体的行被修改的内容。优点是记录清晰，复制完整；缺点是日志量较大。 statement：记录的是执行的S","tags":["mysql"],"title":"MySQL执行一条语句的过程","uri":"https://PI-KA-CHU.github.io/2022/03/mysql%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1%E8%AF%AD%E5%8F%A5%E7%9A%84%E8%BF%87%E7%A8%8B/","year":"2022"},{"content":"前言：Concurrent包的层次结构  java.util.concurrent包下提供了大量针对并发编程的高性能、实用的工具类，其目录结构图如下：\n  JUC包中包含了两个子包，分别是atomic（原子类）包和lock（可重入锁）包（AQS就在lock包中），其他还包括阻塞队列、 executors等，底层主要利用CAS和volatile读写实现，下面是current包的整体技术及功能实现图：\n  一、AQS简介  同步器（AQS）是用来构建锁和其他同步组件的基础框架，负责同步状态的管理，线程的排队，等待和唤醒这些底层操作，主要依赖一个int成员变量来表示同步状态以及通过一个FIFO队列构成等待队列。它的子类必须重写AQS的几个protected修饰的用来改变同步状态的方法，其他方法主要实现了排队和阻塞机制。状态的更新使用getState，setState以及compareAndSetState这三个方法。\n  同步器子类被推荐定义为自定义同步组件的静态内部类，同步器提供了独占式和共享式获取同步状态的方法，可以方便不同类型的同步组件的使用，但是同步器本身没有实现任何同步接口，它定义了同步状态的获取和释放的逻辑，将同步状态的具体实现和释放交由自定义同步组件者实现，这是设计模式中的模板方法设计模式。\n  二、AQS的模板方法设计模式  AQS的设计是使用模板方法设计模式，将同步状态逻辑封装在AQS（如acquire和release）中，将同步状态的具体实现由开发者实现（如tryAcquire和tryRelease），开发者可以自主实现同步资源的获取，如采用公平或者非公平方式获取，下面简单举个例子\n AQS中的方法 // 由开发者实现 protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException(); } // AQS封装的执行逻辑，调用开发者实现的方法 public final void acquire(int arg) { if (!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } ReentrantLock中NonfairSync（继承AQS）重写的方法 protected final boolean tryAcquire(int acquires) { // 继承者重写了tryAcquire，提供了具体实现（省略） return nonfairTryAcquire(acquires); } AQS中可重写的方法 AQS提供的模板方法  三、AQS的构成  下图为AQS的核心构成，AQS提供了共享式锁和独享式锁接口，并且封装了下面实线方框的相关接口，虚线方框则是由开发者（锁开发者）具体实现的方法，比如锁的获取是否公平（公平式抢占和非公平式抢占）等。   同步方式  独占式锁  void acquire(int arg)：独占式获取同步状态，如果获取失败则插入同步队列进行等待； void acquireInterruptibly(int arg)：与acquire方法相同，但在同步队列中进行等待的时候可以检测中断； boolean tryAcquireNanos(int arg, long nanosTimeout)：在acquireInterruptibly基础上增加了超时等待功能，在超时时间内没有获得同步状态返回false; boolean release(int arg)：释放同步状态，该方法会唤醒在同步队列中的下一个节点   共享式锁  void acquireShared(int arg)：共享式获取同步状态，与独占式的区别在于同一时刻有多个线程获取同步状态； void acquireSharedInterruptibly(int arg)：在acquireShared方法基础上增加了能响应中断的功能； boolean tryAcquireSharedNanos(int arg, long nanosTimeout)：在acquireSharedInterruptibly基础上增加了超时等待的功能； boolean releaseShared(int arg)：共享式释放同步状态，共享锁被多个线程读时锁会被多个线程占有    数据结构  同步队列（Node结点：head、tail）  volatile int waitStatus：节点状态 volatile Node prev：当前节点/线程的前驱节点 volatile Node next：当前节点/线程的后继节点 volatile Thread thread：加入同步队列的线程引用 Node nextWaiter：等待队列中的下一个节点 节点状态  int CANCELLED = 1：节点从同步队列中取消 int SIGNAL = -1：后继节点的线程处于等待状态，如果当前节点释放同步状态会通知后继节点，使得后继节点的线程能够运行； int CONDITION = -2：当前节点进入等待队列中 int PROPAGATE = -3：表示下一次共享式同步状态获取将会无条件传播下去 int INITIAL = 0：初始状态     线程状态：state 锁拥有者：exclusiveOwnerThread   四、参考  https://juejin.im/post/5aeb055b6fb9a07abf725c8c https://juejin.im/post/5aeb07ab6fb9a07ac36350c8  ","id":2,"section":"posts","summary":"前言：Concurrent包的层次结构 java.util.concurrent包下提供了大量针对并发编程的高性能、实用的工具类，其目录结构图","tags":["","多线程"],"title":"JUC包结构及AQS抽象队列同步器","uri":"https://PI-KA-CHU.github.io/2020/01/juc%E5%8C%85%E7%BB%93%E6%9E%84%E5%8F%8Aaqs%E6%8A%BD%E8%B1%A1%E9%98%9F%E5%88%97%E5%90%8C%E6%AD%A5%E5%99%A8/","year":"2020"},{"content":"一、happens-before  【happens-before定义了某些操作下的可见性和\u0026quot;有序性\u0026quot;】\n由于重排序（编译器重排和处理器重排）的底层规则的存在导致程序理解变得复杂，严重影响开发效率，为了解决此问题，JMM为程序员在上层提供了六条原则，这样我们就可以根据规则去推论跨线程的内存可见性问题，而不用再去理解底层重排序的规则。下面以两个方面来说。\n 1.1 happens-before定义  happens-before的概念最初由Leslie Lamport在其一篇影响深远的论文（《Time，Clocks and the Ordering of Events in a Distributed System》）中提出。JSR-133使用happens-before的概念来指定两个操作之间的执行顺序。由于这两个操作可以在一个线程之内，也可以是在不同线程之间。因此，JMM可以通过happens-before关系向程序员提供跨线程的内存可见性保证（如果A线程的写操作a与B线程的读操作b之间存在happens-before关系，尽管a操作和b操作在不同的线程中执行，但JMM向程序员保证a操作将对b操作可见）。具体的定义为：\n  如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。  此规则是JMM对程序员的承诺。从程序员的角度理解happens-before：如果A happens-bofore B，那么Java内存模型将向程序员保证——A操作的结果将对B可见，且A的执行顺序排在B之前。真正的运行顺序可能是不一样的，但是运行结果可以这么认为。   两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么这种重排序并不非法（也就是说，JMM允许这种重排序）。  此规则是JMM对编译器和处理器重排序的约束规则。JMM是遵循一个基本原则：只要不改变程序的执行结果（指单线程和正确同步的多线程），编译器和处理器怎么优化都行。\tJMM这么做的原因是：程序员对于这两个操作是否真的被重排序并不关心，关心的是程序执行时的语义不能被改变（即执行结果不能被改变）。其本质上和as-if-serial类似。    1.2 JMM原生happens-before规则  下面规则为Java语言中无需任何同步手段保障就能成立的先行发生的规则。\n  程序次序规则：在一个线程内，按照程序代码顺序，书写在前面的操作Happens-Before书写在后面的操作（前面操作对后面代码可见） 管程锁定规则：一个unlock操作Happens-Before后面对同一个锁的lock操作。 volatile变量规则：一个线程对volatile变量的写入操作Happens-Before另外线程对这个变量的读操作。（volatile写操作对后续读可见） 线程启动规则：Thread对象的start()方法Happens-Before此线程的每一个动作。 线程中断规则： 对线程interrupt()方法的调用Happens-Before被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupt()方法检测到是否有中断发生。 线程终止规则：线程中的所有操作都Happens-Before对此线程的终止检测。 对象终结规则：一个对象的初始化完成（构造方法执行结束）happens-before它的finalize()方法的开始。 传递性：如果某个动作a happens-before 动作b，且b happens-before 动作c，则有a happens-before c。  1.3 happens-before推导  Java中原生满足happens-before关系的有以上八条，下面是由上面八条推导的规则，如：\n  将一个元素放入一个线程安全的队列的操作Happens-Before从队列中取出这个元素的操作 将一个元素放入一个线程安全容器的操作Happens-Before从容器中取出这个元素的操作 在CountDownLatch上的倒数操作Happens-BeforeCountDownLatch#await()操作 释放Semaphore许可的操作Happens-Before获得许可操作 Future表示的任务的所有操作Happens-BeforeFuture#get()操作 向Executor提交一个Runnable或Callable的操作Happens-Before任务开始执行操作  1.4 小结  如果两个操作不存在上述（前面8条 + 后面6条）任一一个happens-before规则，那么这两个操作就没有顺序的保障，JVM可以对这两个操作进行重排序。如果操作A happens-before操作B，那么操作A在内存上所做的操作对操作B都是可见的。 happen-before原则是JMM中非常重要的原则，它是判断数据是否存在竞争、线程是否安全的主要依据，保证了多线程环境下的可见性。 具体的例子可以参考： https://www.cnblogs.com/chenssy/p/6393321.html   二、as-if-serial  as-if-serial的语义是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、处理器都必须遵守as-if-serial语义。为了遵守 as-if-serial 语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序，从而提高处理性能。\n  三、参考  https://www.cnblogs.com/chenssy/p/6393321.html https://juejin.im/post/5ae6d309518825673123fd0e http://ifeve.com/from-singleton-happens-before/  ","id":3,"section":"posts","summary":"一、happens-before 【happens-before定义了某些操作下的可见性和\u0026quot;有序性\u0026quot;】 由于重排序（编译器重","tags":["","多线程"],"title":"Java happens-before及as-if-serial原则","uri":"https://PI-KA-CHU.github.io/2019/12/java%E5%B9%B6%E5%8F%91%E5%8E%9F%E5%88%99/","year":"2019"},{"content":"一、原子性操作方法 1.1 同步锁  synchronized (this){ ++ count; } 1.2 Atomic类  Atomic底层利用CAS实现，而CAS则是利用UnSafe类（native） + 反射技术实现，通过do...while（自旋）的模式，每次都会检查变量是否被修改过，修改过则while成立并重做。  private AtomicLong acount = new AtomicLong(0L); acount.incrementAndGet(); 1.3 LongAdder类  LongAdder为JDK8提供的多线程技术类，底层是利用分而治之的思想，即为每个线程分配一个计数副本，线程之间的副本相互隔离，最终将所有副本相加，多线程下计数速度最快。  private LongAdder lacount = new LongAdder(); // 计数 lacount.increment(); // 获取结果 lacount.sum()  二、Atomic类实现原理-CAS    通过Unsafe获取对象属性偏移量    利用Unsafe的CMS方法及获取到的偏移量实现自增（操作内存）    public class LockDemo1 { private volatile int value = 0; // 直接操作内存，修改对象、数组内存等 private static Unsafe unsafe; // 获取属性偏移量 private static long valueOfferset; static { try { //反射技术获取unsafe值 Field field = Unsafe.class.getDeclaredField(\u0026#34;theUnsafe\u0026#34;); field.setAccessible(true); unsafe = (Unsafe)field.get(null); // 获取到value属性偏移量（用于定位value属性在内存中的具体地址） valueOfferset = unsafe.objectFieldOffset(LockDemo1.class.getDeclaredField(\u0026#34;value\u0026#34;)); } catch (NoSuchFieldException | IllegalAccessException e) { e.printStackTrace(); } } public void add() { // i++; 在Java层面分为三个步骤 // CAS + 循环重试（自旋） int current; do { // 操作耗时的话线程会占用大量的CPU执行时间 current = unsafe.getIntVolatile(this, valueOfferset); } while (!unsafe.compareAndSwapInt(this, valueOfferset, current, current + 1)); // 执行失败则重试 } public static void main(String[] args) throws InterruptedException { LockDemo1 lockDemo1 = new LockDemo1(); for (int i = 0; i \u0026lt; 2; i++) { new Thread(() -\u0026gt; { for (int j = 0; j \u0026lt; 10000; j++) { lockDemo1.add(); } }).start(); } Thread.sleep(2000L); System.out.println(lockDemo1.value); } } ","id":4,"section":"posts","summary":"一、原子性操作方法 1.1 同步锁 synchronized (this){ ++ count; } 1.2 Atomic类 Atomic底层利用CAS实现，而CAS则是利用UnSafe类（native） + 反射技术","tags":["多线程"],"title":"线程安全之原子性操作","uri":"https://PI-KA-CHU.github.io/2019/12/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%80%A7%E6%93%8D%E4%BD%9C/","year":"2019"},{"content":"一、初识Volatile what  volatile是Java语言提供的一种较弱的同步机制，用来确保将变量的更新操作通知到其他线程，volatile修饰的变量具有可见性和有序性，即变量的修改不会被缓存在寄存器或者其他高速缓存器中（可见），并且不会被遍历器和处理器指令重排（有序）。synchronize提供了原子性和可见性，与synchronize相比，volatile不保证原子性，所以在多线程并发情况下仍可能会出现线程安全问题，另外两者的有序性也是不一样的，volatile的有序性是禁止指令重排，而synchronize的有序性是建立在同步之上的，就是说保证单线程执行，并且同步区域的变量会被写入主存（对其他线程可见），但是同步区域仍然可以进行指令重排，这也就是DCL（双重检验锁）的问题。\n where and why  volatile主要应用在需要确保自身状态可见和指令重排会出现问题的场景，如确保所引用对象的状态可见性、标识一些重要的程序生命周期事件的发生（如初始化或关闭）。volatile没有加锁的消耗，因此在一些不需要加锁访问并且需要对其他线程可见的场景能提供更好的性能。 volatile的使用条件：\n 对变量的写入操作不依赖变量的当前值，获取保证只有单线程更新变量的值。 该变量不会与其他状态变量一起纳入不变性条件中。 在访问变量时不需要加锁。   how  volatile用于修饰共享变量，保证变量的修改在多线程间的可见性和有序性（禁止指令重排），下面的例子是单例模式中的双重检验锁（DCL）模式，使用volatile保证要创建的对象的可见性，否则在同步代码块中，对象的new在底层分为多步骤进行，如：a.分配内存、b.初始化对象、c.对象引用指向内存区域，由于同步代码块中可以进行指令重排（由abc变成acb，可参考笔者的另一篇博客Java并发基础），其他线程在判断对象不为空时对象可能还没初始化，获取到的是空的对象，由此造成安全问题。\n public class Singleton { private static volatile Singleton singleton; private Singleton(){ } public static Singleton getInstance(){ if(singleton != null){ synchronized (Singleton.class){ if (singleton != null){ // 可能会有指令重排 singleton = new Singleton(); } } } return singleton; } }  二、Volatile的底层原理  JMM（Java内存模型）定义了volatile的内存语义，当一个变量声明为volatile时，它的读写操作将具有特殊的含义。\n 2.1 volatile内存语义 2.1.1 可见性的内存语义  写的内存语义：当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值刷新到主内存。其他线程在监听到总线对该内存地址的写入后，如果他们的对该地址内存的缓存状态为S，则让缓存失效并置为I。 读的内存语义：当读一个volatile变量时，由于volatile写入时MESI协议会把该线程对应的本地内存置为无效，线程接下来从主内存中读取共享变量。  2.1.2 可见性的内存语义实现  如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，该指令将变量所在缓存行的数据写回系统内存。但是即使内存被写回，缓存在其他处理器上的数据仍然是旧数据，在多核处理器下，为了保证各个处理器的缓存是一致的，CPU厂商制定了缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行的内存地址被修改，就会将当前处理器的缓存行设置为无效（仅仅设置为无效，不会直接更新），当需要数据的时候发现缓存行状态为无效，则从主存中读取。  2.2.1 有序性的内存语义  volatile读：volatile读之后的操作不会被重排序到volatile读之前 volatile写：volatile写之前的操作不会被重排到volatile之后 先volatile写-后volatile读：不可重排序  2.2.2 有序性的内存语义实现  通过对编译器和处理器的重排序的限制，从而实现了volatile的内存语义。\n   对编译器重排序的限制\n 为了实现volatile的内存语义，在编译器生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器排序。     对处理器重排序的限制  对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎是不可能的，为此，JMM采取了保守策略（即在volatile写的前面和后面分别插入内存屏障，在volatile读操作后面插入两个内存屏障）：    在在每个volatile写操作的前面插入一个StoreStore屏障；    在每个volatile写操作的后面插入一个StoreLoad屏障；    在每个volatile读操作的后面插入一个LoadLoad屏障；    在每个volatile读操作的后面插入一个LoadStore屏障。     屏障的作用：  StoreStore屏障：禁止上面的普通写和下面的volatile写重排序； StoreLoad屏障：防止上面的volatile写与下面可能有的volatile读/写重排序 LoadLoad屏障：禁止下面所有的普通读操作和上面的volatile读重排序 LoadStore屏障：禁止下面所有的普通写操作和上面的volatile读重排序   《Java并发编程艺术》书中例图     三、总结  扩展：虚拟机规范中，写64位的double和long分成了两次32位值的操作（非原子性），而添加了volatile修饰的long和double读写总是原子的，读写引用也是原子的。商业JVM则不会有此问题，考虑到实际应用都实现了原子性。 本篇博客对volatile的可见性和有序性进行了描述，并针对底层实现进行了简单总结，设计到编译器、处理器的指令重排机制及实现volatile可见性的MESI缓存一致性协议、编译器Locl前缀的指令和实现有序性的CPU内存屏障，内容主要总结自其他大佬博客及个人理解，有不正确的地方希望大家指出。\n  参考\n https://juejin.im/post/5ae9b41b518825670b33e6c4#heading-1 https://gorden5566.com/post/1018 https://blog.csdn.net/tb3039450/article/details/67636391   ","id":5,"section":"posts","summary":"一、初识Volatile what volatile是Java语言提供的一种较弱的同步机制，用来确保将变量的更新操作通知到其他线程，volatile修","tags":["","多线程"],"title":"Volatile关键字","uri":"https://PI-KA-CHU.github.io/2019/12/volatile%E5%85%B3%E9%94%AE%E5%AD%97/","year":"2019"},{"content":"一、初识Synchronize what  Synchronize是JVM的底层关键字，可以为对象的方法或者代码块加锁，加锁的代码同一时刻最多只能有一个线程执行。多个线程并发访问同一个对象的加锁同步代码时，一个时间内只能有一个线程获取到锁，其他线程会进入等待队列，待锁被释放后再进行抢占。Synchronize修饰的方法和代码块具有同步性和可见性。\n where和why  Synchronize被使用在多线程并发的场景，多个线程并发对一块非线程安全的代码进行操作，会出现线程安全问题，出现无法预料的结果。对被并发执行的代码添加Synchronize关键字，可以将多线程执行的代码同步化，同一时间内只能有一个线程获取到锁并进行操作，可以保证线程的变化（共享数据的变化）被其他线程看到，此时的代码块或方法是线程安全的。\n how  修饰实例方法（对象锁）  public synchronized void synMethod(){ //方法体 }  修饰静态方法（类锁）  public static synchronized void synMethod(){ //方法体 }  修饰代码块  public Object synMethod(Object a1){ synchronized(a1){ //一次只能有一个线程进入 } }  二、Synchronize的底层原理  HotSpot JVM： https://www.cs.princeton.edu/picasso/mats/HotspotOverview.pdf 参考： https://blog.csdn.net/javazejian/article/details/72828483   Synchronize主要用于修饰方法和代码块，两者在JVM中的底层实现是不同的，同步代码块的实现主要基于对象监视器（Monitor）实现，其主要是利用monitorenter和monitorexit指令实现代码同步；同步方法则是依靠方法修饰符上的ACC_SYNCHRONIZED标识实现同步。两者的JVM指令（利用javap命令对.class文件进行反编译）如下图所示：\n 同步代码块：\n同步方法：\n2.1 对象的同步属性  Synchronize的底层同步（重量级锁）就是利用Monitor监视器实现线程间的互斥，进而达到同步的效果，下面简单描述Java对象在内存中的存储结构及Monitor如何实现同步监视。\n 2.1.1 对象的内存布局  对象头（Header）   Mark Word：主要存储对象运行时数据，如存储哈希码、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，长度在32位和63位的虚拟机中分别为32bit和64bit。Mark Word被设计为非固定大小的数据结构，在不同的锁状态下存储不同的内容，其中偏向锁是Java 6之后对synchronize的优化新添加的，锁标识位是01；重量级锁的锁标识位是10，其中指针指向monitor对象的起始地址，monitor在下面会进行较详细的介绍。  例：在对象未被锁定的状态下，32bit的Mark Word中的25bit用于存储对象哈希码，4bit用于存储对象分代年龄，2bit用于存储锁标志位，1bit固定为0（当1时候表示偏向锁）。    类型指针:即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。如果是数组对象还会存储数组长度的数据。\n   实例数据（Instance Data）：对象真正存储的有效信息，即程序代码中所定义的各种类型的字段内容（包括父类字段）。 对齐填充（Padding）：非必要存在，占位符（对象的大小必须是8的整数倍，不足时补充对齐）。  2.2.1 monitor监视器底层  每个对象都存在对应的monitor与之关联，对象与其monitor之间的关系有存在多种实现方式，如monitor可以与对象一起创建销毁或当线程试图获取对象锁时自动生成，monitor对象被某个线程持有后会进入锁定状态，monitor是由ObjectMonitor实现，其数据结构如下（C++源码）：\n ObjectMonitor() { _header = NULL; _count = 0; //记录线程获取锁的次数个数 _waiters = 0, _recursions = 0; //线程的重入次数，为0时释放锁 _object = NULL; _owner = NULL; //持有ObjectMonitor对象（锁）的线程，释放锁时为NULL _WaitSet = NULL; //处于wait状态的线程，会被加入到_WaitSet（如线程被调用wait方法后，需要等待notify方法唤醒） _WaitSetLock = 0 ; _Responsible = NULL ; _succ = NULL ; _cxq = NULL ; FreeNext = NULL ; _EntryList = NULL ; //存放处于等待锁阻塞（block）状态的线程 _SpinFreq = 0 ; _SpinClock = 0 ; OwnerIsThread = 0 ; } 上面为monitor对象的数据结构，主要包括_EntryList和_WaitSet队列及_owner锁持有者和其他的信息，获取到monitor对象（锁）的线程可以正常执行代码。\n2.2 synchronize锁状态的变化  synchronize在1.6版本后进行了优化，获取锁的过程为偏向锁（乐观锁，适应于一个线程执行的场景，进行简单的线程Id校验，减少轻量级锁的CAS次数） - 轻量级锁（乐观锁，适用于多线程交替执行同步块的场景，轻量级锁认为当前线程的竞争程度很轻，即可能是两个线程交互执行，只要稍微（自旋）等待一会就可以获取到锁，如果自旋次数超过一定次数，则认为当前锁竞争激烈，升级为重量级锁） - 重量级锁（悲观锁，适用于多线程同时进入同步块场景，需要获取monitor对象监视器，阻塞除了拥有锁以外的其他线程）\n   偏向锁加锁过程\n   访问Mark Work中偏向锁的标识是否为1（默认为1，即开启状态）    如果为可偏向状态，则检查_owner线程Id是否指向当前线程，如果是，进入步骤5，否则进入步骤3    如果线程ID未指向当前线程，则通过CAS操作竞争锁。如果加锁成功（成功加锁后获取锁的线程再次进入时只需校验是否当前获取锁的线程Id，无需再进行CAS操作），则执行步骤5；如果加锁失败，则执行步骤4。    如果CAS获取偏向锁失败，则表示当前存在竞争，在到达全局安全点时，获取到偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续执行同步代码。    执行同步代码。    释放锁：偏向锁不会由线程主动释放，而是等到其他竞争锁的线程出现时，暂停拥有偏向锁的线程，检查线程是否存活，如果不存活，则恢复到无锁状态，允许其他线程竞争；如果存活，则挂起持有偏向锁的线程，将对象头Mark word修改为指向锁记录指针的标识，锁升级为轻量级锁状态（00），最后重新唤醒挂起的线程。      轻量级锁加锁过程\n 参考：  https://gorden5566.com/post/1019.html https://www.cnblogs.com/paddix/p/5405678.html https://www.zhihu.com/question/53826114     在代码进入同步块的时候，如果同步对象锁状态为无锁状态（锁标志位为“01”状态，是否为偏向锁标识为“0”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝，官方称之为Displaced Mark Word。\n    拷贝对象头中的Mark Word复制到锁记录中。    拷贝成功后，虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，并将Lock record里的owner指针指向object mark word。如果更新成功，则执行步骤4，否则执行步骤5。    如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00”，即表示此对象处于轻量级锁定状态，这时候线程堆栈与对象头的状态如下图所示。\n    如果这个更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁（重入，每次获取轻量级锁时都会创建一个 Lock Record，锁重入时会创建多个指向同一个Object的Lock Record，除第一次设置Displaced Mark Word ，后面均设置为 null），那就可以直接进入同步块继续执行。否则说明多个线程竞争锁，（多次自旋无果后）轻量级锁就要膨胀为重量级锁，锁标志的状态值变为“10”，Mark Word中存储的就是指向重量级锁（互斥量）的指针，后面等待锁的线程也要进入阻塞状态。 而当前线程便尝试使用自旋来获取锁，自旋就是为了不让线程阻塞，而采用循环去获取锁的过程。    释放锁：利用CAS操作把当前线程的栈帧中的Displaced Mark Word替换回锁对象的Mark Word中去，如果替换成功，则解锁成功，恢复到无锁的状态（01）。若替换失败，则轻量级锁膨胀为重量级锁后再解锁。      重量级锁\n  Synchronize的优化  锁消除  jitwatch下载地址： https://github.com/AdoptOpenJDK/jitwatch 参考博客：  http://www.cnblogs.com/stevenczp/p/7975776.html https://www.cnblogs.com/stevenczp/p/7978554.html       例如StringBuffer是一个线程安全的类，其内部方法使用Synchronize修饰，但是如果我们是在单线程下使用StringBuffer进行字符串操作，那么不存在线程安全问题（线程封闭），但是频繁的加锁和解锁会造成性能消耗，此时Java在运行时会把锁消除进行优化（运行时优化，javap无法查看），可以利用工具查看Java最终运行的汇编代码，利用汇编指令表可以对照查看。\n  锁粗化   例如在for循环中循环的加锁，会频繁的进行加锁解锁操作，造成性能消耗，Java会进行优化，将锁范围进行扩大，例如扩到到for循环外部。\n   偏向锁 （参考上文）\n  轻量级锁 （参考上文）\n  自旋锁\n   轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。这是基于在大多数情况下，线程持有锁的时间都不会太长，如果直接挂起操作系统层面的线程可能会得不偿失，毕竟操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，因此自旋锁会假设在不久将来，当前的线程可以获得锁，因此虚拟机会让当前想要获取锁的线程做几个空循环(这也是称为自旋的原因)，一般不会太久，可能是50个循环或100循环，在经过若干次循环后，如果得到锁，就顺利进入临界区。如果还不能获得锁，那就会将线程在操作系统层面挂起，这就是自旋锁的优化方式，这种方式确实也是可以提升效率的。最后没办法也就只能升级为重量级锁了。\n  三、总结  一个synchronize就涉及到大量的知识点，文章主要根据网易微专业视频并查找相关的博客、书籍学习后总结而成，主要涉及到synchronize锁状态及底层同步处理的过程，如有错误或问题，请联系笔者。\n ","id":6,"section":"posts","summary":"一、初识Synchronize what Synchronize是JVM的底层关键字，可以为对象的方法或者代码块加锁，加锁的代码同一时刻最多只能有一个","tags":["","多线程"],"title":"Synchronize关键字","uri":"https://PI-KA-CHU.github.io/2019/12/synchronize%E5%85%B3%E9%94%AE%E5%AD%97/","year":"2019"},{"content":"问题描述  今天一个在暑假的时候认识的朋友加了我微信让我帮忙看下电脑，电脑的情况是：打开一个软件后蓝屏了，并且一直在windows的自动修复下循环，即修复失败，具体错误如下图：    解决分析 相似案例查找  http://tieba.baidu.com/p/5120842696?share=9105\u0026amp;fr=share\u0026amp;unique=BD19F5E3CE7D1EFAE1CBAF2EDA4007C4\u0026amp;st=1576513965\u0026amp;client_type=1\u0026amp;client_version=10.3.16\u0026amp;sfc=copy https://www.auslogics.com/en/articles/fix-srttrail-txt-bsod-error-win10/  本人采用修复六    解决方案 1、关闭Windows自动修复\n 在修复模式的CMD命令下执行bcdedit /set {default} recoveryenabled No关闭自动修复，关闭后重启异常如下：   2、利用U盘启动进行系统引导修复\n U盘启动盘制作工具（此处用的是老毛桃）： http://www.laomaotao.org/ 修复流程参考： https://www.fujieace.com/computer-practical/guide.html  3、利用U盘启动PE删除原系统异常文件（注意，如果是windows重要文件异常请勿进行）\n 此处主要是要找到系统盘的位置，在步骤2中自动修复的时候会有显示系统盘地址。后根据蓝屏异常文件进行删除。 其他流程参考： https://www.fujieace.com/computer-practical/bootsafe-sys.html 删除后正常启动   ","id":7,"section":"posts","summary":"问题描述 今天一个在暑假的时候认识的朋友加了我微信让我帮忙看下电脑，电脑的情况是：打开一个软件后蓝屏了，并且一直在windows的自动修复下循","tags":["疑难杂症"],"title":"U盘修复工具","uri":"https://PI-KA-CHU.github.io/2019/12/u%E7%9B%98%E5%90%AF%E5%8A%A8%E4%BF%AE%E5%A4%8Dwin10/","year":"2019"},{"content":" Idea已经称为越来越普遍的开发工具，其功能强大，并且界面简洁舒服，但是Idea快捷键用起来并没有Eclipse舒服，很多快捷键用起来很不方便，但是Idea提供了适配Eclipse的功能，需要更换的朋友可以自行百度，下面的快捷键都是切换后的，也就是大部分都是Eclipse的快捷键，对原版Idea的不适用。\n Idea快捷键  Idea快捷键大全： https://segmentfault.com/a/1190000008536680 Eclipse快捷键大全： https://www.runoob.com/eclipse/eclipse-shortcuts.html  快速回退到上个光标位置  alt + 方向键（左右） https://blog.csdn.net/u010814849/article/details/76682701  快速显示OutLine（类方法视图）  control + O  显示当前类的继承结构（t：tree）  control + t  进入内联函数  control + shift + i  进入光标所在方法或变量处  F3  打开类型层次结构  F4  左右切换窗口  control + alt + 左右键  隐藏窗口（hide）  shift + esc  快速执行/结束代码  执行：alt + shift + x 结束：control + F2  显示工具描述（如显示方法的doc）  F2  ","id":8,"section":"posts","summary":"Idea已经称为越来越普遍的开发工具，其功能强大，并且界面简洁舒服，但是Idea快捷键用起来并没有Eclipse舒服，很多快捷键用起来很不方","tags":["开发工具"],"title":"Idea高效开发","uri":"https://PI-KA-CHU.github.io/2019/12/idea%E9%AB%98%E6%95%88%E5%BC%80%E5%8F%91/","year":"2019"},{"content":" 本篇文章主要讲述是并发编程前需要学习的一些基础知识，如Java程序执行过程、线程的状态、CPU的性能优化方法及带来的问题和解决方案等，这些在后面的并发编程中是必不可少的，特别是CPU的缓存、指令重排相关问题都与Java的并发编程有着重要联系。本篇博客主要参考网易云微专业 - Java高级开发工程师的慕课视频，视频的知识体系是比较完整的，不过在知识深度的讲解上还不够，所以会参考其他大佬博客的文章，由于时间原因可能主要以引用为主，希望学完并发编程部分自己能够更大的提升！\n  一、Java程序的运行分析  下面主要介绍Java类从编译到运行的过程：【编译 - 加载 - 运行】及此过程中资源的分配情况\n 编译  Java编译即由.java源文件到.class字节码文件的过程，编译时如果发现依赖类没有被编译，则会先编译依赖类，编译完的字节码文件主要包含两部分：常量池和方法字节码（还有其他的文件源信息，JDK版本等），其中（main）方法字节码如下如所示：    编译命令\n 编译：javac Demo1.java 反编译：javap -v Demo1.class \u0026gt; Demo1.txt （反编译后的文件中有相关的JVM指令操作，可以参考JVM指令码表）    访问标识（Demo1类）：   常量池类信息表：   加载  加载类到JVM内存中（如将方法字节码加载到JVM方法区），根据字节码文件执行相关指令，发现未加载的依赖类时，会先加载依赖类（可参考书籍码出高效中的类加载过程）  运行    JVM创建线程来执行代码，线程在创建的时候会分配相关的资源   独占空间（线程私有）：  虚拟机栈：一个线程对应一个栈，一个栈对应多个栈帧，栈帧即为方法对应的操作 程序计数器：记录当前线程的字节码执行位置 内存区域：创建线程独占空间      在执行的过程中，程序计数器会记录方法区中执行的方法的字节码指令位置，并且在该方法栈帧中利用局部变量表和操作栈执行及记录相关指令，如果调用其他方法，则会在该线程栈中开辟新的栈帧（即每个方法对应一个栈帧）执行上述类似的过程。      二、线程基础  下面主要简单描述了线程的各个状态及安全的线程终止方式，部分已经有学习及记录过，在此不再重复，可参考以前的博客。\n 线程状态 线程的终止方式  stop()方法：不推荐，会导致线程安全问题，是一个强制中止线程的行为 interrupt()方法：如果遇到运行中或阻塞中的线程，会抛出InterruptException异常，由 开发者自行捕获和处理。 标识符：如在线程1中有flag标识符，另外的线程修改线程1的flag状态，达到终止线程的目的， 而不是拦腰截断。   三、CPU缓存及内存屏障  参考：  https://zhuanlan.zhihu.com/p/29881777 https://crowhawk.github.io/2018/02/10/volatile/    3.1 CPU的性能优化 - 缓存  由于CPU和内存的运行速度相差很大，为了提高CPU的性能，在CPU到主内存间加入多个高速缓存，如L1，L2，L3三级缓存，尽量让CPU从高速缓存中获取数据，从而提高其性能。\n  L1 Cache（一级缓存）：分为数据缓存和指令缓存。容量通常为：32-4096KB L2 Cache（二级缓存）：由于L1高速缓存容量的限制，为了再次提高CPU的运算速度，在CPU外部防止一高速存储器，即二级缓存。 L3 Cache（三级缓存）：L3缓存可以进一步降低延迟，提升大数据量计算时处理器的性能。具有较大L3缓存的处理器提供更有效的文件系统缓存行为及较短信息和处理器队列长度。一般是多核共享一个L3缓存。  缓存一致性协议（总线索和MESI协议）\n 多CPU读取同样的数据，进行不同的运算后，最终写入主存的以哪个CPU为准？为了应对这种高速缓存回写的场景，常见的解决缓存一致性的方法有两种，总线锁和MESI。\n    总线锁：总线锁（Bus Locking）实现是当一个CPU对缓存中的数据进行操作的时候，往总线中发送一个Lock信号。这个时候，所有CPU收到这个信号之后就不操作自己缓存中的对应数据了，当操作结束，释放锁以后，所有的CPU就去内存中获取最新数据更新。但是总线锁会导致CPU的性能下降，因而出现了另外一种CPU缓存一致性协议：MESI。    MESI：CPU厂商制定了MESI协议，规定了每条缓存有个状态位，同时定义了以下四个状态；   修改态（Modified）：此cache行已经被修改过（脏行），内容已经不同于主存，为此cache专有； 专有态（Exclusive）：此cache行内容同于主存，但不出现在其它cache中； 共享态（Shared）：此cache行内容同于主存，但也出现在其他cache中； 无效态（Invalid）：此cache行内容无效（空行）；    CPU的读取遵循下面几点：\n 如果缓存状态是I，那么就从内存中读取，否则就从缓存中直接读取。 如果缓存处于M或E的CPU读取到其他CPU有读操作，就把自己的缓存写入到内存中，并将自己的状态设置为S。 只有缓存状态是M或E的时候，CPU才可以修改缓存中的数据，修改后，缓存状态变为M。   多处理器时，单个CPU对缓存中的数据进行了改动，需要通知给其他CPU，即CPU既要控制自己的读写操作，还要监听其他CPU发出的通知，从而保证最终一致性。\n 3.2 CPU性能优化 - 运行时指令重排  参考： http://ifeve.com/memory-barriers-or-fences/   由于可能存在多个CPU共享缓存同块缓存区的情况，如果某个CPU写缓存的时候缓存被其他CPU占用的情况，会将该CPU的读缓存先执行，即进行指令重排序，以此来提高CPU的性能，但是重排需要遵守as-if-serial语义：无论怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。也就是说，编译器和处理器不会对存在数据依赖关系的操作做重排序。\n 3.2.1 重排序的分类  编译器重排序：编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 处理器重排序  指令重排序：如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存重排序：不改变单线程结果的情况下，缓存可以改变写入的变量的顺序。    3.2.2 重排序对单线程的影响  重排序不会改变单线程程序的执行结果，所有的重排序都遵守as-if-serial语义。对于存在数据依赖关系的曹祖，编译器和处理器都不会进行重排序。  3.2.3 重排序对多线程的影响  多线程情况下，对不存在数据依赖（相对于当前线程而言）的操作进行重排序，也会影响结果。比如下面代码，如果存在读线程和写线程，对于各自的线程而言，1、2和3、4都不是数据依赖的关系，所以2可能先于1执行，那么读线程可能会读到1未执行时的a的值，此为重排序引起的并发问题。  class ReorderExample { int a = 0; boolean flag = false; public void writer() { a = 1; // 1 flag = true; // 2 } Public void reader() { if (flag) { // 3 int i = a * a; // 4 } } }\t3.2.4 数据竞争（博客）  数据竞争是指未同步的情况下，多个线程对同一个变量的读写竞争，它的定义如下：    在一个线程中写一个变量    在另一个线程中读同一个变量    上述的两个操作未正确同步     如果存在数据竞争，程序的执行结果是没有保证的。但是如果正序是正确的同步的，就不存在数据竞争，比如上面的代码中，共享变量flag和a的读写在可能存在两个线程中，并且读写方法未同步，存在数据竞争。如果读写方法是同步方法，则没有数据竞争关系，程序的执行将具有顺序一致性。  3.2.5 竞态条件（Java并发编程实战）  当某个计算的正确性取决于多个线程的交替执行时序时，就会发生竞态条件。大多数的竞态条件的本质是：基于一种可能失效的观察结果来做出判断或者执行某个计算，其常见类型的主要为：先检查后执行、读取-修改-写入，其观察的结果可能变得无效，从而导致各种问题（未预期的异常、数据被覆盖、文件被破坏等） 竞态条件与数据竞争的描述类似，都是简述了多线程并发下可能存在问题的场景，笔者觉得竞态条件描述有点抽象，数据竞争的描述更具体些，可以很好的带入到各个出现问题的案例中。  3.3 存在的问题   CPU高速缓存下的问题：缓存中的数据与主存中的数据不是实时同步的，各CPU间缓存的数据也不是实时同步的。在同一时间点，各个CPU看到同一内存地址中的数据的值可能是不一致的\n  CPU指令重排序下的问题：虽然遵守了as-if-serial语义，但是仅仅在单CPU自己执行的情况下保证结果正确。多核多线程中，指令逻辑无法分辨因果关联，可能会出现乱序执行，导致程序运行结果错误。\n  3.4 内存屏障  为了解决上面出现的问题，CPU厂商为处理器提供了两个内存屏障指令，一旦内存数据被推送到缓存，就会有消息协议来确保所有的缓存会对所有共享数据保持一致，使得CPU或编译器在对内存进行操作的时候，严格按照一定的顺序来执行，也就是说在Store Barrier之前的指令和Load Barrier之后的指令不会由于系统优化等原因导致乱序。\n  写内存屏障（Store Memory Barrier）：在指令后插入Store Barrier，能让写入缓存中的最新数据更新写入主内存，即强制所有在store屏障指令之前的store指令，都在该store屏障指令执行之前被执行并刷入内存，让其他线程可见。保证了内存写操作。  强制写入主内存，CPU就不会因为性能考虑而去对指令重排，而是严格按照执行顺序执行。   读内存屏障（Load Memory Barrier）：在指令前插入Load Barrier，可以让高速缓存中的数据失效，强制从新主内存中加载数据。即强制所有在load屏障指令之后的load指令，都在该load屏障指令执行之后被执行，并且一直等到load缓冲区被该CPU读完才能执行之后的load指令。保证了内存读操作。  强制读取主存内容，让CPU缓存与主内存保持一致，避免了缓存导致的一致性问题。   完全内存屏障（full memory barrier）：保障了早于内存屏障的内存读写操作的结果提交到内存后，再执行晚于屏障的读写操作。  3.5 本节小结  本章节主要介绍了CPU的性能优化方法及出现的相关问题和处理方法，为后面JVM线程安全问题做铺垫。\n  四、线程通信  要实现多个线程之间的协同，如线程执行先后顺序、获取某个线程的执行结果的等。涉及到线程之心啊的相互通信，主要分为以下四种：文件共享、网络共享、变量共享和JDK提供的线程协调API（wait/notify、park/unpark）\n 4.1 文件共享 4.2 网络共享 暂略。\n4.3 变量共享 4.4 JDK线程协作API  在Java中典型的案例即：生产者 - 消费者模式\n 三种API：\n suspend和resume  已经被弃用，在同步代码块中或者先后顺序有误时会导致死锁 suspend后不会释放锁，容易造成死锁问题，而wait则会释放   wait和notify/notifyAll  只能在同步代码块中调用，否则会抛出异常 调用wait后会释放锁和CPU，进入waiting状态等待被唤醒 虽然notify会自动解锁，但是对顺序有要求，如果在notify调用后才调用wait，线程会永远处于WAITING状态。   park和unpark  对顺序无要求，类似于颁发许可证书的情况，线程在park的时候，如果存在许可，则立即返回（类似于正常唤醒），如果没有许可，则会进入阻塞状态。许可只有一个，不可累加，即多次调用unpark不可累加许可。 不会释放锁，在同步代码块中会造成死锁，比如消费者获取到对象锁后调用LockSupport.park()方法，此时锁被占用，生产者无法获取到对象锁，也无法调用LockSupport.unpack(oneThread)唤醒。    伪唤醒：\n Java官方建议在循环中检查等待条件，原因是处理等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件而是用If语句，程序就会在没有满足结束条件的情况下退出。伪唤醒是指线程并非因为notify、notifyAll、unpark等api调用而唤醒，这涉及到更底层的因素，会造成程序逻辑的错误。\n 4.5 本节小结  本节主要对线程的通信进行简单介绍，并且着重介绍了JDK中的线程协作API，在很多JDK多线程开发工具类中都是底层实现，需要重点掌握。\n  五、线程封闭  多线程访问共享可变数据时，涉及到线程间数据同步的问题，而有时候我们并不希望数据被共享，所以有了线程封闭的概念。 数据都被封闭在各自的线程中，就不需要进行同步操作，这种通过将数据封闭在线程中而避免实用同步的技术称为线程封闭。\n 5.1 ThreadLocal  ThreadLocal是Java中的一种特殊变量，是一个线程级别的变量。在ThreadLocal中，每个线程都有自己独立的一个变量，JVM在其底层中维护了一个Map\u0026lt;ThreadName,value\u0026gt;，为每个线程分配了一个副本，线程与线程间的副本之间彼此独立，互不影响，从而消除了线程间的竞争条件，在并发模式下是绝对安全的变量。\n  用法：ThreadLocal\u0026lt;T\u0026gt; var = new ThreadLocal\u0026lt;T\u0026gt;() 应用场景：  多线程下多个方法需要使用相同的变量时（不互相依赖），可以减少代码量 线程中多个方法使用某变量，可以替换方法传参的做法。（即成员变量的作用）    5.2 栈封闭（局部变量表）  局部变量的固有属性之一就是封闭在线程中，它们位于执行线程的栈中，例如在JVM操作栈中，每个方法对应一个栈帧，栈帧中有局部变量表和操作栈，其他线程无法获取到当前栈帧中的局部变量表。\n  六、线程池原理  线程在Java中的一个对象，更是操作系统的资源。\n 6.1 为什么要用线程池 线程是不是越多越好？\n   线程的创建、销毁需要时间。如果创建时间 + 销毁时间 \u0026gt; 执行任务时间就是很不划算的。    Java对象占用堆内存，操作系统线程占用系统内存，根据jvm规范，一个线程默认最大栈大小是1M，这个栈空间是需要从系统内存中分配的。线程过多，会消耗很多的内存。    线程过多时，操作系统需要频繁地切换上下文，从而影响性能，即CPU的大部分时间都被花费在线程切换上了。     从上面三点可以知道，线程并不是越多越好的，不能够无限制的创建，线程池的推出，就是为了方便控制线程数量。\n 6.2 线程池的原理 - 概念  线程池管理器：用于创建并管理线程池，包括创建线程池，销毁线程池，添加新任务； 工作线程：线程池中线程，在没有任务时处于等待状态，可以循环的执行任务。 任务接口：每个任务必须实现的接口，以供工作线程调度任务的执行，它主要规定了任务的入口，任务执行完后的收尾工作，任务的执行状态等，例如线程需要实现的Runnable接口或者Callable接口等； 任务队列：用于存放没有处理的任务，比如线程达到核心线程数量后，会将多出的线程存放在任务队列中，作为一种缓存机制存在。 拒绝策略：当线程数量超过最大线程数后执行的策略，一共有四种策略：抛异常、抛弃、利用当前传递的管理线程执行、丢弃最早的任务，将其放入任务队列中。  6.3 线程池API 接口定义和实现类  接口  Executor：最上层接口，定义了执行任务的方法execute ExecutorService：继承自Executor接口，扩展你了Callable、Future、关闭方法 ScheduledExecutorService：继承自ExecutorService，增加了定时任务相关的方法   实现类  ThreadPoolExecutor：基础、标准的线程池实现。  submit：提交任务到线程池 getPoolSize：获取当前线程池数量 getQueue().size()：获取当前队列中的等待线程数量   ScheduledThreadPoolExecutor：继承自ThreadPoolExecutor，实现了定时任务相关的方法。    常见线程池类型   创建方法：Executors.newFixedThreadPool() 和new ThreadPoolExecutor()，阿里巴巴开发手册中强制规定使用第二种方式创建，由于Executors中的任务队列和最大线程数量都默认为无界，在大量请求下会导致OOM。\n  ThreadPoolExecutor（标准线程池）：通过自定义new，并配置相关参数，如核心线程数量、最大线程数量、线程存活时间、阻塞队列等，如果阻塞队列为定长队列，还需要配置拒绝策略。一般是\n  // 此处的拒绝策略用Lambda表示式实现，即`new RejectedExecutionHandler()`并重写`rejectedExecution`方法 ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 10, 5, TimeUnit.SECONDS, new LinkedBlockingQueue\u0026lt;\u0026gt;(3), (r, executor) -\u0026gt; System.err.println(\u0026#34;有任务被拒绝执行了\u0026#34;));\t FixedThreadPool（定长线程池，最大线程数 = 核心线程数）- 长期较少的任务  LinkedBlockingQueue：基于链表的无界阻塞队列，利用AQS实现并发操作。    ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 5, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;());  CachedThreadPool（缓存（弹性）线程池，起始无线程，线程有存活时间，无线程数量限制）- 当任务提交到线程池时，如果有空闲线程则直接调用，没有则创建新线程。适合短期较多的异步任务，任务大小无法控制、动态变化因素比较多的场景。  SynchronousQueue：无缓存阻塞队列，队列始终为空，必须等待提交的任务被消费者消费后才会继续提交，故任务的提交是顺序的，但是结束顺序是不确定的。底层采用CAS实现并发操作，有公平（TransferQueue）和非公平（TransferStack）两种模式。    ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;());  SingleThreadPool（单线程线程池）- 顺序执行的任务 ScheduledThreadpool（定时器线程池）- 周期性执行的任务  线程数量  生产环境中，一般CPU的使用率达到80%则表示CPU充分被利用，少于80%则可能需要开多些线程，使用率太高则表示线程数量太多，太低则表示线程数量过少。\n 如何确定合适数量的线程？  计算型任务：纯内存操作，占用CPU比较高的，一般线程数量为cpu数量的1-2倍 IO型任务：网络操作、数据库操作、文件操作，可能会有IO阻塞，一般需要开启较多线程。  tomcat默认线程数量：200 CacheThreadPool自动增减线程数     七、总结  以上都是Java并发的基础操作，在后续AQS、关键字Synchronize、Volatile底层都有相关操作，特别是CPU内存屏障及指令重排，需要有较深入的理解。\n ","id":9,"section":"posts","summary":"本篇文章主要讲述是并发编程前需要学习的一些基础知识，如Java程序执行过程、线程的状态、CPU的性能优化方法及带来的问题和解决方案等，这些在","tags":["","多线程"],"title":"Java并发基础","uri":"https://PI-KA-CHU.github.io/2019/12/java%E5%B9%B6%E5%8F%91%E5%9F%BA%E7%A1%80/","year":"2019"},{"content":"一、项目拆解 分布式系统  一个请求由多个系统协同完成，这种系统称为分布式系统\n  服务拆分：单体项目一个模块修改需要重新部署、运维整个项目，故将其拆分解耦 配置中心：拆分为多个子项目后，由于每个模块都需要配置文件，如果配置文件修改的话，需要 修改所有模块的配置，故将配置隔离到配置中心统一管理。 调度中心：定时任务的执行，需要统一管理、统一调度 日志统一管理：统一收集所有模块的日志信息 预警系统：故障追踪 监控系统  报警系统 故障案例：\n FullGC线上故障案例  stop-the-world：GC的时候其他线程是不工作的，请求会被挂起的   慢SQL调优案例  排查路线：\n 问题现象描述 初步判断  版本更新 参数异常（收到异常数据）   临时解决方案  能够支持故障切换，则保留故障现场 重新、版本回滚（复盘认识自己的不足，详细了解问题，看看自己能够在这次事故中能够学习到什么）      Linux的重要性，在排查问题的时候，Linux的命令是很重要的，需要掌握。 生产环境绝不可以用工具操作生产环境\t，如Navicat工具直接修改数据，应该都走SQL语句变更流程， 否则会导致数据被修改但是无法追踪。    二、方案落地  技术并不是最复杂的，人和事及关系才是麻烦\n 提出技术方案 现状/问题：\n 单体应用：单个体量不合理，主平台体量太大，无法整体更新维护； 多版本共存：版本混乱，只敢添加，不敢修改； 系统整体脆弱，访问量一大就挂； 管理问题：发布困难、测试困难、修改困难、排错困难；  技术落地顺序：  数据库是整个信息系统中生命周期最长、最难修改的部分。 开发规范化； 引入项目管理，将具体服务的开发实现，最好以工单或项目来落地； (作为一个开发组长，应该学习如何更好的落地，更好的具体的开发任务，并分配和跟进)  落地方法  对项目进行总体规划 项目管理 知识分享 团队建设 技术分享 工程师文化  包结构和命名规范  规范命名各个包名及包的相关内容，有明确的项目规范，对项目落地、开发都会很顺畅  模块分层及依赖关系  系统各个模块的分层以及模块和模块间的依赖关系  接口和类定义规范  三、实践场景 GitLab/Github的高效使用  熟练git的使用 学会从Issue中寻找答案   四、衍生价值 博客书写-数据理论型  书写博客：掌握3W1H分析法\n  What 是什么  示例：NIO始于Java 1.4，提供了新的非阻塞Java IO操作API   Where 什么场景  用意是替换部分Java IO和网络相关的API   Why 为什么要使用  非阻塞IO，一个线程可以处理多个IO操作，利用率更高   How 怎么使用  NIO中的三个核心组件需要学习，分别是：Buffer缓冲区、Channel通道、Selector选择器    博客书写-实操类技巧  实操类文章三部曲：步骤 + 演示 + 总结\n   第一步：\n 描述：接下来我们执行下列xxxx实现xxxx功能 实操代码/命令演示 总结：通过上面操作，我们以及完成了xxxx，接下来    第二部： \u0026hellip;\n  小结  定时的总结自己学习的知识，真正的理解并吸收为自己的\n  频率：一周1~2篇，一周一次总结 内容：以学习内容为主 平台：开放的公共平台为主，再转发到自己的个人博客/公众号   五、总结  本篇博客的内容主要总结于网易云课堂，其中部分截图是用收集拍照比较模糊。通过网易的案例视频， 我学习到了完整的企业级分布式（微服务）架构及各个模块的划分，如业务系统，报警系统，性能监控系统，分布式配置系统等， 在未来的工作中让我有了完整的架构及技术选型参考，同时学习了博客的书写流程，希望在接下来的博客中能够加以运用，写出逻辑清晰、有 内容的博客。\n  ","id":10,"section":"posts","summary":"一、项目拆解 分布式系统 一个请求由多个系统协同完成，这种系统称为分布式系统 服务拆分：单体项目一个模块修改需要重新部署、运维整个项目，故将其拆分","tags":[" "],"title":"网易案例分析","uri":"https://PI-KA-CHU.github.io/2019/12/%E7%BD%91%E6%98%93%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/","year":"2019"},{"content":" 做时间，没有拼搏的精神，全力以赴的投入，是不可能成功的。人性是充满惰性的，没有到生死关头，是不回使尽全力来付出， 但是有和智慧的人并不是把自己逼到谷底才努力，二十遇见自己不努力的未来，将是沮丧，绝望，被众人唾弃， 若等到丧失所有资源才觉悟，那是通常年岁已长，时机也已过了。 - \u0026laquo;你要配得上自己所受的苦\u0026raquo;\n 课程学什么 提高逻辑思维   演绎推导法 示例：因果推理。如Java中提供的BIO和NIO两种网络编程方式，所以一切框架中，涉及到网络处理的，都可以用这两个知识点探究原理。\n  归纳总结法 示例：可能正确的猜想。线上10台服务器，有三台总是会没有重启，收集相关信息后，发现是运维修改监控系统配置的时候，漏掉了提高这三台机器的重启阈值。\n  归纳总结法\n  类比法\n   如何高效互动 提问的智慧  参考： https://github.com/tvvocold/How-To-Ask-Questions-The-Smart-Way   学习安排建议  坚持打卡，每天安排时间学习 每周每月进行一次总结 碎片化学习效率较低，需要合理安排时间，学习需要状态 基础不好的，坚持将示例代码都亲自敲一遍。 一精多专   作业考核  提交到GitLab  ","id":11,"section":"posts","summary":"做时间，没有拼搏的精神，全力以赴的投入，是不可能成功的。人性是充满惰性的，没有到生死关头，是不回使尽全力来付出， 但是有和智慧的人并不是把自己","tags":[],"title":"预备课（一）","uri":"https://PI-KA-CHU.github.io/2019/12/%E9%A2%84%E5%A4%87%E8%AF%BE%E4%B8%80/","year":"2019"},{"content":"一、如何基于Spring写优雅易扩展的代码 1.1 案例一 1.1.1 如何实现订单金额的计算   策略模式 实现面向接口编程，由于计算金额的方法是会增加的，比如促销方案是会变的，可以利用策略模式，将订单金额计算的方法抽象为接口，有不同的促销模式时候实现该接口（接口隔离原则），避免频繁的修改代码（开闭原则）\n  工厂模式 利用工厂方法实现方案计算方法实例的生成，（根据传入的String）利用反射获取各种金额计算方案的实现，如果是自己利用反射技术实现简单工厂的话，每次有新促销方案的话我们需要修改自定义的工厂，因为工厂需要初始化各个促销接口的实现类，为此引入下列几种方式，通过将变化的部分以配置文件的方式隔离（SPI）、Spring注入、Spring提供的工厂真正实现开闭原则。\n  Java SPI：接口服务发现机制，对接口的实现类自动完成加载。SPI机制可以避免硬编码的问题，厂商通过规定的配置规则，即使用方提供规则，提供方根据规则把自己加载到提供方的思想。在Java中通过ServiceLoader加载约定配置下的实现类，ServiceLoader\u0026lt;Driver\u0026gt; s = ServiceLoader.load(Driver.class);如：\n   MySQL的驱动加载时，在META/services中以调用者给定的完整接口名命名文件，文件内容为该接口的具体实现完整类名；    Spring中的component-scan 注解标签，会将@Controller、@Service等的标签注入到容器中，也就是Spring制定了规则，开发者根据规则去实现。   参考： https://zhuanlan.zhihu.com/p/28909673     Dubbo SPI\n  Spring实现\n @Autowired Map\u0026lt;String,Order\u0026gt; map，利用Map可以自动将所有Order接口的实现类注入 利用ApplicationContext获取bean并实现方法的调用，即利用Spring提供的工厂模式       二、如何提高工作效率 为什么我们的工作效率不高  我们掌握的方法、技能、工具不多，不知道还有更好的开发模式，一直采用传统的堆代码开发，要提高工作效率，需要多掌握更多更好的开发方法、工具和技能。  高薪人的优势是什么  能够解决更大的问题，更难的问题 相同时间解决的问题更多，绝对不是因为加班多才会薪资高   三、如何基于Spring写解耦易扩展的代码 3.1 案例分析 3.1.1 业务场景演示 常规代码流程：\n问题\n 同步调用 如何邮件发送异常，则短信服务也无法进行，订票失败 违反单一原则（在订单方法中加入了邮件和短信通知） 与不是强相关的类耦合（航班预定与邮箱、短信类） - 复用性极低 如果有新功能加入，如需要发微信、QQ，则会违反开闭原则，不易扩展   基于事件驱动的流程（观察者模式）：\n事件驱动\n 发布者：OrderService类中的预定订单方法 事件：航班预定事件，即Order（订单）对象为事件 事件通道：spring（ApplicationContext提供事件的发布通道，如publishEvent(Object o)和publishEvent(ApplicationEvent event)方法，一个用于对象事件如订单生成Order对象、一个用于应用事件如应用启动关闭事件，可以用于Spring启动的时候进行相关的初始化任务） 监听者：邮件发送类、短信发送类  Spring注解\n  @Async：异步执行注解\n  @Transactional：添加事务\n  观察者模式\n 监听者：@EventListener  @EventListener @Async //异步执行 public void handleOrderEvent(Order order){ //上面的传入对象参数即为监听的事件，该监听方法只对Order事件感兴趣 Email mail = ...; this.sendMail(email); } 发布者：在发生订单事件的方法中调到Spring提供的`ApplicationContext`的发布方法 utowired plicationContext context; blic void bookOrder(Order o){ . ntext.publishEvent(order);    四、如何基于Spring写异步、定时任务 异步方式\n 方式一：配置ApplicationEventMuticaster的Bean（基于事件的异步）  Spring提供，配置configuration的Bean，并在其中配置线程池（没有线程池配置则为同步） 缺点：一刀切（所有的事件执行都变为异步的）   方式二：@EnableAsync @Async（基于Bean的异步）  控制Bean的方法调用为异步执行（灵活），需要特定线程池则进行配置 @EnableAsync：开启SpringBoot异步模式 @Async：被标识的方法在调用的时候为异步调用，未被标识的方法为同步执行    定时任务\n @EnableScheduling：开启SpringBoot定时任务 @Schedule：在执行逻辑的Bean（@Component）的方法上注释即可实现定时任务   五、如何扩展我们的技术广度  针对事情快速应对，即想到相应的方法。需要有好的技术，想想如何达到高薪的位置，而不是单单想着拿高薪。 单机的技术使用跟分布式的使用是不同的，单机的可以充分利用Spring的高级用法，而分布式则更多的依赖于中间件，比如上面说的事件驱动编程，如果在Spring中可以利用其注解快速实现，而分布式环境中则需要利用MQ实现发布订阅模式，要能快速的想出解决方案，需要我们有足够的知识广度。\n 扩展技术广度的方法\n 参加课程   技术大牛带 项目的洗礼 去大的互联网公司 多看书、多交流探讨   ","id":12,"section":"posts","summary":"一、如何基于Spring写优雅易扩展的代码 1.1 案例一 1.1.1 如何实现订单金额的计算 策略模式 实现面向接口编程，由于计算金额的方法是会增加的，比如促销方","tags":["","spring"],"title":"Spring高级应用","uri":"https://PI-KA-CHU.github.io/2019/12/spring%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8/","year":"2019"},{"content":"一、Web安全入门 Web安全案例  1、某诈骗集团通过短信发送了钓鱼网站给我们，以下是通过一系列的手段最终追踪到用户的手段\n  User-Agent：用电脑模拟手机User-Agent登陆网站 XSS获取管理后台权限  XSS为跨站脚本攻击，获取其后台管理凭证，比如获取到其后台管理地址及后台管理凭证（cookie）   利用JSP漏洞，获取其电脑登陆过的其他网站的账号，比如获取到保存在cookie中的百度username 通过获取的百度贴吧username人肉它，搜索其发过的贴子，进而获取其个人信息，甚至是相关的联系方式等。  安全证书  https://zhuanlan.zhihu.com/p/21750494  CISP：中国 CISSP：国际化    常见的攻击手段 企业攻防对抗示例  二、Web安全实战 1、寻找邮箱信息  给你一个邮箱地址，如何找到和这个邮箱相关的更多信息\n  通过百度或者谷歌搜索邮箱账号，获取相关信息 通过Firefox Monitor查看个人数据是否泄漏  https://monitor.firefox.com/    2、查找网站后台  准备  公司的网站是？ 后台的关键字 利用搜索引擎、社工库、社交论坛等查找   搜索引擎  类别  baidu.com（过滤比较严重） google.com（语法强大，挖掘能力较好） duckduckgo.com（推荐，挖坟能力简直不能强大，把祖坟都挖出来了）   命令  site:edu.cn：包含edu.cn域名的网站 inurl:admin：网页中包含admin的网站 intitle:奖励：搜索网站标题中包含奖励的网页 filetype:pdf：只搜索指定类型的文档的网页 其他谷歌命令参考： https://www.zhihu.com/question/20161362      金句  始于兴趣，成于坚持  ","id":13,"section":"posts","summary":"一、Web安全入门 Web安全案例 1、某诈骗集团通过短信发送了钓鱼网站给我们，以下是通过一系列的手段最终追踪到用户的手段 User-Agent：","tags":["安全"],"title":"Web安全入门","uri":"https://PI-KA-CHU.github.io/2019/12/web%E5%AE%89%E5%85%A8%E5%85%A5%E9%97%A8/","year":"2019"},{"content":"一、前言  Docker Compose和Dockerfile的区别： Dockerfile主要是记录了一个镜像的构建过程，可以达到快速构建项目的目的，而Docker Compose则是构建项目（服务）的过程，一个服务中可以有多个容器。docker-compose.yml不提供镜像的构建信息，而是通过Dockerfile进行获取（需要build的情况）。\n  二、Docker Compose的安装 # 下载安装 curl -L https://get.daocloud.io/docker/compose/releases/download/1.20.0/docker-compose-`uname -s`-`uname -m` \u0026gt; /usr/local/bin/docker-compose # 添加权限 chmod +x /usr/local/bin/docker-compose #查看版本 docker-compose version  三、搭建项目环境 3.1、项目目录结构\n docker目录下每个文件夹对应一个服务，并且有各自的Dockerfile（用于该服务的镜像构建）及相关挂载的数据，docker-compose.yml则控制各个服务的镜像构建，数据挂载，命令启动及启动流程等，通过docker-compose命令实现快速部署启动。  3.2、镜像文件详情\na、Dockerfile:\napp: FROM maven:3.5-jdk-8 mysql： FROM mysql/mysql-server:5.7 redis： FROM redis:5.0.5 rocketmq：未整合 b、docker-compose.yml：\nversion: \u0026#39;3\u0026#39; services: nginx: container_name: bigdata-nginx image: nginx:1.16.1 restart: always ports: - 80:80 - 443:443 volumes: - ./nginx/conf.d:/etc/nginx/conf.d - /tmp/logs/nginx:/var/log/nginx mysql: container_name: bigdata-mysql build: mysql # command: --default-authentication-plugin=mysql_native_password environment: MYSQL_DATABASE: multisource_bigdata MYSQL_ROOT_PASSWORD: root MYSQL_ROOT_HOST: \u0026#39;%\u0026#39; MYSQL_USER: \u0026#39;hive\u0026#39; MYSQL_PASS: \u0026#39;hive\u0026#39; TZ: Asia/Shanghai ports: - 3306:3306 volumes: - ./mysql/mysql_data:/var/lib/mysql restart: always redis: container_name: bigdata-redis build: redis working_dir: /redis volumes: - ./redis/redis.conf:/etc/redis/redis.conf restart: always ports: - 6379:6379 command: redis-server /etc/redis/redis.conf app: container_name: bigdata-app build: app volumes: - ././:/app - ~/.m2:/root/.m2 - /tmp/logs/app:/usr/local/logs working_dir: /app restart: always expose: - 8080 command: mvn clean spring-boot:run -Pprod -Dmaven.test.skip=true depends_on: - nginx - mysql - redis c、/nginx/conf.d/app.conf：nginx的挂载文件\nserver { listen 80; charset utf-8; access_log off; location / { proxy_pass http://app:8080; proxy_set_header Host $host:$server_port; proxy_set_header X-Forwarded-Host $server_name; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } location /static { access_log off; expires 30d; alias /app/static; } } d、/mysql/mysql_data：mysql数据库数据的挂载\ne、/redis/redis.conf：redis配置文件的挂载\n 比如我们先在本地配置redis，并成功整合项目后，可以将原来的配置文件挂载到容器的redis配置文件下，从而快速实现整合   四、快速部署并启动项目 1、cd 进入项目 /src/docker 目录 2、执行 `docker-compose up` (会出现相关的启动图标。如redis、springboot等)  3、访问项目的index界面，测试nginx： - http://192.168.125.131/index.html - （resource/static/index.html） 4、访问接口，测试数据库连接及redis缓存（接口实现缓存的情况） - http://192.168.125.131/api/test/getUser 5、容器内部访问（非必须） # 查看容器Id docker ps -a # 进入容器内部 docker exec -it 容器Id bash  五、问题与解决： 问题一：执行docker-compose up后异常\ncontainer_linux.go:345: starting container process caused \u0026#34;exec: \\\u0026#34;mvn\\\u0026#34;: executable file not found in $PATH\u0026#34;: unknown 解决：\n 参考：https://medium.com/p/baf30968163e/responses/show  构建的docker容器中没有maven环境，无法通过maven进行打包，将web项目的镜像改为 maven:3.5-jdk-8 ，此镜像包含了jdk1.8 + maven3.5，即可正常运行  问题二：修改mysql密码不生效问题\n解决：\n重启mysql的docker服务 问题三：navicat无法正常连接docker数据库\n解决：\n 本人问题：mysql的映射端口弄错了，搞了半天，坑爹啊 其他参考：https://blog.csdn.net/liqz666/article/details/82225575  问题四：无法正常连接服务，如redis、mysql等\n解决：\n  参考：https://blog.csdn.net/harris135/article/details/74167910\n  方案一：将相关端口添加到防火墙开放：\n  # 添加开放端口 firewall-cmd --zone=public --add-port=80/tcp --permanent # 重启防火墙（不重启可能无法生效） systemctl restart firewalld.service  方案二：关闭防火墙：systemctl stop firewalld.service   六、参考  http://www.ityouknow.com/docker/2018/03/22/docker-compose.html https://www.jianshu.com/p/efd70ad53602 http://www.ityouknow.com/springboot/2018/04/02/docker-favorites.html http://www.ityouknow.com/springboot/2018/03/28/dockercompose-springboot-mysql-nginx.html  ","id":14,"section":"posts","summary":"一、前言 Docker Compose和Dockerfile的区别： Dockerfile主要是记录了一个镜像的构建过程，可以达到快速构建项目的目的，而Do","tags":["docker","部署工具"],"title":"Docker实战：3、三剑客之Docker Compose","uri":"https://PI-KA-CHU.github.io/2019/08/docker%E5%AE%9E%E6%88%983%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-compose/","year":"2019"},{"content":"一、概念  镜像的定制实际就是定制每一层所添加的配置、文件。如果我们把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么就可以实现快速的搭建一个服务环境并快速配置环境相关变量，而不是靠手动的搭建及命令行操作，这个脚本就是Dockerfile。\n 1.1、文件格式：(四部分)  基础镜像信息 维护者信息 镜像操作指令 容器启动执行指令  # 1、第一行必须指定 基础镜像信息 FROM ubuntu # 2、维护者信息 MAINTAINER docker_user docker_user@email.com # 3、镜像操作指令 RUN echo \u0026#34;deb http://archive.ubuntu.com/ubuntu/ raring main universe\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list RUN apt-get update \u0026amp;\u0026amp; apt-get install -y nginx RUN echo \u0026#34;\\ndaemon off;\u0026#34; \u0026gt;\u0026gt; /etc/nginx/nginx.conf # 4、容器启动执行指令 CMD /usr/sbin/nginx  1.2、常用命令 镜像构建：(-f指定Dockerfile路径，.表示当前路径)\ndocker build . docker build -f /path/to/a/Dockerfile . 镜像标签：(-t构建 仓库/标签，可以支持多个)\ndocker build -t nginx/v3 . docker build -t nginx/v3:1.0.2 -t nginx/v3:latest .  1.3、指令格式及说明 (1)、FROM（指定基础image）\nFROM \u0026lt;image\u0026gt; # 指定版本 FROM \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; (2)、MAINTAINER（用来指定镜像创建者信息）\nMAINTAINER \u0026lt;name\u0026gt; (3)、RUN（安装软件用）\n 构建指令，RUN可以运行任何被基础image支持的命令。如基础image选择了ubuntu，那么软件管理部分只能使用ubuntu的命令。 RUN命令将在当前image中执行任意合法命令并提交执行结果。命令执行提交后，就会自动执行Dockerfile中的下一个指令。 RUN 指令缓存不会在下个命令执行时自动失效。比如 RUN apt-get dist-upgrade -y 的缓存就可能被用于下一个指令. \u0026ndash;no-cache 标志可以被用于强制取消缓存使用。  RUN \u0026lt;command\u0026gt; (the command is run in a shell - /bin/sh -c) RUN [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34; ... ] (exec form) (4)、CMD（设置container启动时执行的操作）\n 设置指令，用于container启动时指定的操作。该操作可以是执行自定义脚本，也可以是执行系统命令。该指令只能在文件中存在一次，如果有多个，则只执行最后一条。  # （三种格式） CMD [\u0026#34;executable\u0026#34;,\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] (like an exec, this is the preferred form) CMD command param1 param2 (as a shell) # 当Dockerfile指定了ENTRYPOINT，那么使用下面的格式： CMD [\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] (作为ENTRYPOINT的默认参数) (5)、ENTRYPOINT（设置container启动时执行的操作）\n# 单独使用 ENTRYPOINT [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;] (like an exec, the preferred form) ENTRYPOINT command param1 param2 (as a shell) # 于CMD结合使用 - 2种情况 # a、CMD为完整的可执行命令，则指令将不会被执行，只有ENTRYPOINT指令被执行（相互覆盖） CMD echo “Hello, World!” ENTRYPOINT ls -l # b、CMD是参数部分，ENTRYPOINT指令只能使用JSON方式指定执行命令，而不能指定参数 FROM ubuntu CMD [\u0026#34;-l\u0026#34;] ENTRYPOINT [\u0026#34;/usr/bin/ls\u0026#34;] (6)、USER（设置container容器的用户）\n 设置指令，设置启动容器的用户，默认是root用户。  # 指定memcached的运行用户 ENTRYPOINT [\u0026#34;memcached\u0026#34;] USER daemon 或 ENTRYPOINT [\u0026#34;memcached\u0026#34;, \u0026#34;-u\u0026#34;, \u0026#34;daemon\u0026#34;] (7)、EXPOSE（指定容器需要映射到宿主机器的端口）\n  设置指令，该指令会将容器中的端口映射成宿主机器中的某个端口。当你需要访问容器的时候，可以不是用容器的IP地址而是使用宿主机器的IP地址和映射后的端口。\n 一、在Dockerfile使用EXPOSE设置需要映射的容器端口 二、在运行容器的时候指定-p选项加上EXPOSE设置的端口    # 注：若无指定宿主主机的映射端口，则会被随机映射 # 映射一个端口 EXPOSE port1 # 相应的运行容器使用的命令 docker run -p port1 image # 映射多个端口 EXPOSE port1 port2 port3 # 相应的运行容器使用的命令（随机） docker run -p port1 -p port2 -p port3 image # 指定需要映射到宿主机器上的某个端口号 （指定） docker run -p host_port1:port1 -p host_port2:port2 -p host_port3:port3 image  端口映射是docker比较重要的一个功能，原因在于我们每次运行容器的时候容器的IP地址不能指定，而是在桥接网卡的地址范围内随机生成的。宿主机器的IP地址是固定的，我们可以将容器的端口的映射到宿主机器上的一个端口，免去每次访问容器中的某个服务时都要查看容器的IP的地址。对于一个运行的容器，可以使用docker port + 容器中需要映射的端口和容器的ID来查看该端口号在宿主机器上的映射端口。  (8)、ENV（用于设置环境变量）\n ENV设置的环境变量，可以使用docker inspect命令来查看。同时还可以使用docker run --env \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt;来修改环境变量。  ENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; # 例: ENV JAVA_HOME /path/to/java/dirent (9)、ADD（从src复制文件到container的dest路径）\n 所有拷贝到container中的文件和文件夹权限为0755，uid和gid为0 如果文件是可识别的压缩格式，则docker会自动解压缩（注意压缩格式） 如果是文件且中不使用斜杠结束，则会将视为文件，的内容会写入 如果是文件且中使用斜杠结束，则会文件拷贝到目录下。  # \u0026lt;src\u0026gt; 是相对被构建的源目录的相对路径，可以是文件或目录的路径，也可以是一个远程的文件url; # \u0026lt;dest\u0026gt; 是container中的绝对路径 ADD \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; (10)、VOLUME (指定挂载点)\n 创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。 Volume设置指令，使容器中的一个目录具有持久化存储数据的功能，该目录可以被容器本身使用，也可以共享给其他容器使用。我们知道容器使用的是AUFS，这种文件系统不能持久化数据，当容器关闭后，所有的更改都会丢失。当容器中的应用有持久化数据的需求时可以在Dockerfile中使用该指令。如MySQL数据、日志数据等如果不挂载到宿主机器，重启容器后数据都会消失。  VOLUME [\u0026#34;\u0026lt;mountpoint\u0026gt;\u0026#34;] # 例： VOLUME [\u0026#34;/tmp/data\u0026#34;]  运行通过该Dockerfile生成image的容器，/tmp/data目录中的数据在容器关闭后，里面的数据还存在。例如另一个容器也有持久化数据的需求，且想使用上面容器共享的/tmp/data目录，那么可以运行下面的命令启动一个容器：  # container1为第一个容器的ID，image2为第二个容器运行image的名字。 docker run -t -i -rm -volumes-from container1 image2 bash (11)、WORKDIR（切换目录）\n 设置指令，可以多次切换(相当于cd命令)，对RUN,CMD,ENTRYPOINT生效。  WORKDIR /path/to/workdir # 例:在 /p1/p2 下执行 vim a.txt WORKDIR /p1 WORKDIR p2 RUN vim a.txt (12)、ONBUILD（在子镜像中执行）\n ONBUILD 指定的命令在构建镜像时并不执行，而是在它的子镜像中执行。  ONBUILD \u0026lt;Dockerfile关键字\u0026gt; (13)、COPY(复制本地主机的src文件为container的dest)\n 复制本地主机的src文件（为Dockerfile所在目录的相对路径、文件或目录 ）到container的dest。目标路径不存在时，会自动创建。  # 当使用本地目录为源目录时，推荐使用COPY COPY \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; (14)、ARG(设置构建镜像时变量)\n ARG指令在Docker1.9版本才加入的新指令，ARG 定义的变量只在建立 image 时有效，建立完成后变量就失效消失  ARG \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; (15)、LABEL(定义标签)\n 定义一个 image 标签 Owner，并赋值，其值为变量 Name 的值。  LABEL Owner=$Name  二、实战（简单版 - nginx为例） 2.1、创建Dockerfile文件\n FROM：选择指定基础镜像 RUN：执行指令  FROM nginx RUN echo \u0026#39;\u0026lt;h1\u0026gt;Hello, Docker!\u0026lt;/h1\u0026gt;\u0026#39; \u0026gt; /usr/share/nginx/html/index.html 2.2、运行Dockerfile构建镜像\n# 构建镜像 docker build -t nginx:v1 . # 查看镜像 docker images 2.3、启动镜像容器\n# --name 为容器名，-d为后台运行，-p为指定映射端口:端口，nginx:v1为镜像:标签 docker run --name docker_nginx_v1 -d -p 80:80 nginx:v1   问题一：主机无法访问虚拟机nginx\n  解决：防火墙配置问题\n https://blog.csdn.net/harris135/article/details/74167910    问题二：无法正常启动docker实例\n  (iptables failed: iptables --wait -t nat -A DOCKER -p tcp -d 0/0 --dport 8801 -j DNAT --to-destination 172.17.0.3:80 ! -i docker0: iptables: No chain/target/match by that name.  解决：重启docker  systemctl restart docker  三、修改容器内容  在容器启动后，需要对里面的文件进行修改，可以使用docker exec -it xx bash命令再次进行修改（此处修改nginx的主页面显示）  docker exec -it docker_nginx_v1 bash root@3729b97e8226: echo \u0026#39;\u0026lt;h1\u0026gt;Hello, Docker neo!\u0026lt;/h1\u0026gt;\u0026#39; \u0026gt; /usr/share/nginx/html/index.html root@3729b97e8226: exit exit  （容器的存储层）修改后，可以通过 docker diff 命令看到具体的改动  docker diff docker_nginx_v1  四、SpringBoot整合Docker  在已经搭建好SpringBoot的基础上  4.1、项目docker支持准备 a、在pom.xml文件中添加docker支持\n\u0026lt;properties\u0026gt; \u0026lt;docker.image.prefix\u0026gt;springboot-docker\u0026lt;/docker.image.prefix\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;!-- docker插件支持 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.spotify\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;docker-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- 注意artifactId不能包含大写字母 --\u0026gt; \u0026lt;imageName\u0026gt;${docker.image.prefix}/${project.artifactId}\u0026lt;/imageName\u0026gt; \u0026lt;dockerDirectory\u0026gt;src/main/docker\u0026lt;/dockerDirectory\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;targetPath\u0026gt;/\u0026lt;/targetPath\u0026gt; \u0026lt;directory\u0026gt;${project.build.directory}\u0026lt;/directory\u0026gt; \u0026lt;include\u0026gt;${project.build.finalName}.jar\u0026lt;/include\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; b、在项目的src/main/docker目录下创建Dockerfile文件：\nFROM openjdk:8-jdk-alpine VOLUME /tmp ADD bigdata_presentation.jar app.jar ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;,\u0026#34;-jar\u0026#34;,\u0026#34;/app.jar\u0026#34;] 参数解释：\n  FROM：表示使用 Jdk8 环境 为基础镜像，如果镜像不是本地的会从 DockerHub 进行下载\n  VOLIME：VOLUME 指向了一个/tmp的目录，由于 Spring Boot 使用内置的Tomcat容器，Tomcat 默认使用/tmp作为工作目录。这个命令的效果是：在宿主机的/var/lib/docker目录下创建一个临时文件并把它链接到容器中的/tmp目录\n  ADD：拷贝文件并且重命名\n  ENTRYPOINT：为了缩短 Tomcat 的启动时间，添加java.security.egd的系统属性指向/dev/urandom作为 ENTRYPOINT\n   4.2、项目打包环境构建 a、dcker安装\n请参考上一篇 b、JDK环境安装\n# 安装 yum -y install java-1.8.0-openjdk* # 修改环境变量（/etc/profile） export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-0.b14.el7_4.x86_64 export PATH=$PATH:$JAVA_HOME/bin # 使环境变量生效 source /etc/profile # 检查是否安装成功 java -version c、Maven环境安装\n# 下载压缩包 wget http://mirrors.shu.edu.cn/apache/maven/maven-3/3.1.1/binaries/apache-maven-3.1.1-bin.tar.gz # 解压 tar zxvf apache-maven-3.5.2-bin.tar.gz # 移动 mv apache-maven-3.5.2 /usr/local/maven3 # 修改环境变量（/etc/profile） MAVEN_HOME=/usr/local/maven3 export MAVEN_HOME export PATH=${PATH}:${MAVEN_HOME}/bin # 使环境变量生效 source /etc/profile # 修改maven为阿里云镜像仓库，否则镜像构建的时候会很慢 自行百度 # 检测是否安装成功 mvn -version  4.3、构建SpringBoot的docker镜像  将SpringBoot项目拷贝到服务器中，并通过命令行进入其项目目录\n a、测试SpringBoot环境是否正常\n# 打包 mvn clean package -Dmaven.skip.test=true # 启动 java -jar target/spring-boot-docker-1.0.jar b、SpringBoot正常则进行其docker镜像的构建\n# 构建docker镜像 mvn package docker:build # 查看构建成功的镜像 docker images # 运行镜像容器 docker run -p 8080:8080 -t springboot-docker/bigdata_presentation # 查看正运行的容器（-a 可查看所有） docker ps # 构建成功如下 ... Building image springboot-docker/bigdata_presentation Step 1/4 : FROM openjdk:8-jdk-alpine ---\u0026gt; a3562aa0b991 Step 2/4 : VOLUME /tmp ---\u0026gt; Using cache ---\u0026gt; b7ebabcea704 Step 3/4 : ADD bigdata_presentation.jar app.jar ---\u0026gt; ed2bf2401a6f Step 4/4 : ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;,\u0026#34;-jar\u0026#34;,\u0026#34;/app.jar\u0026#34;] ---\u0026gt; Running in 718ca03aaa8f Removing intermediate container 718ca03aaa8f ---\u0026gt; 4f1bedaaf992 ProgressMessage{id=null, status=null, stream=null, error=null, progress=null, progressDetail=null} Successfully built 4f1bedaaf992 Successfully tagged springboot-docker/bigdata_presentation:latest [INFO] Built springboot-docker/bigdata_presentation [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 18.061s [INFO] Finished at: Tue Aug 20 11:54:44 CST 2019 [INFO] Final Memory: 43M/103M [INFO] ------------------------------------------------------------------------ 访问首页：  4.4、问题及解决 问题一：springBoot执行mvn命令异常，Spring Boot:jar中没有主清单属性\n解决：\n 参考：https://blog.csdn.net/u010429286/article/details/79085212\n 问题二（坑爹）：mvn构建docker镜像时候异常\n I/O exception (java.io.IOException) caught when processing request to {}-\u0026gt;unix://localhost:80: Connection reset by peer\n 解决：\n pom文件中的artifactId不能为大写的，否则会出现上面的异常（坑爹） 参考：https://blog.csdn.net/EasternUnbeaten/article/details/79825851\n 问题三：如何自定义maven打包后的文件名\n解决：\n在pom文件的\u0026lt;build\u0026gt;\u0026lt;/build\u0026gt;中加入\u0026lt;finalName\u0026gt;自定义文件名\u0026lt;/finalName\u0026gt;  五、参考  http://www.ityouknow.com/docker/2018/03/12/docker-use-dockerfile.html http://www.ityouknow.com/docker/2018/03/15/docker-dockerfile-command-introduction.html https://www.jianshu.com/p/cbce69c7a52f（好文） https://www.cnblogs.com/ityouknow/p/8599093.html https://www.jianshu.com/p/efd70ad53602  ","id":15,"section":"posts","summary":"一、概念 镜像的定制实际就是定制每一层所添加的配置、文件。如果我们把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜","tags":["docker","部署工具"],"title":"Docker实战：2、Dockerfile构建镜像","uri":"https://PI-KA-CHU.github.io/2019/08/docker%E5%AE%9E%E6%88%982dockerfile%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F/","year":"2019"},{"content":"一、Docker安装（Centos7）  1、Docker要求Centos内核要高于3.10，所以首先应该检查是否满足要求（如果是本地虚拟机，需要保证可以联网，可以通过ping检测）  uname -r  2、使用 root 权限登录 Centos。确保 yum 包更新到最新。  sudo yum update  3、卸载旧版本(如果安装过旧版本的话)  sudo yum remove docker docker-common docker-selinux docker-engine  4、安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的  sudo yum install -y yum-utils device-mapper-persistent-data lvm2  5、设置yum源  # 阿里云版（推荐） sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # 官方版（国内速度慢） sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo  6、可以查看所有仓库中所有docker版本，并选择特定版本安装  yum list docker-ce --showduplicates | sort -r  7、更新 yum 软件源缓存，并安装 docker  # 更新 yum 软件源缓存 sudo yum makecache fast # 默认安装最新版本 sudo yum install docker-ce # 指定版本下载，例如：sudo yum install docker-ce-17.12.0.ce sudo yum install \u0026lt;FQPN\u0026gt;  8、启动并加入开机自启  sudo systemctl start docker sudo systemctl enable docker  9、验证安装是否成功  docker version  10、Docker镜像加速器  添加镜像加上文件\n# 在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件） { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34; ] } 重新服务\nsudo systemctl restart docker  二、Docker常见命令（以Redis为例）  镜像拉取  docker pull redis:latest  获取本地镜像列表  docker image ls / docker images  运行镜像  docker run 镜像名称 \u0026amp; # 注: \u0026amp;表示后台运行  删除本地镜像  docker image rm \u0026lt;镜像\u0026gt; # 注：\u0026lt;镜像\u0026gt;可以是 镜像短 ID、镜像长 ID、镜像名 或者 镜像摘要  问题：删除的时候可能会报镜像被容器引用的异常： Error response from daemon: conflict: unable to remove repository reference \u0026ldquo;hello-world\u0026rdquo; (must \u0026gt; \u0026gt; force) - container d4f4b4789f90 is using its referenced image fce289e99eb9\n解决：输入命令 docker ps -a 显示当前所有容器（包括未运行的），然后使用 docker rm 容器ID \u0026gt; 将其删除后即可正常删除镜像。\n  查看容器  docker ps -a # 注：-a表示查看所有容器，包括未运行的，不加只显示当前运行的容器  容器启动、重启及停止  docker start container_name/container_id docker stop container_name/container_id docker restart container_name/container_id  删除容器  docker rm container_name/container_id  删除所有停止的容器  docker rm $(docker ps -a -q)  查看docker信息  docker info  查找Docker Hub上的nginx镜像  docker search nginx   三、参考  https://www.cnblogs.com/yufeng218/p/8370670.html https://www.funtl.com/zh/docs-docker/CentOS-%E5%AE%89%E8%A3%85-Docker.html#%E4%BD%BF%E7%94%A8-yum-%E5%AE%89%E8%A3%85 镜像加速： https://blog.csdn.net/qq_37495786/article/details/83246421 redis镜像： https://blog.csdn.net/cookily_liangzai/article/details/80726163 Docker命令： https://snailclimb.top/JavaGuide/#/tools/Docker-Image?id=%E4%B8%80-docker-%E4%B8%8B%E8%BD%BD%E9%95%9C%E5%83%8F http://www.ityouknow.com/docker/2018/03/07/docker-introduction.html  ","id":16,"section":"posts","summary":"一、Docker安装（Centos7） 1、Docker要求Centos内核要高于3.10，所以首先应该检查是否满足要求（如果是本地虚拟机，需","tags":["docker","部署工具"],"title":"Docker实战：1、安装及常见命令","uri":"https://PI-KA-CHU.github.io/2019/08/docker%E5%AE%9E%E6%88%981%E5%AE%89%E8%A3%85%E5%8F%8A%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4/","year":"2019"},{"content":"一、Oozie的定时任务方式 1.1 官方定义 1.2 corntab方式  二、编写定时任务 2.1 代码编写 job.properties\nnameNode=hdfs://hh:8020 jobTracker=hh:8032 queueName=qcpj_access_log_queue examplesRoot=accessLogAnalyze # workflow参数为 oozie.wf.application.path oozie.coord.application.path=${nameNode}/oozie/workflows/qcpj/${examplesRoot} start=2019-08-05T23:00+0800 end=2099-08-05T23:30+0800 workflowAppUri=${nameNode}/oozie/workflows/qcpj/${examplesRoot} workflow.xml\n\u0026lt;workflow-app name=\u0026#39;qcpj_accessLog_analyze\u0026#39; xmlns=\u0026#34;uri:oozie:workflow:0.3\u0026#34;\u0026gt; \u0026lt;start to=\u0026#34;accessLogAnalyze\u0026#34;/\u0026gt; \u0026lt;!-- 分析accessLog日志数据 --\u0026gt; \u0026lt;action name=\u0026#34;accessLogAnalyze\u0026#34;\u0026gt; \u0026lt;java\u0026gt; \u0026lt;job-tracker\u0026gt;${jobTracker}\u0026lt;/job-tracker\u0026gt; \u0026lt;name-node\u0026gt;${nameNode}\u0026lt;/name-node\u0026gt; \u0026lt;main-class\u0026gt;com.bnuz.qcpj.mr.accessLogAnalyze.AccessLogRunner\u0026lt;/main-class\u0026gt; \u0026lt;/java\u0026gt; \u0026lt;ok to=\u0026#34;eventCountRunner\u0026#34;/\u0026gt; \u0026lt;error to=\u0026#34;kill\u0026#34;/\u0026gt; \u0026lt;/action\u0026gt; \u0026lt;!-- 统计相关事件数量 --\u0026gt; \u0026lt;action name=\u0026#34;eventCountRunner\u0026#34;\u0026gt; \u0026lt;java\u0026gt; \u0026lt;job-tracker\u0026gt;${jobTracker}\u0026lt;/job-tracker\u0026gt; \u0026lt;name-node\u0026gt;${nameNode}\u0026lt;/name-node\u0026gt; \u0026lt;main-class\u0026gt;com.bnuz.qcpj.mr.accessLogEventCount.EventCountRunner\u0026lt;/main-class\u0026gt; \u0026lt;/java\u0026gt; \u0026lt;ok to=\u0026#34;end\u0026#34;/\u0026gt; \u0026lt;error to=\u0026#34;kill\u0026#34;/\u0026gt; \u0026lt;/action\u0026gt; \u0026lt;kill name=\u0026#34;kill\u0026#34;\u0026gt; \u0026lt;message\u0026gt;mapreduce failed, error message:${wf:errorMessage(wf:lastErrorNode())}\u0026lt;/message\u0026gt; \u0026lt;/kill\u0026gt; \u0026lt;end name=\u0026#34;end\u0026#34;/\u0026gt; \u0026lt;/workflow-app\u0026gt; coordinator.xml\n 修改时区timezone 修改版本xmlns  \u0026lt;coordinator-app name=\u0026#34;qcpj_access_log_analyze_coor\u0026#34; frequency=\u0026#34;${coord:minutes(5)}\u0026#34; start=\u0026#34;${start}\u0026#34; end=\u0026#34;${end}\u0026#34; timezone=\u0026#34;GMT+0800\u0026#34; xmlns=\u0026#34;uri:oozie:coordinator:0.2\u0026#34;\u0026gt; \u0026lt;action\u0026gt; \u0026lt;workflow\u0026gt; \u0026lt;app-path\u0026gt;${workflowAppUri}\u0026lt;/app-path\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;jobTracker\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;${jobTracker}\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;nameNode\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;${nameNode}\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;queueName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;${queueName}\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/workflow\u0026gt; \u0026lt;/action\u0026gt; \u0026lt;/coordinator-app\u0026gt; 2.2 任务操作命令 （以下命令在任务目录下执行，作业Id可以在Oozie Web主页中查看）\n# 提交任务（提交后为PRE状态） oozie job -oozie http://hh:11000/oozie -config job.properties -submit # 开始任务(指定jobId，PRE -\u0026gt; RUN) oozie job -oozie http://hh:11000/oozie -start 14-20090525161321-oozie-joe # 暂停任务（RUN -\u0026gt; SUSPEND） oozie job -oozie http://hh:11000/oozie -suspend 14-20090525161321-oozie-joe # 恢复任务（SUSPEND -\u0026gt; RUN） oozie job -oozie http://hh:11000/oozie -resume 14-20090525161321-oozie-joe # 重新运行工作流（仅限工作流） oozie job -oozie http://hh:11000/oozie -config job.properties -rerun 14-20090525161321-oozie-joe -D oozie.wf.rerun.failnodes=false # 重新运行定时任务（action表示如下图 future-1） oozie job -oozie http://hh:11000/oozie -config job.properties -rerun 14-20090525161321-oozie-joe -refresh -action 1-4 # 提交并启动任务（RUN状态） oozie job -oozie http://hh:11000/oozie -config job.properties -run # 杀死指定任务（KILL状态） oozie job -oozie http://hh:11000/oozie -kill 0000358-190502173832787-oozie-hado-C # 查看工作流信息 oozie job -oozie http://hh:11000/oozie -config job.properties -info 14-20090525161321-oozie-joe # 查看工作流日志 oozie job -oozie http://hh:11000/oozie -config job.properties -log 14-20090525161321-oozie-joe # future-1\n 三、参考  https://www.cnblogs.com/30go/p/8391328.html 定时任务： https://www.cnblogs.com/cenzhongman/p/7259226.html 官方文档： https://oozie.apache.org/docs/4.0.0/DG_CommandLineTool.html  ","id":17,"section":"posts","summary":"一、Oozie的定时任务方式 1.1 官方定义 1.2 corntab方式 二、编写定时任务 2.1 代码编写 job.properties nameNode=hdfs://hh:8020 jobTracker=hh:8032 queueName=qcpj_access_log_queue examplesRoot=accessLogAnalyze # workflow参数为 oozie.wf.application.path oozie.coord.application.path=${nameNode}/oozie/workflows/qcpj/${examplesRoot} start=2019-08-05T23:00+0800 end=2099-08-05T23:30+0800 workflowAppUri=${nameNode}/oozie/workflows/qcpj/${examplesRoot} workflow.xml \u0026lt;workflow-app name=\u0026#39;qcpj_accessLog_analyze\u0026#39; xmlns=\u0026#34;uri:oozie:workflow:0.3\u0026#34;\u0026gt; \u0026lt;start to=\u0026#34;accessLogAnalyze\u0026#34;/\u0026gt;","tags":["大数据"],"title":"Oozie学习（定时任务）","uri":"https://PI-KA-CHU.github.io/2019/08/oozie%E5%AD%A6%E4%B9%A0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/","year":"2019"},{"content":"一、MySQL事务隔离级别 1.1、未提交读（read uncommitted）  特点：事务A执行修改语句，但是事务未结束，此时事务B可以读取事务A的修改语句的结果（此时数据库尚未修改） 问题：出现脏读现象，如A回滚了，但是B读取到的是A修改的数据。  1.2、已提交读（read committed ）  特点：事务A执行修改语句且提交事务后，事务B才可以读取。（SQLServer 、Oracle的默认隔离级别） 问题：出现不可重复读问题，如A执行事务中读取了B事务（已提交）修改的结果，然后在A事务还没结束的时候，B事务再次修改该属性，A事务再次查询时得到了不同的结果。  1.3、可重复读（repeatable read ）  特点：事务A读取事务B的数据，事务B即使在事务A未结束前再次修改，事务A读取的数据仍然是第一次读取到的数据（快照）。（MySQL默认隔离级别） 问题：出现幻读（插入数据时）问题，如事务A提交了修改某范围内表数据的事务，但是此时事务B插入了某条数据，事务A再次读取时，会出现幻行现象（幻读）。 解决：加上检索范围锁，锁为只可读。  1.4、串行化（serializable）  特点：当启动 serializable 隔离级别时，其他事务对表的写操作都将被挂起，故不会产生不可重复读、幻读、脏读的问题。但是由于同步化的特性，其性能较差，一般不使用。   二、Spring的事务管理 2.1、Spring事务隔离（5种，如上面的4种 + 1种默认，其中的默认隔离是指采用数据库的隔离级别） 2.2、Spring的事务传播行为（7种）   事务的传播指两个事务方法之间的调用行为\n 支持当前事务的：  PROPAGATION_REQUIRED：存在事务在加入、不存在则开启 PROPAGATION_SUPPORTS：存在事务则加入 PROPAGATION_MANDATORY：存在事务则加入、不存在则抛出异常   不支持当前事务的：  PROPAGATION_REQUIRES_NEW：存在事务则挂起并开启新事务 PROPAGATION_NOT_SUPPORTED：存在事务则被挂起 PROPAGATION_NEVER：存在事务则抛出异常 PROPAGATION_NESTED：不太明白        2.3、Spring事务管理（1种编程式事务 + 3中声明式事务）  Spring事务管理方法：  编程式事务：一般不用  1、在XML中配置事务管理相关的Bean，并配置注入模板中 2、在代码中调用注入好的模板的beginTransaction()、commit()、rollback()等事务管理相关的方法   声明式事务  基于TransactionProxyFactoryBean的方式：很少用 基于AspectJ的声明式事务：常用  XML方式：通过XML配置（#63） 注解方式：通过在业务方法上加@Transactional实现         三、参考   MySQL事务隔离\n https://baijiahao.baidu.com/s?id=1629344395894429251\u0026amp;wfr=spider\u0026amp;for=pc https://www.jianshu.com/p/4e3edbedb9a8 https://blog.csdn.net/qq_35433593/article/details/86094028    Spring事务管理\n https://blog.csdn.net/liuwenbiao1203/article/details/52439835 事务传播：https://blog.csdn.net/weixin_39625809/article/details/80707695    ","id":18,"section":"posts","summary":"一、MySQL事务隔离级别 1.1、未提交读（read uncommitted） 特点：事务A执行修改语句，但是事务未结束，此时事务B可以读取事务","tags":["","mysql","spring"],"title":"MySQL事务隔离级别及Spring事务管理","uri":"https://PI-KA-CHU.github.io/2019/08/mysql%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%8F%8Aspring%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86/","year":"2019"},{"content":"一、oozie组件  Workflow：工作流（流程图）  常用结点：  控制流结点（Control Flow Nodes）  start、end、kill、分支、合并等   动作结点（Action Nodes）     Coordinator：定时器 Bundle Job：绑定多个Coordinator  二、环境搭建   学习网站： https://www.bilibili.com/video/av35357845/?p=4\u0026amp;spm_id_from=333.788.b_6d756c74695f70616765.4\n  下面记录的是一些零碎的过程\n 将依赖的Lib上传到Hdfs中 创建Oozie.sql文件，运行该文件自动生成表（可以通过Navicat查看） 启动Oozie bin/oozied.sh start jps -l：查看运行的Java类，其中Bootstrap表明Oozie启动成功    三、Oozie shell的编写及启动：  工作流程  编写job.properties及workflow.xml 将文件上传到Haoop的HDFS中 通过命令执行oozie任务 查看oozie的执行页面：http://hh:11000/oozie/     文件编写：\n job.poperties配置：  指定NameNode（HDFS）和JobTracker（Yarn） 指定workflow在HDFS中的路径    nameNode=hdfs://hh:8020 # hdfs地址 jobTracker=hh:8032 # yarn地址 queueName=default examplesRoot=shell oozie.wf.application.path=${nameNode}/oozie/workflows/example/oozie-example/apps/${examplesRoot}/ # workflow上传到HDFS后的地址  workflow.xml配置（官方例子）：  注意下面配置中，xmlns版本与oozie-site.xml的版本需要一致，否则会报无法识别的错误    \u0026lt;workflow-app xmlns=\u0026#34;uri:oozie:workflow:0.3\u0026#34; name=\u0026#34;shell-wf\u0026#34;\u0026gt; \u0026lt;start to=\u0026#34;shell-node\u0026#34;/\u0026gt; \u0026lt;action name=\u0026#34;shell-node\u0026#34;\u0026gt; \u0026lt;shell xmlns=\u0026#34;uri:oozie:shell-action:0.1\u0026#34;\u0026gt; \u0026lt;job-tracker\u0026gt;${jobTracker}\u0026lt;/job-tracker\u0026gt; \u0026lt;name-node\u0026gt;${nameNode}\u0026lt;/name-node\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapred.job.queue.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;${queueName}\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;exec\u0026gt;echo\u0026lt;/exec\u0026gt; \u0026lt;argument\u0026gt;my_output=Hello Oozie\u0026lt;/argument\u0026gt; \u0026lt;capture-output/\u0026gt; \u0026lt;/shell\u0026gt; \u0026lt;ok to=\u0026#34;check-output\u0026#34;/\u0026gt; \u0026lt;error to=\u0026#34;fail\u0026#34;/\u0026gt; \u0026lt;/action\u0026gt; \u0026lt;decision name=\u0026#34;check-output\u0026#34;\u0026gt; \u0026lt;switch\u0026gt; \u0026lt;case to=\u0026#34;end\u0026#34;\u0026gt; ${wf:actionData(\u0026#39;shell-node\u0026#39;)[\u0026#39;my_output\u0026#39;] eq \u0026#39;Hello Oozie\u0026#39;} \u0026lt;/case\u0026gt; \u0026lt;default to=\u0026#34;fail-output\u0026#34;/\u0026gt; \u0026lt;/switch\u0026gt; \u0026lt;/decision\u0026gt; \u0026lt;kill name=\u0026#34;fail\u0026#34;\u0026gt; \u0026lt;message\u0026gt;Shell action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]\u0026lt;/message\u0026gt; \u0026lt;/kill\u0026gt; \u0026lt;kill name=\u0026#34;fail-output\u0026#34;\u0026gt; \u0026lt;message\u0026gt;Incorrect output, expected [Hello Oozie] but was [${wf:actionData(\u0026#39;shell-node\u0026#39;)[\u0026#39;my_output\u0026#39;]}]\u0026lt;/message\u0026gt; \u0026lt;/kill\u0026gt; \u0026lt;end name=\u0026#34;end\u0026#34;/\u0026gt; \u0026lt;/workflow-app\u0026gt; 文件上传\n hadoop dfs -put /home/hadoop/oozie-example/apps/shell/* /oozie/workflows/example/oozie-example/apps/shell  执行作业\n oozie job -oozie http://hh:11000/oozie -config job.properties -run  其中job.properties为本地的文件，而不是HDFS上的    查看作业\n Oozie访问页面：http://hh:11000/oozie/  Yarn访问页面：http://172.20.13.20:8088/cluster   ","id":19,"section":"posts","summary":"一、oozie组件 Workflow：工作流（流程图） 常用结点： 控制流结点（Control Flow Nodes） start、end、kill、分支、合","tags":["大数据"],"title":"Oozie学习（基本任务）","uri":"https://PI-KA-CHU.github.io/2019/08/oozie%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E4%BB%BB%E5%8A%A1/","year":"2019"},{"content":"一、什么是Docker   Docker是使用google公司推出的GO语言进行开发实现，于 Linux 内核的 cgroup，namespace，以及 AUFS 类的 Union FS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术，由于隔离的进程独立于宿主和其他隔离的进程，因此称为容器。\n  Docker在容器的基础上，进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护，使得Docker比虚拟技术更为轻便、快捷。\n   二、Docker与VMware 2.1、Docker与VMware的区别  Docker利用的是宿主机的内核进行容器的资源分配（直接调用硬件资源），隔离的是进程，VMware是在宿主机上虚拟底层硬件环境，通过创建操作系统实现隔离。 Docker由于没有臃肿的操作系统，启动速度比VMware快很多 Docker的隔离性比VMware差 Docker的资源利用比VMware大很多，VMware由于是直接虚拟操作系统，而创建时需要分配资源，相同的硬件环境Docker创建的容器及资源的利用率要比VMware多。     特性 容器 虚拟机     启动 秒级 分钟级   硬盘使用 一般为 MB 一般为 GB   性能 接近原生 弱于   系统支持量 单机支持上千个容器 一般几十个     2.2、Docker的优点（为什么使用Docker）  更高效的利用系统资源  不需要虚拟硬件及运行完整的操作系统等开销   更快速的启动时间  直接依赖于宿主内核，无需启动操作系统，运行速度达到毫秒或秒级   一致的运行环境  可提高一致的开发、测试、生产环境   持续集成、交付和部署  开发人员可以通过Dockerfile来进行镜像构建，并结合 持续集成(Continuous Integration) 系统进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至结合持续部署(Continuous Delivery/Deployment) 系统进行自动部署。   更轻松的迁移  Docker确保了运行环境的一致性，在不同平台上都能得到相同的运行结果，故其迁移十分便捷。   更轻松的维护和扩展   三、Docker的三大基本概念   镜像（Image）：一个特殊的文件系统（类）\n 操作系统分为内核和用户空间，操作系统启动后，会挂载root文件系统为其提供用户空间支持，而Docker（Image）,就相当于一个root文件系统。 Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 镜像不包含任何动态数据，其内容在构建之后也不会被改变。 镜像采用分层存储的架构（便于复用、定制，类似于继承），实际有多层文件系统联合组成。镜像在构建时，会一层一层的构建，前一层是后一层的基础，每一层构建后就不再改变，后一层的改变只发生在这一层（即删除上一层只是在本层进行标识）    容器（Container）：镜像运行时的实体（实例）\n 容器与镜像的关系如同OOP中的类与实例一样，镜像是静态的定义，容器是运行时的实体，可以被创建、启动、停止、运行和暂停等。 容器的实质是进程，但与直接在宿主执行的进程不同，其拥有独立的命名空间，而且使用的也是分层存储。 容器的存储层的生命周期和容器一样，容器消亡时其存储层也随之消亡，即存储层的数据都会随着容器的删除而丢失，因此对于文件的写入操作，都应该用数据卷（Volume）、或者绑定宿主目录，这些操作会跳转存储层直接对宿主进行读写，其性能和稳定性更高，容器删除后也不会消失，容器的存储层应保持无状态。    仓库（Repository）：集中存放镜像文件的地方（代码管理）\n 镜像构建完后，可以很容易的在本地宿主运行，但是如果要在其他服务器上使用镜像，我们就需要一个集中处理、分发镜像的服务，Docker Registry就是这样的服务。 一个 Docker Registry中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像，镜像仓库是用来集中存放镜像仓库的地方，类似与代码仓库。 通常一个仓库会包含同一软件不同版本的镜像，标签通常就对应软件的各个版本，可以通过\u0026lt;仓库名\u0026gt;:\u0026lt;标签\u0026gt;来指定哪个版本的镜像，没有指定标签的话就默认latest。 常见的镜像仓库服务：  https://hub.docker.com/ 时速云：https://hub.tenxcloud.com/ 网易云：https://www.163yun.com/product/repo 阿里云：https://www.aliyun.com/product/containerservice?utm_content=se_1292836       四、Build、Ship、Run Docker的运行过程：\n Docker将镜像仓库的镜像拉取到本地，从而运行为一个个容器，故Docker常被称为码头工人,主要包括以下三步：  Build（构建镜像）：镜像就像集装箱，包括运行环境、文件、资源等 Ship（运输镜像）：主机和仓库间运行，仓库就如同超级码头 Run（运行镜像）：运行的镜像就是容器，容器是程序运行的地方     五、参考  https://snailclimb.top/JavaGuide/#/tools/Docker https://github.com/Snailclimb/JavaGuide/blob/master/docs/tools/Docker.md https://www.zhihu.com/question/48174633  ","id":20,"section":"posts","summary":"一、什么是Docker Docker是使用google公司推出的GO语言进行开发实现，于 Linux 内核的 cgroup，namespace，以及 AUFS 类的 Union","tags":["docker","部署工具"],"title":"Docker的基本概念","uri":"https://PI-KA-CHU.github.io/2019/08/docker%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","year":"2019"},{"content":"一、容器间的区别 1.1、List、Map、Set三者间的区别  List：List接口存储一组可重复的、有序的对象 Map：Map接口存储以key为索引，value为值数据，其中key不可以重复，value可以 Set：Set接口存储一组唯一的数据，不允许多个元素引用相同的对象  1.2、ArrayList和LinkedList的区别   是否线程安全：都是非线程安全的\n  底层数据结构：\n ArrayList：底层是由Object数组实现 LinkedList：是用双向链表实现（JDK1.7前为循环双向链表    插入及删除效率是否受位置影响：\n ArrayList：由于数组的特性，插入及删除效率受位置的影响，需要对相关数据进行移动。 LinkedList：由于链表的特性，其插入及删除效率不受位置影响，时间复杂度都为O(1)    是否支持快速随机访问：\n ArrayList：支持高速随机访问，实现了RandomAccess标识接口（实现则采用indexBinarySearch方法，否则采用iteratorBinarySearch方法） LinkedList：不支持高速随机访问，需要遍历数据    内存空间占用：\n ArrayList：在List结尾会预留一定的容量空间 LinkedList：则主要体现在每个元素都需要空间存放直接前驱和直接后继。    扩容机制：\n ArrayList默认长度为10，扩容时为原来的1.5倍    1.3、ArrayList和Vector的区别  Vector：所有方法都是线程同步的，所以在同步操作上需要耗费大量时间 ArrayList：是非线程安全的，在单线程的情况下采用ArrayList会有更好的性能  1.4、HashMap和HashTable（已被淘汰）的区别   是否线程安全：HashTable为线程安全\n  效率：HashTable由于是全表同步的，其同步开销较大，效率较低\n  对Null key和value的支持：\n HashMap：支持key和value为null，但是key中的null必须是唯一的 HashTable：支持value为null，当key为null时会报NullPointException    初始容量及扩容：\n HashTable：默认大小为11，扩容为原来的2n+1，当给定了初始值时，会直接使用该值 HashMap：默认大小为16，扩容时为原来的2倍，当给定了初始值时，会先扩充为2的幂次方    HashMap的构造方法： public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;Illegal initial capacity: \u0026#34; + initialCapacity); if (initialCapacity \u0026gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor \u0026lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\u0026#34;Illegal load factor: \u0026#34; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); } public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); }  底层数据结构：  JDK1.8后，HashMap在处理哈希冲突上采用的是数组 + 链表的形式，同时在链表长度超过阈值时（默认为8），将链表转换为红黑树，提高了搜索效率，将链表（寻址时间复杂度为O(N)）转换为红黑树（寻址时间复杂度为O(log(N))），而HashTable则没有    1.5、HashMap和HashSet的区别  HashSet：  底层是基于HashMap实现 在校验重复上，插入时先比较HashCode是否相同，如果相同则调用equal方法比较，都相同则不能成功插入。    1.6、HashTable和ConCurrentHashMap的区别   底层数据结构：\n HashTable：数组（主体） + 链表（解决哈希冲突） ConCurrentHashMap：  JDK1.7：分段数组 + 链表 JDK1.8：数组 + 链表/红黑二叉树      实现线程安全的方式：\n HashTable：  同一把锁，使用synchronized保证线程安全，效率非常低下。    ConCurrentHashMap：  JDK1.7：采用分段锁，对整个桶数据进行了分割分段，每段有一把锁，多线程并发不同数据段的数据，不会有锁竞争，提高了并发效率。  JDK1.8：取消了分段锁机制，采用Node 数组（实现了Map接口） + 链表 + 红黑树，并发控制使用synchronized 和CAS。synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发，效率又提升N倍。        二、容器的特性 2.1、HashMap的长度为什么是2的幂次方   为了让HashMap存取高效，尽量减少碰撞，也就是尽量的把数据分配均匀。Hash 值的范围值-2147483648到2147483647，前后加起来大概40亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个40亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是(length - 1) \u0026amp; hash。（n代表数组长度）。这也就解释了 HashMap 的长度为什么是2的幂次方。\n  \u0026amp;的运算比%的运算效率是高很多的，为了提高运算，在保证长度为2的幂次方的条件下， hash \u0026amp; (length - 1) = hash % length是成立的，此时使用位运算\u0026amp;可以提高运行效率。\n  2.2、Comparable和Comparator的区别  comparable接口出自java.lang包 它有一个 compareTo(Object obj)方法用来排序，一般是在实体中实现此接口并重写compareTo方法。  //person对象没有实现Comparable接口，所以必须实现，这样才不会出错，才可以使TreeMap中的数据按顺序排列，而String和Integer等类型都已经默认实现了Comparable接口 public class Person implements Comparable\u0026lt;Person\u0026gt; { private int age; public int getAge() { return age; } public void setAge(int age) { this.age = age; } /** * TODO重写compareTo方法实现按年龄来排序 */ @Override public int compareTo(Person o) { // TODO Auto-generated method stub if (this.age \u0026gt; o.getAge()) { return 1; } else if (this.age \u0026lt; o.getAge()) { return -1; } return age; } public static void main(String[] args) { //TreeMap是 TreeMap\u0026lt;Person, String\u0026gt; pdata = new TreeMap\u0026lt;Person, String\u0026gt;(); pdata.put(new Person(\u0026#34;张三\u0026#34;, 30), \u0026#34;zhangsan\u0026#34;); pdata.put(new Person(\u0026#34;李四\u0026#34;, 20), \u0026#34;lisi\u0026#34;); pdata.put(new Person(\u0026#34;王五\u0026#34;, 10), \u0026#34;wangwu\u0026#34;); pdata.put(new Person(\u0026#34;小红\u0026#34;, 5), \u0026#34;xiaohong\u0026#34;); // 得到key的值的同时得到key所对应的值 Set\u0026lt;Person\u0026gt; keys = pdata.keySet(); for (Person key : keys) { System.out.println(key.getAge() + \u0026#34;-\u0026#34; + key.getName()); } } }  comparator接口出自java.util包，它有一个compare(Object obj1, Object obj2)方法用来排序，通常是与Collections类一起使用  ArrayList\u0026lt;Integer\u0026gt; arrayList = new ArrayList\u0026lt;Integer\u0026gt;(); //反转 Collections.reverse(arrayList); //按自然排序升序排序 Collections.sort(arrayList); // 定制排序的用法 Collections.sort(arrayList, new Comparator\u0026lt;Integer\u0026gt;() { @Override public int compare(Integer o1, Integer o2) { return o2.compareTo(o1); } }); 2.3、哈希冲突的解决方法  开放地址法：从发生冲突的那个单元起，按照一定的次序，从哈希表中选择一个空闲的单元，将发生冲突的单元放入即可，开放地址法的缺点是不能真正的删除元素，只能做特殊标识，直到有下个元素插入才可以真正删除，否则会引起查找错误。  线行探查法：依次判断 平方探查法：d[i] + n^2 双散列函数探查法：   拉链法：数组 + 链表 再哈希法：构造多个不同的哈希函数，出现ch 建立公共溢出区：将哈希表分为公共表和溢出表，当溢出发生时，将所有溢出数据统一放到溢出区。   三、容器底层数据结构总结 3.1、List：  ArrayList：Object数组 Vector：Object数组 LinkedList：双向链表（1.6前为循环双向链表）  3.2、Map  HashMap：  1.7以前：数组 + 链表 1.8以后：数组 + 链表/红黑二叉树   HashTable：  数组 + 链表   ConcurrentHashMap：  1.7以前：数组 + 链表（分段锁） 1.8以后：Node数组 + 链表/红黑二叉树（只锁定当前链表或红黑二叉树的首节点）   TreeMap：红黑树（自平衡的排序二叉树） LinkedHashMap：继承自 HashMap，在 HashMap 基础上，通过维护一条双向链表，解决了 HashMap 不能随时保持遍历顺序和插入顺序一致的问题（参考：https://www.imooc.com/article/22931）  3.3、Set  HashSet（无序、唯一）：基于 HashMap 实现的，底层采用 HashMap 来保存元素 LinkedHashSet： LinkedHashSet 继承于 HashSet，并且其内部是通过 LinkedHashMap 来实现的。 TreeSet（有序，唯一）： 红黑树(自平衡的排序二叉树，需要重写Compare或者CompareTo方法) 参考：https://blog.csdn.net/lijock/article/details/80410202  3.4、集合的选用  主要根据集合的特点来选用，比如我们需要根据键值获取到元素值时就选用Map接口下的集合，需要排序时选择TreeMap,不需要排序时就选择HashMap,需要保证线程安全就选用ConcurrentHashMap.当我们只需要存放元素值时，就选择实现Collection接口的集合，需要保证元素唯一时选择实现Set接口的集合比如TreeSet或HashSet，不需要就选择实现List接口的比如ArrayList或LinkedList，然后再根据实现这些接口的集合的特点来选用。   四、参考  https://www.jianshu.com/p/4d3cb99d7580 https://snailclimb.top/JavaGuide/#/java/collection/Java%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98?id=collection  ","id":21,"section":"posts","summary":"一、容器间的区别 1.1、List、Map、Set三者间的区别 List：List接口存储一组可重复的、有序的对象 Map：Map接口存储以key","tags":[""],"title":"Java容器学习","uri":"https://PI-KA-CHU.github.io/2019/07/java%E5%AE%B9%E5%99%A8%E5%AD%A6%E4%B9%A0/","year":"2019"},{"content":"注册服务  https://jingyan.baidu.com/article/870c6fc3557921b03fe4bef7.html https://www.cnblogs.com/liuxiaoji/p/8016261.html https://blog.csdn.net/lsj19830812/article/details/6187233  开机自启脚本  https://blog.csdn.net/slf450272048/article/details/79378517 https://blog.csdn.net/qq_36999656/article/details/80278437  删除服务  https://blog.csdn.net/gaoxin_gx/article/details/89639867  ","id":22,"section":"posts","summary":"注册服务 https://jingyan.baidu.com/article/870c6fc3557921b03fe4bef7.html https://www.cnblogs.com/liuxiaoji/p/8016261.html https://blog.csdn.net/lsj19830812/article/details/6187233 开机自启脚本 https://blog.csdn.net/slf450272048/article/details/79378517 https://blog.csdn.net/qq_36999656/article/details/80278437 删除服务 https://blog.csdn.net/gaoxin_gx/article/details/89639867","tags":["部署工具"],"title":"Windows服务注册及脚本开机自启","uri":"https://PI-KA-CHU.github.io/2019/07/windows%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%8F%8A%E8%84%9A%E6%9C%AC%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF/","year":"2019"},{"content":"一、什么是AOP？ 1.1、AOP的定义  AOP即面向切面编程，相比OOP —— 面向对象编程（最基本单位为类和实例），AOP的基本单位为切面。AOP编程主要可以用在如：权限校验、日志服务、事务控制、异常处理等，能够对业务进行统一管理编程，减少了代码的冗余，降低了业务代码也服务管理部分的耦合程度，同时提高了代码的可维护性，对下面是Spring中关于AOP的定义：   面向切面——Spring提供了面向切面编程的丰富支持，允许通过分离应用的业务逻辑与系统级服务（例如审计（auditing）和事务（transaction）管理）进行内聚性的开发。应用对象只实现它们应该做的——完成业务逻辑——仅此而已。它们并不负责（甚至是意识）其它的系统级关注点，例如日志或事务支持。\n 1.2、AOP的的基本概念   通知/增强（Adivce） = 定位到方法后做什么事\n Before：前置通知（方法调用前） After：后置通知（方法调用后，无论是否成功，相当于Finally代码块） After-returning：最终通知（方法调用成功后） After-throwing：异常通知（在方法抛出异常后调用通知） Around：环绕通知（可以在目标方法前或方法后执行，即可以在环绕方法里调用目标方法）  @Around(\u0026#34;execution(...)\u0026#34;) public Object around(ProceedingJoinPoint joinPoint) throws Throwable { System.out.println(\u0026#34;我是环绕通知前....\u0026#34;); //执行目标函数 Object obj= (Object) joinPoint.proceed(); System.out.println(\u0026#34;我是环绕通知后....\u0026#34;); return obj; }   切点（Pointcut） = 定位到具体方法的表达式\n 切点可以认为是对应某个方法的点（可以被AOP扫描到的某个方法），它通常与通知一起使用，比如，不同方法的权限校验，使用AOP进行编程的话这几个方法属于不同的切点，多个切点则组成了切面。  execution(* com.zdy..*(..))：com.zdy包下所有类的所有方法. execution(* com.zdy.Dog.*(..)): Dog类下的所有方法.      连接点（Join point）\n 连接点是一个比较虚拟的概念，可以理解为满足所有切点扫描的所有时机，每个连接点可以将切面代码插入到其中，并为之添加新的行为，如调用某个接口时，异常抛出时，或者修改某个字段时等。    切面（Aspect） = Advice + Pointcut\n 切面是切点和通知的集合，一般单独作为一个类进行管理。通知和切点共同定义了关于切面的全部内容，它在什么时候（Advice的时间）、什么地方（切点）完成什么功能（Advice的功能）。    引入（Introduction）\n 引用允许我们向现有的类添加新的方法或者属性    织入（Weaving）\n 组装方面来创建一个被通知对象。这可以在编译时完成（例如使用AspectJ编译器），也可以在运行时完成。Spring和其他纯Java AOP框架一样，在运行时完成织入。     二、Spring对AOP的支持 2.1、Spring的代理模式  AOP思想的实现一般都是基于代理模式，在JAVA中一般采用JDK动态代理，但是由于JDK动态代理只能代理接口，如果代理类的话就不行了。因此，Sping AOP会自动进行切换，因为Spring同时支持JDK动态代理、CGLIB和AspectJ，当对象有实现接口时，会默认采用JDK动态代理，否则使用CGLIB代理。   如果目标对象的实现类实现了接口，Spring AOP 将会采用 JDK 动态代理 来生成 AOP 代理类；\n 实现了接口如何强制使用CGLIB实现AOP？ （1）添加CGLIB库，SPRING_HOME/cglib/*.jar （2）在spring配置文件中加入\u0026lt;aop:aspectj-autoproxy proxy-target-class=\u0026quot;true\u0026quot;/\u0026gt;    如果目标对象的实现类没有实现接口，Spring AOP 将会采用 CGLIB 来生成 AOP 代理类——不过这个选择过程对开发者完全透明、开发者也无需关心。\n    2.2、CGLIB和JDK动态代理的区别 a、原理区别：\n JDK动态代理是利用反射机制生成一个实现代理接口的匿名类，在调用具体方法前调用InvokeHandler来处理。 CGLIB是利用asm开源包，将代理对象类的class文件加载进来，通过修改其字节码生成子类来处理。  b、功能区别：\n JDK动态代理只能对实现了接口的类生成代理，而不能针对类。 CGLIB是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法，因为是继承，所以该类或方法最好不要声明成final   三、Sping AOP配置 切面配置：\n XML配置  \u0026lt;aop:config\u0026gt; \u0026lt;!-- 这是定义一个切面，切面是切点和通知的集合--\u0026gt; \u0026lt;aop:aspect id=\u0026#34;do\u0026#34; ref=\u0026#34;PermissionVerification\u0026#34;\u0026gt; \u0026lt;!-- 定义切点 ，后面是expression语言，表示包括该接口中定义的所有方法都会被执行--\u0026gt; \u0026lt;aop:pointcut id=\u0026#34;point\u0026#34; expression=\u0026#34;execution(* wokao666.club.aop.spring01.Subject.*(..))\u0026#34; /\u0026gt; \u0026lt;!-- 定义通知 --\u0026gt; \u0026lt;aop:before method=\u0026#34;canLogin\u0026#34; pointcut-ref=\u0026#34;point\u0026#34; /\u0026gt; \u0026lt;aop:after method=\u0026#34;saveMessage\u0026#34; pointcut-ref=\u0026#34;point\u0026#34; /\u0026gt; \u0026lt;/aop:aspect\u0026gt; \u0026lt;/aop:config\u0026gt;  注解配置（AspectJ注解）  AspectJ是一个AOP框架，它能在编译期对JAVA代码进行AOP编译，让其具有AOP功能，AspectJ是目前实现AOP框架中最成熟的，并且与JAVA完全兼容。AspectJ单独就是一门语言，它需要专门的编译器(ajc编译器)，Spring很机智回避了这点，转向采用动态代理技术的实现原理来构建Spring AOP的内部机制（动态织入），而AspectJ是静态织入，这是最根本的区别，并且Sping AOP不尝试提供完整的AOP功能，其注重的是Spring AOP与Spring IOC容器的结合，借助IOC优势处理切面问题。Spring AOP只是整合了AspectJ的那套注解，底层仍然是采用Spring AOP的动态代理技术实现。    切面类：\n@Aspect //声明自己是一个切面类 public class MyAspect { /** * 前置通知 */ // @Before是增强中的方位 // @Before括号中的就是切入点了 // before()就是传说的增强(建言):说白了，就是要干啥事. @Before(\u0026#34;execution(* com.zdy..*(..))\u0026#34;) public void before(){ System.out.println(\u0026#34;前置通知....\u0026#34;); } } XML文件：\n//开启AspectJ功能. \u0026lt;aop:aspectj-autoproxy /\u0026gt; \u0026lt;bean id=\u0026#34;dog\u0026#34; class=\u0026#34;com.zdy.Dog\u0026#34; /\u0026gt; \u0026lt;!-- 定义aspect类 --\u0026gt; \u0026lt;bean name=\u0026#34;myAspect\u0026#34; class=\u0026#34;com.zdy.MyAspect\u0026#34;/\u0026gt;  四、参考  https://juejin.im/post/5aa7818af265da23844040c6 https://www.cnblogs.com/leifei/p/8263448.html https://juejin.im/post/5a55af9e518825734d14813f  ","id":23,"section":"posts","summary":"一、什么是AOP？ 1.1、AOP的定义 AOP即面向切面编程，相比OOP —— 面向对象编程（最基本单位为类和实例），AOP的基本单位为切面。AO","tags":["","spring"],"title":"Spring AOP学习","uri":"https://PI-KA-CHU.github.io/2019/07/spring-aop%D1%A7%CF%B0/","year":"2019"},{"content":"一、定义 bean的定义  Bean是一个被实例化、组装、并被Spring容器所管理的对象，是由容器提供的配置元数据（如bean.xml）创建的。（下图是Bean与Spring容器的关系）   bean的三种元数据配置（bean的创建）  参考： https://blog.csdn.net/isea533/article/details/78072133   基于XML的配置  \u0026lt;beans\u0026gt; \u0026lt;bean name=\u0026#34;\u0026#34; class=\u0026#34;\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; 基于注解的配置  @Component public class MyBeanConfig { @Autowired private Country country; @Bean public Country country(){ return new Country(); } @Bean public UserInfo userInfo(){ return new UserInfo(country); } } 基于Java的配置（@Configuration实现自动依赖注入，且通过代理操作，而@Component则没有代理，且需要加上@Autowired，否则两个方法返回的是不同的实例）  @Configuration public class MyBeanConfig { @Bean public Country country(){ return new Country(); } @Bean public UserInfo userInfo(){ return new UserInfo(country()); } } bean的作用域 （可通过Xml的scope属性及@Scope注解进行配置）\n singleton prototype request session global-session  bean的生命周期   转自： https://www.zhihu.com/question/38597960/answer/77600561   1、Spring对Bean进行实例化（相当于程序中的new Xx()，反射实现）\n  2、Spring将值和Bean的引用注入进Bean对应的属性中（反射实现）\n  3、如果Bean实现了BeanNameAware接口，Spring将Bean的ID传递给setBeanName()方法（实现BeanNameAware清主要是为了通过Bean的引用来获得Bean的ID，一般业务中是很少有用到Bean的ID的）\n  4、如果Bean实现了BeanFactoryAware接口，Spring将调用setBeanFactory(BeanFactory bf)方法并把BeanFactory容器实例作为参数传入。（实现BeanFactoryAware 主要目的是为了获取Spring容器，如Bean通过Spring容器发布事件等）\n  5、如果Bean实现了ApplicationContextAwaer接口，Spring容器将调用setApplicationContext(ApplicationContext ctx)方法，把应用上下文作为参数传入.(作用与BeanFactory类似都是为了获取Spring容器，不同的是Spring容器在调用setApplicationContext方法时会把它自己作为setApplicationContext 的参数传入，而Spring容器在调用setBeanDactory前需要程序员自己指定（注入）setBeanDactory里的参数BeanFactory )\n  6、如果Bean实现了BeanPostProcess接口，Spring将调用它们的postProcessBeforeInitialization（预初始化）方法（作用是在Bean实例创建成功后对进行增强处理，如对Bean进行修改，增加某个功能）\n  7、如果Bean实现了InitializingBean接口，Spring将调用它们的afterPropertiesSet方法，作用与在配置文件中对Bean使用init-method声明初始化的作用一样，都是在Bean的全部属性设置成功后执行的初始化方法。\n  8、如果Bean实现了BeanPostProcess接口，Spring将调用它们的postProcessAfterInitialization（后初始化）方法（作用与第六步的一样，只不过是在Bean初始化前执行的，而这个是在Bean初始化后执行的，时机不同 )\n  9、经过以上的工作后，Bean将一直驻留在应用上下文中给应用使用，直到应用上下文被销毁\n  10、如果Bean实现了DispostbleBean接口，Spring将调用它的destory方法，作用与在配置文件中对Bean使用destory-method属性的作用一样，都是在Bean实例销毁前执行的方法。\n  综合上面十个步骤，可以简化为：Bean实例的创建、Bean属性的注入、Bean相关Aware接口的实现，Bean实例调用前的初始化、Bean实例的调用、Bean实例销毁前的初始化、Bean实例的销毁\n ","id":24,"section":"posts","summary":"一、定义 bean的定义 Bean是一个被实例化、组装、并被Spring容器所管理的对象，是由容器提供的配置元数据（如bean.xml）创建的。","tags":["","spring"],"title":"Spring Bean学习","uri":"https://PI-KA-CHU.github.io/2019/07/spring-bean%D1%A7%CF%B0/","year":"2019"},{"content":"一、什么是IOC  IOC（控制反转）又叫做DI（依赖注入），它描述了对象的定义和依赖的一个过程，也就是说，依赖的对象通过构造参数、工厂方法参数或属性注入，当对象实例后依赖的对象才被创建，当创建bean后容器注入这些依赖对象。与原来在类中使用其他类时相比（new Object()），这个过程是反向的，即对象与对象间的依赖不再是主动的去new 一个对象，而是交由Bean容器在对象实例化的时候进行注入。   二、为什么要用IOC  IOC是Spring的重要部分，那么为什么要使用IOC呢？下面是使用IOC与不使用的比较：\n a. 原始的对象间的关系：\n b.基于IOC容器的对象间的关系：\n  从上面的图中对比可以看出，原始的对象间的关系存在着耦合过高的问题，在大型软件系统中其耦合程度更是极其复杂，IOC容器将复杂的系统分解成相互合作的对象，降低了解决问题的复杂度，并且由于Spring默认是单例模式，使得对象可以被灵活地扩展和重用，极大降低了系统的开销。通过IOC容器，实现了具有依赖关系的对象间的解耦。\n  三、IOC的实现原理：  IOC中最基本的技术就是反射，JAVA反射机制就是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能调用它的任意方法和属性；这种动态获取信息以及动态调用对象方法的功能称为JAVA语言的反射机制；这里简单讲述一下\n 为什么要使用反射：  程序运行前需要先编译，编译过程中将代码中需要的类加载到JVM中，运行的时候进行内存分配（类的实例化），相当于加载的类已经是固定的，如果使用静态编译的话，增加某个类的创建需要重新编译整个软件，而使用反射机制（动态编译）则不需要，可以在运行的过程中动态的获取。\n a. 无反射技术的工厂模式：\n//构造工厂类 //也就是说以后如果我们在添加其他的实例的时候只需要修改工厂类就行了 class Factory{ public static fruit getInstance(String fruitName){ fruit f=null; if(\u0026#34;Apple\u0026#34;.equals(fruitName)){ f=new Apple(); } if(\u0026#34;Orange\u0026#34;.equals(fruitName)){ f=new Orange(); } return f; } b. 基于反射技术的工厂模式：\nclass Factory{ public static fruit getInstance(String ClassName){ fruit f=null; try{ f=(fruit)Class.forName(ClassName).newInstance(); }catch (Exception e) { e.printStackTrace(); } return f; } }  由上面的代码示例可以看出，反射技术极大提升了代码的灵活性，由于无法知道需要创建的Bean类型，反射技术可以在运行时动态的调用构造方法进行类的动态创建，IOC实现的工厂模式即是使用反射技术，能否在运行时动态创建也是衡量一门语言是否是动态语言的标准。之一（可以查下动态语言和静态语言的区别）。\n  反射机制的作用：  在运行的时候能够判断任意对象所属的类 在运行时获取类的对象 在运行时访问java的属性、方法和构造方法等；   反射技术的优缺点：  优点：能够动态的创建对象和编译，具有极强的灵活性。 缺点：性能相对较差，使用反射基本是一种解释性操作。   四、Spring IOC的重要内容 4.1 依赖注入的实现（DI） 4.1.1 注入方式  基于构造函数的注入  TextEditor.java\npublic class TextEditor { private SpellChecker spellChecker; public TextEditor(SpellChecker spellChecker) { System.out.println(\u0026#34;Inside TextEditor constructor.\u0026#34; ); this.spellChecker = spellChecker; } public void spellCheck() { spellChecker.checkSpelling(); } } SpellChecker.java\npublic class SpellChecker { public SpellChecker(){ System.out.println(\u0026#34;Inside SpellChecker constructor.\u0026#34; ); } public void checkSpelling() { System.out.println(\u0026#34;Inside checkSpelling.\u0026#34; ); } } MainApp.java\npublic class MainApp { public static void main(String[] args) { ApplicationContext context = new ClassPathXmlApplicationContext(\u0026#34;Beans.xml\u0026#34;); TextEditor te = (TextEditor) context.getBean(\u0026#34;textEditor\u0026#34;); te.spellCheck(); } } xml配置文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;!-- Definition for textEditor bean --\u0026gt; \u0026lt;bean id=\u0026#34;textEditor\u0026#34; class=\u0026#34;com.tutorialspoint.TextEditor\u0026#34;\u0026gt; \u0026lt;constructor-arg ref=\u0026#34;spellChecker\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;!-- Definition for spellChecker bean --\u0026gt; \u0026lt;bean id=\u0026#34;spellChecker\u0026#34; class=\u0026#34;com.tutorialspoint.SpellChecker\u0026#34;\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt;   基于setter方法的注入（由于注入是基于java反射机制实现的，即使没有 setter 声明的方法，也可以进行注入）  TextEditor.java\npublic class TextEditor{ private SpellChecker spellChecker; // a setter method to inject the dependency. public void setSpellChecker(SpellChecker spellChecker) { System.out.println(\u0026#34;Inside setSpellChecker.\u0026#34; ); this.spellChecker = spellChecker; } // a getter method to return spellChecker public SpellChecker getSpellChecker() { return spellChecker; } public void spellCheck() { spellChecker.checkSpelling(); } } SpellChecker.java\npublic class SpellChecker { public SpellChecker(){ System.out.println(\u0026#34;Inside SpellChecker constructor.\u0026#34; ); } public void checkSpelling() { System.out.println(\u0026#34;Inside checkSpelling.\u0026#34; ); } } xml配置文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;!-- Definition for textEditor bean --\u0026gt; \u0026lt;bean id=\u0026#34;textEditor\u0026#34; class=\u0026#34;com.tutorialspoint.TextEditor\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;spellChecker\u0026#34; ref=\u0026#34;spellChecker\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;!-- Definition for spellChecker bean --\u0026gt; \u0026lt;bean id=\u0026#34;spellChecker\u0026#34; class=\u0026#34;com.tutorialspoint.SpellChecker\u0026#34;\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; 4.1.2 注入配置  基于XML的注入配置（如上面的配置） 基于注解的注入配置（通过@Autowired进行注入）   4.2 Spring IOC的初始化（Bean的初始化）   参考：https://www.jianshu.com/p/70886997c46b\n  主要分为三个步骤（容器的初始化是通过refresh()实现）\n 定位：通过Resource定位BeanDefinition，BeanDefinition定义了Bean的元信息、依赖关系等，即寻找Bean的过程。 载入：BeanDefinition的信息已经定位到了，第二步就是把定义的BeanDefinition在Ioc容器中转化成一个Spring内部标示的数据结构的过程。 注册：将抽象好的BeanDefinition统一注册到IoC容器中，IoC容器是通过ConcurrentHashMap来维护BeanDefinition信息的，key为beanName，value为BeanDefinition。     五、IOC的优缺点 优点：\n 实现了对象之间的解耦，基于单例模式可以有效减少系统资源的消耗。  缺点：\n 基于反射实现，性能会稍微差一些，单例模式引入了线程安全问题。 生成对象的步骤变得复杂了，需要投入学习成本和精力。   六、IOC的源码解析 BeanFactory和ApplicationContext的区别\n ApplicationContext接口继承自BeanFactory接口，同时继承了MessageSource和ResourceLoader等其他接口，相比BeanFactory，ApplicationContext提供了更多的扩展功能，如能够实现国际化访问、事务传播及AOP等服务。 BeanFactory是懒加载（延迟加载），BeanFactory加载后，需要第一次调用getBean()方法才会实例化，而ApplicaitonContext实现的是饿汉加载，在容器初始化的时候，会实例化所有的bean。 ApplicationContext可以及时检查Bean是否完成注入，BeanFactory需要调用getBean()的时候才会抛出异常。   参考  https://www.cnblogs.com/wang-meng/p/5597490.html https://blog.csdn.net/u010325193/article/details/80865672 *https://blog.csdn.net/fuzhongmin05/article/details/61614873 https://blog.csdn.net/java_gchsh/article/details/78111200 https://blog.csdn.net/qq_19782019/article/details/85038081 （setter注入问题）  ","id":25,"section":"posts","summary":"一、什么是IOC IOC（控制反转）又叫做DI（依赖注入），它描述了对象的定义和依赖的一个过程，也就是说，依赖的对象通过构造参数、工厂方法参数","tags":["","spring"],"title":"Spring IOC学习","uri":"https://PI-KA-CHU.github.io/2019/07/spring-ioc%D1%A7%CF%B0/","year":"2019"},{"content":" 最近在学习Spring的相关模块及源码，在学习Bean的相关创建方法时，发现spring的bean一般都是单例模式，在xml中可以通过scope=prototype、注解中可以通过@Scope(\u0026quot;prototypr\u0026quot;)设置为多例模式（singleton为单例模式），不禁想到自己项目组中的Spring MVC中使用的是单例还是多例？下面是对Controller层的代码测试：\n  1、Spring MVC的单例和多例测试  下面代码中在controller层中加入成员变量（此处添加只为测试，为非线程安全），现在通过调用此url接口，发现输出的test分别为：1,2,3...，说明conreoller层为单例模式，只生成了一个实例，加入@Scope(\u0026quot;prototype\u0026quot;)的注解后，输出结果为：1,1,1...，说明为多例模式(原型模式)，每次请求都会生成对象。  @Controller @RequestMapping(\u0026#34;/api/login\u0026#34;) public class LoginController { private Logger logger = LoggerFactory.getLogger(this.getClass()); private int test = 1; //非线程安全 @Autowired private LoginService loginService; @Autowired private LoginDao loginDao; @RequestMapping(value = \u0026#34;/admin\u0026#34;, method = RequestMethod.GET) public void adminLogin() { System.out.println(test ++); }  2、单例和多例的应用场景  如果对象是无状态的（如上面的LoginService），或者对象的成员变量（共享）是线程安全的，如静态变量，常量，Threadlocal变量等，则可以使用单例模式，这样在性能上能够大大提升，减少了实例对象的开销。 如果对象存在线程不安全的变量，则需要采用多例模式，否则会出现线程安全问题。（一般情况下都是将变量修改为线程安全模式），所以在controller的编程中尽量不要出现成员变量。   3、单例模式的扩展  单例模式有懒汉模式和饿汉模式  饿汉模式：在启动Bean容器时，为xml中配置的bean都实例化一个对象（Spring的默认形式） 懒汉模式：第一个请求时才创建实例，后续的请求都使用此实例。     参考  https://blog.csdn.net/qianyiyiding/article/details/77104736 https://blog.csdn.net/qq_35661171/article/details/83180546  ","id":26,"section":"posts","summary":"最近在学习Spring的相关模块及源码，在学习Bean的相关创建方法时，发现spring的bean一般都是单例模式，在xml中可以通过sco","tags":["","spring"],"title":"Spring MVC学习","uri":"https://PI-KA-CHU.github.io/2019/06/spring-mvc%D1%A7%CF%B0/","year":"2019"},{"content":"一、线程池 原理：  类似于操作系统中的缓冲区，流程如下：先启动若干数量的线程，并让线程处于休眠的状态，当客户端有新请求时就回唤醒其中一个睡眠线程进行请求处理，处理完又处于睡眠状态。\n JAVA的六大线程池（Executors）   FixedThreadPool（定长连接池）：\n 固定线程池中线程的个数。使用静态方法 newFixedThreadPool() 创建线程池的时候指定线程池个数。 适用：执行长期的任务。    CachedThreadPool（弹性缓存线程池）\n 使用 newCachedThreadPool() 方法创建该线程池对象，创建之初池中没有线程，当 execute 方法或 submit 方法向线程池提交任务时，会自动创建线程；如果有空余线程则不被创建，线程60秒后被回收。 适用：执行很多短期异步的小程序或者负载较轻的服务器。    SingleThreadPool（单线程线程池）\n 池中只有一个线程，如果有多个线程，则需要进行排队，可以保证任务的顺序执行。 适用：顺序执行的任务场景。    ScheduledThreadpool（定时器线程池，定长线程池）\n 支持定时及周期性任务执行。 适应：周期性执行任务的场景。    WorkStealingPool\n  ForkJoinPool\n   二、连接池 原理  数据库连接是一种关键的有限的昂贵资源，这一点在多用户的网页应用程序中体现得尤为突出。 一个数据库连接对象均对应一个物理数据库连接，每次操作都打开一个物理连接，使用完都关闭连接，这样造成系统的 性能低下。 数据库连接池的解决方案是在应用程序启动时建立足够的数据库连接，并讲这些连接组成一个连接池(简单说：在一个“池”里放了好多半成品的数据库联接对象)，由应用程序动态地对池中的连接进行申请、使用和释放。对于多于连接池中连接数的并发请求，应该在请求队列中排队等待。并且应用程序可以根据池中连接的使用率，动态增加或减少池中的连接数。 连接池技术尽可能多地重用了消耗内存地资源，大大节省了内存，提高了服务器地服务效率，能够支持更多的客户服务。通过使用连接池，将大大提高程序运行效率，同时，我们可以通过其自身的管理机制来监视数据库连接的数量、使用情况等。  主流的数据库连接池   C3p0：开源的JDBC连接池，实现了数据源和JNDI绑定，支持JDBC3规范和JDBC2的标准扩展。目前使用它的开源项目有Hibernate、Spring等。单线程，性能较差，适用于小型系统，代码600KB左右。\n  DBCP (Database Connection Pool)：Apache开发的一个Java数据库连接池项目， Jakarta commons-pool对象池机制，Tomcat使用的连接池组件就是DBCP。单独使用dbcp需要3个包：common-dbcp.jar,common-pool.jar,common-collections.jar，预先将数据库连接放在内存中，应用程序需要建立数据库连接时直接到连接池中申请一个就行，用完再放回。单线程，并发量低，性能不好，适用于小型系统。\n  Tomcat Jdbc Pool：Tomcat在7.0以前都是使用common-dbcp做为连接池组件，但是dbcp是单线程，为保证线程安全会锁整个连接池，性能较差，dbcp有超过60个类，也相对复杂。Tomcat从7.0开始引入了新增连接池模块叫做Tomcat jdbc pool，基于Tomcat JULI，使用Tomcat日志框架，完全兼容dbcp，通过异步方式获取连接，支持高并发应用环境，超级简单核心文件只有8个，支持JMX，支持XA Connection。\n  BoneCP：官方说法BoneCP是一个高效、免费、开源的Java数据库连接池实现库。设计初衷就是为了提高数据库连接池性能，根据某些测试数据显示，BoneCP的速度是最快的，要比当时第二快速的连接池快25倍左右，完美集成到一些持久化产品如Hibernate和DataNucleus中。BoneCP特色：高度可扩展，快速；连接状态切换的回调机制；允许直接访问连接；自动化重置能力；JMX支持；懒加载能力；支持XML和属性文件配置方式；较好的Java代码组织，100%单元测试分支代码覆盖率；代码40KB左右。\n  Druid：Druid是Java语言中最好的数据库连接池，Druid能够提供强大的监控和扩展功能，是一个可用于大数据实时查询和分析的高容错、高性能的开源分布式系统，尤其是当发生代码部署、机器故障以及其他产品系统遇到宕机等情况时，Druid仍能够保持100%正常运行。主要特色：为分析监控设计；快速的交互式查询；高可用；可扩展；Druid是一个开源项目，源码托管在github上。\n  HikariCP：HikariCP通过优化(concurrentBag，fastStatementList )集合来提高并发的读写效率，使用threadlocal缓存连接及大量使用CAS的机制，最大限度的避免lock，但可能带来cpu使用率的上升。从字节码的维度优化代码，让方法尽量在35个字节码一下，来提升jvm的处理效率。\n   参考  http://blog.didispace.com/java-datasource-pool-compare/ https://blog.csdn.net/qq_26963495/article/details/79056391 https://blog.csdn.net/WoAiBianCheng123abc/article/details/82913013  ","id":27,"section":"posts","summary":"一、线程池 原理： 类似于操作系统中的缓冲区，流程如下：先启动若干数量的线程，并让线程处于休眠的状态，当客户端有新请求时就回唤醒其中一个睡眠线程","tags":["多线程"],"title":"线程池与数据库连接池的区别","uri":"https://PI-KA-CHU.github.io/2019/06/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E6%B1%A0%E7%9A%84%E5%8C%BA%E5%88%AB/","year":"2019"},{"content":"1、JAVA普通项目转换为JAVA WEB项目  2、JAVA WEB项目在Tomcat启动后的访问路径设置 ","id":28,"section":"posts","summary":"1、JAVA普通项目转换为JAVA WEB项目 2、JAVA WEB项目在Tomcat启动后的访问路径设置","tags":["","开发工具"],"title":"IDEA的JAVA WEB项目配置","uri":"https://PI-KA-CHU.github.io/2019/06/idea%E7%9A%84java-web%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE/","year":"2019"},{"content":"一、实现原理  利用Spring AOP面向切面编程的特性，在请求进入service应用层之前，根据请求的service层的方法名，判断使用读库或者写库，如find、get、list、query开头的标记为读库，其他的标记为写库，实现读写的自动分离。\n  二、代码实现 DataSource.java\n/** * 设置数据源注解//TODO */ @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface DataSource { // 写数据源 public static final String WRITE = \u0026#34;write\u0026#34;; // 读数据源 public static final String READ = \u0026#34;read\u0026#34;; String value() default \u0026#34;write\u0026#34;; } DataSourceHolder.java\n/** * 数据源操作 */ public class DataSourceHolder { // 线程本地环境 private static final ThreadLocal\u0026lt;String\u0026gt; dataSources = new ThreadLocal\u0026lt;String\u0026gt;(); // 设置数据源 public static void setDataSource(String customerType) { dataSources.set(customerType); } // 设置数据源为写库 public static void setWrite() { dataSources.set(\u0026#34;write\u0026#34;);; } // 设置数据库为读库 public static void setRead() { dataSources.set(\u0026#34;read\u0026#34;); } // 设置数据库为考勤数据库 public static void setAttendance() { dataSources.set(\u0026#34;attendanceDataSource\u0026#34;); } // 获取数据源 public static String getDataSource() { return (String) dataSources.get(); } // 清除数据源 public static void clearDataSource() { dataSources.remove(); } } DynamicDataSource.java\n/** * 获取数据源（依赖于spring） */ public class DynamicDataSource extends AbstractRoutingDataSource { @Override protected Object determineCurrentLookupKey() { return DataSourceHolder.getDataSource(); } } DataSourceAspect.java\n/** * 定义数据源的AOP切面，通过Service的方法名判断是应该走读库还是写库 * * @since 2019/05/26 * @author 曾博佳 */ public class DataSourceAspect { private Logger log = LoggerFactory.getLogger(DataSourceAspect.class); /** * 在进入 * * @param point */ public void before(JoinPoint point) { // 获取到当前执行的方法名 String methodName = point.getSignature().getName(); if(isRead(methodName)) { // 设置为读库 DataSourceHolder.setRead(); log.info(\u0026#34;标记为读库\u0026#34;); }else { // 设置为写库 DataSourceHolder.setWrite(); log.info(\u0026#34;标记为写库\u0026#34;); } } /** * 判断是否为读操作 * * @param methodName * @return */ public Boolean isRead(String methodName) { // 方法名以query、find、get、list开头的使用读库 return StringUtils.startsWithAny(methodName, \u0026#34;query\u0026#34;,\u0026#34;find\u0026#34;,\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;); } } spring-service.xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:context=\u0026#34;http://www.springframework.org/schema/context\u0026#34; xmlns:tx=\u0026#34;http://www.springframework.org/schema/tx\u0026#34; xmlns:aop=\u0026#34;http://www.springframework.org/schema/aop\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd\u0026#34;\u0026gt; \u0026lt;!-- 配置事务管理器 --\u0026gt; \u0026lt;bean id=\u0026#34;transactionManager\u0026#34; class=\u0026#34;org.springframework.jdbc.datasource.DataSourceTransactionManager\u0026#34;\u0026gt; \u0026lt;!-- 注入数据库连接池 --\u0026gt; \u0026lt;property name=\u0026#34;dataSource\u0026#34; ref=\u0026#34;dataSource\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;!-- 定义事务策略 --\u0026gt; \u0026lt;tx:advice id=\u0026#34;txAdvice\u0026#34; transaction-manager=\u0026#34;transactionManager\u0026#34;\u0026gt; \u0026lt;tx:attributes\u0026gt; \u0026lt;!--定义查询方法都是只读的 --\u0026gt; \u0026lt;tx:method name=\u0026#34;query*\u0026#34; read-only=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;tx:method name=\u0026#34;find*\u0026#34; read-only=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;tx:method name=\u0026#34;get*\u0026#34; read-only=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;tx:method name=\u0026#34;list*\u0026#34; read-only=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;!-- 主库执行操作，事务传播行为定义为默认行为 --\u0026gt; \u0026lt;tx:method name=\u0026#34;save*\u0026#34; propagation=\u0026#34;REQUIRED\u0026#34; /\u0026gt; \u0026lt;tx:method name=\u0026#34;update*\u0026#34; propagation=\u0026#34;REQUIRED\u0026#34; /\u0026gt; \u0026lt;tx:method name=\u0026#34;delete*\u0026#34; propagation=\u0026#34;REQUIRED\u0026#34; /\u0026gt; \u0026lt;!--其他方法使用默认事务策略 --\u0026gt; \u0026lt;!-- \u0026lt;tx:method name=\u0026#34;*\u0026#34; /\u0026gt; --\u0026gt; \u0026lt;/tx:attributes\u0026gt; \u0026lt;/tx:advice\u0026gt; \u0026lt;!-- 定义AOP切面处理器 --\u0026gt; \u0026lt;bean class=\u0026#34;com.zhsj.common.db.DataSourceAspect\u0026#34; id=\u0026#34;dataSourceAspect\u0026#34; /\u0026gt; \u0026lt;aop:config\u0026gt; \u0026lt;!-- 定义切面，所有包的service的所有方法 --\u0026gt; \u0026lt;aop:pointcut id=\u0026#34;txPointcut\u0026#34; expression=\u0026#34;execution(* com.zhsj.*.service.*.*(..))\u0026#34; /\u0026gt; \u0026lt;!-- 应用事务策略到Service切面 --\u0026gt; \u0026lt;aop:advisor advice-ref=\u0026#34;txAdvice\u0026#34; pointcut-ref=\u0026#34;txPointcut\u0026#34;/\u0026gt; \u0026lt;!-- 将切面应用到自定义的切面处理器上，-9999保证该切面优先级最高执行 --\u0026gt; \u0026lt;aop:aspect ref=\u0026#34;dataSourceAspect\u0026#34; order=\u0026#34;-9999\u0026#34;\u0026gt; \u0026lt;aop:before method=\u0026#34;before\u0026#34; pointcut-ref=\u0026#34;txPointcut\u0026#34; /\u0026gt; \u0026lt;/aop:aspect\u0026gt; \u0026lt;/aop:config\u0026gt; ","id":29,"section":"posts","summary":"一、实现原理 利用Spring AOP面向切面编程的特性，在请求进入service应用层之前，根据请求的service层的方法名，判断使用读库或","tags":["","spring"],"title":"Spring AOP实现自动读写分离","uri":"https://PI-KA-CHU.github.io/2019/05/spring-aop%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/","year":"2019"},{"content":"一、写在前面 Flume在Windows上的使用还真是各种不方便，但是没办法，项目就是搭建在Windows服务器上的，前面的文章是使用自定义拦截器解决了日期划分的问题，接下来解决的是实时监控日志更新的问题\n 二、Flume的三种Source   Exec Source： 可通过tail -f命令去tail住一个文件，然后实时同步日志到sink。但存在的问题是，当agent进程挂掉重启后，会有重复消费的问题。可以通过增加UUID来解决，或通过改进ExecSource来解决。\n  Spooling Directory Source： 监视一个目录，同步目录中的新文件到sink，同步完的文件会打上后缀标识符号（默认为.COMPLETE）,适用于同步新文件，但是不适用于对实时追加日志文件的监督。上一篇文章就是用Spooldir source，但是日志是实时追加的，监控时会因为无法关闭而报异常。\n  Taildir Source：\n 1.7版flume新加入的source，解决了多个文件夹监督的问题，同时可以通过目录直接用正则表达式直接监督符合的文件，而且不会存在重复消费的问题，数据已经读取的位置信息被保存，在另一个文件中，所以即使完成也不会为文件加后缀，可实时监控多个文件。本编文章使用的就是Taildir source（真的好用）。 为了唯一标示一个文件，该source利用操作系统inode的方式获得文件的一个id，目前仅采用unix的方式获取，不支持window，需要更改源码添加window获取inode的方法    Taildir source在Windows环境下的使用   flume虽然强大，但是有点坑爹的是不太兼容Windows平台，比如Taildir Source源码中在inode的处理上是在Linux环境下实现的，如果要在Windows上使用需要重新修改并编译源码，下面讲一下自己的一个编译过程（具体流程按照下面参考文章即可成功）。\n  参考： https://www.jianshu.com/p/5a53c002b1dd\n   三、问题及解决   下载flume源码后，使用编译器打开项目（本人使用的是IDEA，以maven项目的形式导入）\n  导入项目后可以只下载flume-ng-source的maven依赖包即可，就是说不用整个项目都编译成功，可以通过cmd进入该项目下的flume-ng-source根目录，将修改完的项目（按照上面参考）进行编译即可。   maven编译时可能会出现下面格式异常，使用编译命令mvn clean install -Dcheckstyle.skip=true即可解决。   flume传输文件到HDFS后，HDFS中保存的文件有.tmp的后缀，原因是文件仍被打开着，即仍被flume监视及占用着，此时存储在的字节大小是无法正常读取到的，解决方法（flume配置）：\n  //18000秒内如果文件没有被追加上传，则flume关闭文件，此时HDFS文件可以显示正常文件大小 a1.sinks.k2.hdfs.idleTimeout = 18000  参考： https://bit1129.iteye.com/blog/2186026  ","id":30,"section":"posts","summary":"一、写在前面 Flume在Windows上的使用还真是各种不方便，但是没办法，项目就是搭建在Windows服务器上的，前面的文章是使用自定义拦","tags":["大数据"],"title":"Flume收集日志 - windows（实战版二）","uri":"https://PI-KA-CHU.github.io/2019/05/flume%E6%94%B6%E9%9B%86%E6%97%A5%E5%BF%97-windows%E5%AE%9E%E6%88%98%E7%89%88%E4%BA%8C/","year":"2019"},{"content":"flume官方文档：\n http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html  技术博客（较全面的介绍）：\n http://www.51niux.com/?id=197   一、Flume的核心组件  source：用于对源文件的监控（即数据的传入点） channel：用于event数据的传输（即传输管道） sink：用于event数据的输出（即输出点）  1.1 Flume的source类型   https://www.cnblogs.com/swordfall/p/8254271.html\n  https://www.cnblogs.com/qingyunzong/p/8995554.html\n  http://flume.apache.org/FlumeUserGuide.html#flume-sources（官方文档）\nsource类型的种类比较多，上面是关于各种类型的介绍，比较常用的是avor、thrift、exec、jms、spool，本次记录使用的是spool，spool是对本地文件的监视，即监视文件夹中的文件，将新加入的文件进行处理上传，成功处理后的文件默认后缀会加上**.COMPLETED**（可自定义修改，也可修改为在上传后删除）。\n  1.2 Flume的chancel类型  Memory Channel：基于内存传输，实现高速吞吐，但无法保证数据的完整性。 JDBC Channel：事件会被持久化（存储）到可靠的数据库里，目前支持嵌入式Derby数据库，但是该数据库使用起来不太方便，目前不适用于生产环境。 File Channel：基于磁盘传输（持久化隧道），将事件存储于磁盘中，即使宕机也能保证数据的完整性，即数据不会丢失。 Psuedo Transaction Channel（不常见） 相关参数及详情参考最上方的官方文档及技术博客  1.3 Flume的sink类型   https://www.cnblogs.com/swordfall/p/8157766.html\nsink类型同样有很多种，如hdfs sink、hbase sink、avor sink、logger sink等（详情参考上面链接），本案例使用的是hdfs sink，即将日志数据通过flume收集传输到hdfs。\n   二、Flume的常见使用 2.1 多层代理 2.2 并流  从大量Web服务器收集的日志发送给写入HDFS集群的十几个代理，即收集服务器集群的日志文件，汇流到代理flume并存入hdfs  2.3 多路复用  将事件流多路复用到一个或多个目的地。这是通过定义可以复制或选择性地将事件路由到一个或多个信道的流复用器来实现的。即同一个数据源的日志文件我们可能需要分配到多处进行处理，或者说有多个子系统需要使用到，此时可以使用到多路复用，sink指向不同的目的地。   三、flume的相关配置 3.1 flume-env.sh文件  如果Hadoop相关支持包已经导入到flume的lib中，则配置flume的lib路径  $FLUME_CLASSPATH=\u0026#34;D:\\flume\\apache-flume-1.8.0-bin\\lib\u0026#34;  如果Flume/lib中没有Hadoop相关支持包，则需要指定本地Hadoop路径  HADOOP_HOME=\u0026#34;G:\\\\hadoop-2.5.0-cdh5.3.6\u0026#34; FLUME_CLASSPATH=\u0026#34;$HADOOP_HOME/share/hadoop/hdfs//hadoop-hdfs-2.5.0-cdh5.3.1.jar\u0026#34; 3.2 example.conf文件  # flume相关组件的声明 a1.sources = r1 a1.sinks = k1 k2 a1.channels = c1 c2 # 设置source r1 # fileHeader：是否添加绝对路径的标头 # inputCharset：传入event的编码格式 # ignorePattern：忽略的文件名，此处匹配的文件不会被传输（可用正则） # includePattern：包含的文件名，此处匹配的文件会被传输（可用正则） a1.sources.r1.type = spooldir a1.sources.r1.spoolDir = E:\\\\BigDataSolf\\\\log\\\\uploadTest a1.sources.r1.fileHeader = true a1.sources.r1.inputCharset = GBK # a1.sources.r1.ignorePattern = catalina.* a1.sources.r1.includePattern = app.* # 配置r1拦截器(自定义拦截器)，自定义拦截器需要打成jar包后放在flume的lib文件夹中，后面会有自定义拦截器的介绍。 # type：拦截器类型（此处为自定义的拦截器） # regexs和datePattern：都是自定义的正则参数 a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type = com.bnuz.flume.MyLogBuilder a1.sources.r1.interceptors.i1.regexs = localhost_access_log,app a1.sources.r1.interceptors.i1.datePattern = [0-9]{4}-[0-9]{2}-[0-9]{2} # 配置多路复用选择器（根据头信息进行分流） # header：指header中的参数名称（类似于key） # mapping.localhost_access_log：localhost_access_log相当于value，即如果myHeader匹配的value为localhost_access_log就使用c1管道，如果是app则使用c2管道。 a1.sources.r1.selector.type = multiplexing a1.sources.r1.selector.header = myHeader a1.sources.r1.selector.mapping.localhost_access_log = c1 a1.sources.r1.selector.mapping.app = c2 # Use a channel which buffers events in memory for c1 a1.channels.c1.type = memory a1.channels.c1.capacity = 8000 a1.channels.c1.transactionCapacity = 1000 # Use a channel which buffers events in memory for c2 a1.channels.c2.type = memory a1.channels.c2.capacity = 8000 a1.channels.c2.transactionCapacity = 1000 # configure the sink k1 # %{fileDate}是自定义拦截器中加入的日期参数，用于将日志文件按照日期保存到不同的文件夹中 # 将access_log文件传输到access_log文件夹中 # fileSuffix：保存的文件前缀 # fileSuffix：保存的文件后缀 # useLocalTimeStamp：为每个文件名加上存储时的时间戳 a1.sinks.k1.type=hdfs a1.sinks.k1.hdfs.path=hdfs://127.0.0.1/logs/zhsj/%{fileDate}/access_log a1.sinks.k1.hdfs.fileType=DataStream a1.sinks.k1.hdfs.writeFormat=TEXT a1.sinks.k1.hdfs.filePrefix=%{fileDate} a1.sinks.k1.hdfs.fileSuffix=.log a1.sinks.k1.hdfs.rollInterval=0 a1.sinks.k1.hdfs.rollSize=10240000 a1.sinks.k1.hdfs.rollCount=0 a1.sinks.k1.hdfs.minBlockReplicas=1 a1.sinks.k1.hdfs.useLocalTimeStamp = true # configure the sink k2 a1.sinks.k2.type=hdfs a1.sinks.k2.hdfs.path=hdfs://127.0.0.1/logs/zhsj/%{fileDate}/app_log/ a1.sinks.k2.hdfs.fileType=DataStream a1.sinks.k2.hdfs.writeFormat=TEXT a1.sinks.k2.hdfs.filePrefix=%{fileDate} a1.sinks.k2.hdfs.fileSuffix=.log a1.sinks.k2.hdfs.rollInterval=0 a1.sinks.k2.hdfs.rollSize=10240000 a1.sinks.k2.hdfs.rollCount=0 a1.sinks.k2.hdfs.minBlockReplicas=1 a1.sinks.k2.hdfs.useLocalTimeStamp = true # Bind the source r1 and sink to the channel # 绑定不同的channel、sink和source间的关系 a1.sources.r1.channels = c1 c2 a1.sinks.k1.channel = c1 a1.sinks.k2.channel = c2  四、flume的自定义拦截器  此拦截器用于对event的简单过滤，并为不同文件和时间的event进行标识，flume配置文件（如上）会根据参数的不同保存到不同的文件夹中。将写好的拦截器打包成jar包并保存到flume的lib文件夹中。  MyLogBuilder（拦截器入口）\nimport org.apache.flume.Context; import org.apache.flume.interceptor.Interceptor; import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class MyLogBuilder implements Interceptor.Builder{ private Logger log = LoggerFactory.getLogger(MyLogBuilder.class); private String regexs = \u0026#34;\u0026#34;; private String datePattern = \u0026#34;\u0026#34;; //获取传入的相关参数 @Override public void configure(Context context) { regexs = context.getString(\u0026#34;regexs\u0026#34;); datePattern = context.getString(\u0026#34;datePattern\u0026#34;); log.info(\u0026#34;------获取到拦截器参数pattern为：\u0026#34; + regexs); } //调用拦截器并传入参数 @Override public Interceptor build() { log.info(\u0026#34;------初始化自定义拦截器\u0026#34;); return new MyLogInterceptor(regexs,datePattern); } } MyLogInterceptor（拦截器的实现）\nimport org.apache.commons.io.Charsets; import org.apache.flume.Event; import org.apache.flume.interceptor.Interceptor; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.util.*; import java.util.regex.Matcher; import java.util.regex.Pattern; public class MyLogInterceptor implements Interceptor { private Logger log = LoggerFactory.getLogger(MyLogInterceptor.class); //文件地址（包含文件名） private String filePath = \u0026#34;\u0026#34;; //文件类型过滤参数 private String regexs = \u0026#34;\u0026#34;; //日期格式（正则表达式） private String datePattern = \u0026#34;\u0026#34;; //日志日期键值 private final String FILE_DATE_KEY = \u0026#34;fileDate\u0026#34;; private String fileDateValue = \u0026#34;\u0026#34;; //日志类型键值 private final String HEADER_KEY = \u0026#34;myHeader\u0026#34;; private String headerValue = \u0026#34;\u0026#34;; MyLogInterceptor(String regexs,String datePattern){ this.regexs = regexs; this.datePattern = datePattern; } @Override public void initialize() { log.info(regexs + \u0026#34;拦截器initialize方法执行\u0026#34;); } /** * 对每个事件进行拦截修改 * * @param event 文件事件 * @return */ @Override public Event intercept(Event event) { /** * 根据不同的日志文件内容进行不同的正则表达式匹配 * access日志文件每天生成一次，则直接使用文件名进行日期匹配（文件名包含日志） * app日志文件几天生成一次文件，根据具体日志数据进行匹配 */ Pattern pattern = pattern = Pattern.compile(datePattern); Matcher matcher = null; if (filePath.contains(\u0026#34;localhost_access_log\u0026#34;)){ //通过正则表达式获取日志创建日期 matcher = pattern.matcher(filePath); }else if(filePath.contains(\u0026#34;app.\u0026#34;)){ //日志具体数据 String bodyData = new String(event.getBody(), Charsets.UTF_8); log.info(\u0026#34;收集到的数据为：{}\u0026#34;,bodyData); matcher = pattern.matcher(bodyData); } if (matcher != null \u0026amp;\u0026amp; matcher.find()){ log.info(\u0026#34;匹配到日期为：\u0026#34; + matcher.group(0)); fileDateValue = matcher.group(0); event.getHeaders().put(HEADER_KEY,headerValue); event.getHeaders().put(FILE_DATE_KEY,fileDateValue); return event; }else { log.info(\u0026#34;信息被过滤\u0026#34;); return null; } } @Override public List\u0026lt;Event\u0026gt; intercept(List\u0026lt;Event\u0026gt; list) { List\u0026lt;Event\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); //获取文件名 filePath = list.get(0).getHeaders().get(\u0026#34;file\u0026#34;); //将传入的过滤参数进行切割 String[] regexArr = regexs.split(\u0026#34;,\u0026#34;); for (String s : regexArr) { if (filePath.contains(s)) { log.info(\u0026#34;合法文件，允许放行 - {}\u0026#34;, filePath); //设置header键值，用于多路复用（source根据头键值不同进行不同channel的选择） headerValue = s; Event event1; for (Event event : list){ event1 = intercept(event); if (event1 != null){ result.add(event1); } } return result; } } log.info(\u0026#34;拦截器拦截非法文件 - {}\u0026#34;,filePath); return result; } @Override public void close() { log.info(\u0026#34;拦截器关闭\u0026#34;); } }  五、windows下flume的启动   打开cmd命令进入flume的bin文件中\n  使用如下命令启动：（windows启动需要注释掉部分代码）\n  flume-ng agent --conf ../conf --conf-file ../conf/example.conf --name a1 -property flume.root.logger=INFO,console  六、异常及解决：  参考：Flume收集日志 - windows（配置版）  ","id":31,"section":"posts","summary":"flume官方文档： http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html 技术博客（较全面的介绍）： http://www.51niux.com/?id=197 一、Flume的核心组件 source：用于对源文件的监控（即数据的传入点） channel：","tags":["大数据"],"title":"Flume收集日志 - windows（实战版一）","uri":"https://PI-KA-CHU.github.io/2019/05/flume%E6%94%B6%E9%9B%86%E6%97%A5%E5%BF%97-windows%E5%AE%9E%E6%88%98%E7%89%88%E4%B8%80/","year":"2019"},{"content":"Flume的一些常见参数：  https://blog.csdn.net/realoyou/article/details/81514128  Windows配置Flume  https://www.cnblogs.com/edisonchou/p/4445491.html https://blog.csdn.net/wateryouyo/article/details/82057082 https://blog.csdn.net/woshixiazaizhe/article/details/80605293 https://www.cnblogs.com/chevin/p/8491721.html  windows环境下使用flume的权限问题：  http://www.huqiwen.com/2013/07/18/hdfs-permission-denied/  commons-io缺包问题：  http://mangocool.com/1464850911377.html  event未捕获到时间戳问题  http://blog.sina.com.cn/s/blog_db77b3c60102vrzt.html  hadoop缺包问题  https://www.cnblogs.com/zhutianye/articles/5012346.html  flume上传到hdfs出现大量小文件问题（minBlockReplicas参数的配置）  https://blog.csdn.net/whdxjbw/article/details/80606917  JAVA程序log4j整合flume实现日志上传到HDFS  https://blog.csdn.net/hzs33/article/details/79429087 https://blog.csdn.net/antgan/article/details/52087926  Flume分层收集日志  https://blog.csdn.net/lbship/article/details/84951829  Flume异常及解决：   异常：java.nio.charset.MalformedInputException: Input length = 1 \n  解决：编码设置问题：https://blog.csdn.net/qq_32967001/article/details/66973094\n  异常：java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n  解决：hadoop环境变量未配置问题：https://blog.csdn.net/baidu_19473529/article/details/54693523\n  ","id":32,"section":"posts","summary":"Flume的一些常见参数： https://blog.csdn.net/realoyou/article/details/81514128 Windows配置Flume https://www.cnblogs.com/edisonchou/p/4445491.html https://blog.csdn.net/wateryouyo/article/details/82057082 https://blog.csdn.net/woshixiazaizhe/article/details/80605293 https://www.cnblogs.com/chevin/p/8491721.html windows环境下使用flume的权限问题： http://www.huqiwen.com/2013/07/18/hdfs-permission-denied/ commons-io缺包问题","tags":["大数据"],"title":"Flume收集日志 - windows（配置版）","uri":"https://PI-KA-CHU.github.io/2019/04/flume%E6%94%B6%E9%9B%86%E6%97%A5%E5%BF%97-windows%E9%85%8D%E7%BD%AE%E7%89%88/","year":"2019"},{"content":"前言 查看日志是重要的排查异常的方式，在出现error时查找日志可以快速确定出错的原因，而在大数据时代里，随着用户的访问上升，日志数据量也呈爆炸式上升，通过日志可以分析出用户的行为特征，如兴趣，情感，访问时间等，可以通过接口响应时间分析用户体验等，从而为系统制定调优方案。日志在大数据时代下的发挥着重要的作用，日志的规范及格式则是获取日志数据的关键，通过设置写入的日志文件使得在进行数据挖掘时能够获取到相应的数据。\n日志配置 - localhost_access_log  进入tomcat安装目录下的/conf/server.xml文件，找到如下文段并将其取消注释  \u0026lt;Valve className=\u0026#34;org.apache.catalina.valves.AccessLogValve\u0026#34; directory=\u0026#34;logs\u0026#34; prefix=\u0026#34;localhost_access_log\u0026#34; suffix=\u0026#34;.txt\u0026#34; pattern=\u0026#34;%h %l %u %t \u0026amp;quot;%r\u0026amp;quot; %s [agent- %{User-Agent}i -] %b %Dms\u0026#34; fileDateFormat=\u0026#34;yyyy-MM-dd.HH.mm\u0026#34; /\u0026gt; 属性：\npattern属性的参数：\n  设置日志的生成周期\n通过设置fileDateFormat=\u0026quot;yyyy-MM-dd.HH.mm\u0026quot;属性的值可以设置日志的生成时间，如yyyy-MM-dd.HH.mm表示每分钟生成一次，yyyy-MM-dd表示每天生成一次\n   访问者IP设置  正常情况下：pattern=\u0026quot;%h\u0026quot; nginx情况下：pattern=\u0026quot;%{X-Real-IP}i\u0026quot;(根据nginx中的配置修改)     设置User-Agent  pattern=\u0026quot;%{User-Agent}i\u0026quot;    参考  https://blog.csdn.net/qq_30121245/article/details/52861935 https://jingyan.baidu.com/article/36d6ed1f713b9d1bcf4883e1.html https://yq.aliyun.com/articles/518410  ","id":33,"section":"posts","summary":"前言 查看日志是重要的排查异常的方式，在出现error时查找日志可以快速确定出错的原因，而在大数据时代里，随着用户的访问上升，日志数据量也呈爆","tags":["log"],"title":"Tomcat配置日志格式","uri":"https://PI-KA-CHU.github.io/2019/03/tomcat%E9%85%8D%E7%BD%AE%E6%97%A5%E5%BF%97%E6%A0%BC%E5%BC%8F/","year":"2019"},{"content":"搭建参考  参考： https://www.cnblogs.com/edisonchou/p/4405906.html 根据这篇文章可以正常搭建平台，HBase的配置与Hadoop类似，分别修改hbase-env.sh和hbase-site.xml即可（Linux环境下搭建） Window客户端（IDEA）连接HBase，需要导入HBase下的lib目录，可参考 将搭建好的HBase的hbase-site.xml文件复制粘贴到项目的resource目录下，跟搭建hadoop时候类似  Bug  java.net.UnknownHostException: unknown host: node-1异常  解决：需要修改Window下的地址映射，在C:\\WINDOWS\\system32\\drivers\\etc\\hosts文件中添加如下信息：192.125.168.135 node-1（地址对应虚拟机HBase节点所在主机地址） 参考： https://blog.csdn.net/lifuxiangcaohui/article/details/40861079    ","id":34,"section":"posts","summary":"搭建参考 参考： https://www.cnblogs.com/edisonchou/p/4405906.html 根据这篇文章可以正常搭建平台，HBase的配置与Hadoop类似，分别修改hbase-env.sh和hbase-site.x","tags":["大数据"],"title":"HBase的搭建及连接（IDEA）","uri":"https://PI-KA-CHU.github.io/2019/03/hbase%E7%9A%84%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%BF%9E%E6%8E%A5idea/","year":"2019"},{"content":"Hbase简介  HBase利用Hadoop的HDFS作为其文件存储系统，提供高可靠性、高性能、列存储，可伸缩、实时读写的分布式的数据库系统，利用Zookeeper作为其协同服务，适合于非结构化数据存储的分布式数据库。  Hbase特点   存储特点：\n 面向列存储 存储量大：一个表可以有上亿行，上百万列（列多时，插入变慢） 数据稀疏：对于空（null）的列并不占用存储空间，因此，表可以设计的非常稀疏  补充：稀疏数据是指在数据集中绝大多数数值缺失或者为零的数据   数据类型单一：HBase中的数据都是字符串，没有类型      表的构造：\n 每张表里面有多个rowkey（行键），每个rowkey包含多个columnFamily（列族），每个coloumnFamily由一个或者多个cloumn（列）组成。 数据查询结果：Row key + column family:column + timestamp + value     传统关系型数据库和列式数据库的区别：  HBase的原理   HBase的属性\n Row Key  表中每条记录的“主键” 方便快速查找   Column Family  拥有一个名称(string) 包含一个或者多个相关列   Column  属于某一个Column Family 包含在某一列中   Timestamp  每个rowKey有唯一一个 类型为long 默认值是系统时间戳   Value  某一列属性的值（String）        HBase的存储\n https://www.cnblogs.com/duanxz/p/3154487.html Table中的所有行都按照row key的字典序排列，类型为byte字节流，Table在行的方向上分割为多个region；    region是按照大小分割的，每个表开始只是一个region，随着数据的增多，region不断增大，当达到一个值的时候会等分成两个region。     region是分布式存储的最小单元，但并不是存储的最小单元。\n  region由一个或者多个Store组成，每个Store保存一个columns family；\n  每个store由一个MemStore和多个StoreFIle组成组成；\n  MemStore存储在内存中，StoreFile存储在hdfs上；\n 首先数据存储到MemStore上，当数据量达到一定的大小，再flush到storefile上，形成一个StoreFile。         HBase支持的操作\n  所有操作都是基于row key的；\n  支持CRUD（create、read、update、delete）和scan\n 单行操作：put、get、scan 多行操作：scan、multiPut    没有内置join操作，需要使用mapreduce解决\n    HBase的架构   架构图   架构角色分析\n  Client：包含访问HBase的接口，并维护cache来加快对HBase的访问（即缓存遍历hbase:meta的区域数据）\n  Zookeeper：\n 保证任何时候，集群中只有一个master 存储【所有region的寻址入口】（即hbase:meta的位置）  hbase:mate：维护着当前集群上所有区域的列表、状态和位置   实时监控RegionServer的上线和下线信息，并实时通知给Master存储HBase的schema和table的元数据。    Master：\n 为Region server 分配region 负责Region server的负载均衡 发现失效的Region server并重新分配其上的region 管理用户对table的增删该查操作    Region Server：\n 维护region，处理这些region的IO请求 负责切分在运行过程中变得过大的region        HBase的容错性\n  Master容错：Zookeeper重新选择一个新的Master\n 无Master过程中：数据读取仍照常进行 无Master过程中：region切分、负载均衡等无法进行    RegionServer容错：定时的向Zookeeper汇报心跳，如果一段时间内未出现心跳，Master将会将该RegionServer的Region 重新分配到其他RegionServer上。\n 失效服务器上的**“预写”日志由主服务器进行分割并派送给其他RegionServer**。    Zookeeper容错：Zookeeper是一个可靠地服务，一般配置3或5个Zookeeper实例。\n    分区查看  进入HBase Master主页http://hh:60030/rs-status可以查看  MapReduce如果以HBase为源输入，其Map数量由Region数量决定，如果源输入是HDFS，其Map数量是由HDFS文件分片决定。    参考  整理自： http://wenku.uml.com.cn/document.asp?fileid=17427\u0026amp;partname=%B4%F3%CA%FD%BE%DD  ","id":35,"section":"posts","summary":"Hbase简介 HBase利用Hadoop的HDFS作为其文件存储系统，提供高可靠性、高性能、列存储，可伸缩、实时读写的分布式的数据库系统，利","tags":["大数据"],"title":"HBase理论学习与分析","uri":"https://PI-KA-CHU.github.io/2019/03/hbase%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%88%86%E6%9E%90/","year":"2019"},{"content":"一、写在前面 为什么要进行数据切分：  随着互联网应用的广泛普及，海量数据的存储和访问成为了系统设计的瓶颈问题。对于一个大型的互联网应用，每天几十亿的PV无疑对数据库造成了相当高的负载。对于系统的稳定性和扩展性造成了极大的问题。通过数据切分来提高网站性能，横向扩展数据层已经成为架构研发人员首选的方式。\n 数据库优化的策略  水平切分数据库，可以降低单台机器的负载，同时最大限度的降低了了宕机造成的损失。 通过负载均衡策略，有效的降低了单台机器的访问负载，降低了宕机的可能性； 通过集群方案，解决了数据库宕机带来的单点数据库不能访问的问题； 通过读写分离策略更是最大限度了提高了应用中读取（Read）数据的速度和并发量。   二、MySQL分库分表方案 1、利用merge存储引擎来实现分表（水平分表）   merge引擎实现MySQL分表，这种方法比较适用于没有事先考虑分表，而随着数据量增大，查询速度减慢的情况\n  merge的要求：\n 合并的表使用的必须是MyISAM引擎 表的结构必须一致，包括索引、字段类型、引擎和字符集    示例\n   2、简单的MySQL主从复制   MySQL的主从复制解决了数据库的读写分离，并很好的提升了读的性能\n 主数据库负责写，从数据库负责读，主库写入数据后会同步到从库     仍存在的问题：\n 写入无法扩展和缓存 主从的数据复制存在延迟 锁表率上升 表变大，缓存率下降     3、MySQL垂直分区  如果业务足够独立，可以将不同业务的数据垂直切割到不同的数据库，起到负载分流的作用，大大提升了数据库的吞吐能力，垂直切割如下图    4、MySQL水平分片（sharding）  切片方式：  物理切片：通过路由规则访问特定的数据库 数据切片: 对数据进行一系列的切分规则，将数据分布到数据库的不同表中      基于水平分片的三种分库方式和规则   按号段分：如user_id的1~1000对应DB1，1001~200对应DB2\u0026hellip;\n 优点：可迁移部分数据 缺点：数据分布不均匀    hash取摸分：对user_id的哈希进行取模（如hash(user_id) % n），n为要分的数据库数量\n 优点：数据分布均匀 缺点：数据迁移的时候比较麻烦，不能按机器性能分摊数据    在认证库中保存数据库配置：建立一个DB，这个DB单独保存user_id到数据库的映射关系，每次访问的时候都先查询下认证库，从而得到user_id存储的DB信息。\n 优点：灵活性强，一对一的关系 缺点：每次查询都需要先进行一次查询，性能大打折扣         分布式数据方案提供功能如下：\n 提供分库规则和路由规则（RouteRule简称RR），可将上面提供的三种分片规则直接内嵌入系统。 引入集群（Group）概念，保证数据的高可用性。 引入负载均衡策略（LocalBalancePollcy，简称LB） 引入集群节点可用性探测机制，对单点机器的可用性进行定时的定制，以保障LB策略的正确实施，确保系统的高度稳定性。 引入读写分离，提高数据的查询速度。       哈希取模分片简介：\n  将用户按照一定的规则**（按ID哈希）分组，并把该用户的数据存储到一个数据库分片**中，即一个sharding，随着用户的增加，只需要简单的配置一台服务器即可。原理如下图：   获取分片存储的信息如下（先创建一张用户和shard对应的数据表，用于查找用户的shard id，再从对应shard id查找相关数据）：      5、对冷热数据的处理   举例：在一个博客系统中，文章标题，作者，分类，创建时间等，是变化频率慢，查询次数多，而且最好有很好的实时性的数据，我们把它叫做冷数据。而博客的浏览量，回复数等，类似的统计信息，或者别的变化频率比较高的数据，我们把它叫做活跃数据。\n  处理方法：\n 存储引擎的使用不同，冷数据使用MyIsam可以有更好的查询数据。活跃数据，可以使用Innodb ,可以有更好的更新速度。 对冷数据进行更多的从库配置，因为更多的操作是查询，这样来加快查询速度。对热数据，可以相对有更多的主库的横向分表处理。 对于一些特殊的活跃数据，也可以考虑使用memcache,redis之类的缓存，等累计到一定量再去更新数据库     参考  https://www.cnblogs.com/sunny3096/p/8595058.html http://wenku.uml.com.cn/document.asp?fileid=3998\u0026amp;partname=%CA%FD%BE%DD%BF%E2 (推荐)  ","id":36,"section":"posts","summary":"一、写在前面 为什么要进行数据切分： 随着互联网应用的广泛普及，海量数据的存储和访问成为了系统设计的瓶颈问题。对于一个大型的互联网应用，每天几十","tags":["mysql"],"title":"MySQL分表分库学习","uri":"https://PI-KA-CHU.github.io/2019/03/mysql%E5%88%86%E8%A1%A8%E5%88%86%E5%BA%93/","year":"2019"},{"content":"环境  服务器：CentOS 7 Hadoop版本：2.7.7 cli端：Windows 10 开发工具：intellij IDEA  连接前提  已成功创建Hadoop集群，即能正常访问hdfs和mapreduce的web界面（本人是在虚拟机建的集群） 创建Maven项目并新建java class WordCount（此处以单词统计为例子）  Window环境准备   下载hadoop压缩包并解压到Window下（版本为集群所用的hadoop版本）\n 下载地址：https://hadoop.apache.org/releases.html    下载winutils-master（Windows环境需要），解压后将其中对应版本的bin目录下的文件复制到第一步解压的hadoop文件的bin目录下（覆盖原有的）\n 下载地址：http://www.pc0359.cn/downinfo/92994.html    将winutils-master中的hadoop.dll文件复制到Windows的C:\\Windows\\System32目录下\n 参考：https://blog.csdn.net/zimojiang/article/details/80473201    配置Window环境变量\n 在系统环境变量中加入：HADOOP_HOME=D:\\yangjm\\Code\\study\\hadoop\\hadoop-2.6.0 在Path中加入：%HADOOP_HOME%\\bin    IDEA相关准备  pom.xml中加入相关依赖   \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-hdfs\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.7\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.7\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt;   将搭建好的hadoop集群的配置文件log4j.properties和core-site.xml复制后粘贴到IDEA创建的Java项目的resource文件夹下\n  为项目导入本地hadoop的相关jar包（下面图片来自腾讯云社区）\n 打开Moudle设置    导入hadoop相关jar包（在hadoop的\\share\\hadoop\\common目录下）   给导入的lib取个名字，如hadoop等     设置运行参数：\n 以空格隔开，第一个为hdfs中需要处理的文件，第二个为处理结果保存在hdfs的路径（hdfs地址在core-site.xml中）     以上配置完后基本可以正常运行\n  可能出现的Bug  运行时出现权限问题  解决：在JAVA代码中修改为有高级权限的用户 参考：https://blog.csdn.net/diqijiederizi/article/details/82753573    参考  https://blog.csdn.net/mm_bit/article/details/52118904 https://cloud.tencent.com/developer/article/1024534  ","id":37,"section":"posts","summary":"环境 服务器：CentOS 7 Hadoop版本：2.7.7 cli端：Windows 10 开发工具：intellij IDEA 连接前提 已成功创建Hadoop集","tags":["大数据","开发工具"],"title":"IDEA远程连接Hadoop集群","uri":"https://PI-KA-CHU.github.io/2019/03/idea%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5hadoop%E9%9B%86%E7%BE%A4/","year":"2019"},{"content":"一、Hadoop 2.x的相关组件： HDFS（Hadop分布式文件系统）：负责分布式数据存储\n NameNode：存储元信息，包括文件块的分块数量及文件块的储存位置 DataNode：管理文件块的存储 SeconddaryNameNode：  YARN：负责资源管理，即任务调度\n ResourceManager NodeManager  MapReduce：离线计算框架，负责数据的运算\nHDFS和YARN逻辑上分离，但物力物理上总是在一起。\n 二、hadoop的相关配置文件：  default.xml（可在官网查看）：这配置了hadoop默认的配置选项，如果用户没有更改，那么里面的选项将会生效。 site.xml：配置了用户需求自定义的配置选项 区别：site中的配置选项优先级大于default中的，如果有配置的话会覆盖默认的配置选项。   三、关于HDFS的格式化：  首次启动hdfs的时候需要进行格式化 格式化的本质是机进行文本系统的初始化操作，创建一些自己所需要的配置文件 格式化后，若集群启动成功，则后续不要再进行格式化操作，否则可能导致对应不上标识而导致集群创建失败   四、HDFS的创建思路  （大数据存储）传统文件的存储模式缺点：  上传下载速度慢（文件太大） 遇到存储瓶颈，即使进行纵向扩展（扩充磁盘和内存），也会存在极限   HDFS的解决思路：  将文件进行分割切块，将大文件分割成小块进行分布式上传存储 进行横向扩展，添加集群机器进行处理   HDFS解决传统模式后出现的问题及解决：  获取文件的成本变高：切块存储后，需要一个记录文件切割以及保存在哪里的相应信息的数据。 单点故障：某一机器的故障将导致无法切块的文件无法复原。  解决：将文件进行备份存储，在多个机器上备份文件块，在其中一台机器挂掉后，可以到另外机器获取       五、HDFS的重要特性   HDFS的本质：\n 是一个文件系统，同于存储文件，通过统一的命名空间目录树来定位文件。 是分布式的，由很多服务器联合起来是实现其功能。    master / slave架构：\n 一个HDFS集群是由一个Namenode和一定数目的Datanode组成。Namenone是HDFS集群主节点，Datanode是HDFS集群从节点，两者共同协调完成分布式的文件存储服务。    分块存储：\n HDFS中的文件在物理上是分块存储（block）的，块的大小可以通过配置参数来规定，hadoop2.x中默认大小是128M.    名字空间（NameSpace）：\n HDFS支持传统的层次型文件组织结构，用户或者应用程序可以创建目录，然后将文件保存在这些目录里，即用户可以创建、删除、移动和重命名文件。 Namenode负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被Namenode记录下来。 HDFS会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，如\u0026quot;hdfs://namenode:port/etc/host.conf\u0026quot;    Namenode元数据管理：\n 元数据：目录结构及文件分块位置信息 Namenode负责维护整个hdfs文件系统的目录树结构及每一个文件所对应的block块信息（block的id及所在的datanode服务器）    Datanode数据存储：\n 文件的各个block的具体存储管理由datanode节点承担，每个block都可以在多个datanode上。 Datanode需要定时向Namenode汇报自己的block信息。 存储多个副本（副本数量可以通过设置参数dfs.replication,默认是3）    副本机制：\n 为了容错，防止单点故障带来的问题，文本的所有block都会有副本。每个block的大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目，副本系统可以在文件创建的时候指定，也可以在之后改变。    一次写入，多次读出\n HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改。 因为此特性，HDFS适合用来做数据分析的底层存储服务，并不适用用于做网盘等应用（修改不方便，延迟大，网络开销大，成本太高）    ","id":38,"section":"posts","summary":"一、Hadoop 2.x的相关组件： HDFS（Hadop分布式文件系统）：负责分布式数据存储 NameNode：存储元信息，包括文件块的分块数量","tags":["大数据"],"title":"Hadoop的设计目标及重要特性","uri":"https://PI-KA-CHU.github.io/2019/03/hadoop%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87%E5%8F%8A%E9%87%8D%E8%A6%81%E7%89%B9%E6%80%A7/","year":"2019"},{"content":"一、搭建步骤 1. 服务器（Linux）环境准备  修改系统文件需要root权限，可通过以下命令获取  su root （回车后输入密码）  配置服务器的主机名称（CentOS 7的坑，需要修改/etc/hostname文件主机名才会修改有效，重启后生效）  vi /etc/hostname node-1（修改的主机名）  配置服务器IP地址和主机名的映射  vi /etc/hosts 192.168.125.129 node-1 192.168.125.130 node-2 192.168.125.131 node-3  配置主服务器（namenode所在服务器）的ssh免密登陆  # 生成ssh免登陆密钥 ssh-keygen -t rsa（输入命令后四个回车） # 将公钥拷贝到免密登陆的目标机器上 ssh-copy-id node-2（node-2为目标服务器） # 配置成功后，可通过以下命令免密码登陆 ssh node-2（目标主机名） # 退出ssh连接的主机 exit  关闭服务器防火墙（CentOS 7）  # 查看防火墙状态 systemctl status firewalld.service # 关闭防火墙 systemctl stop firewalld.service # 禁止防火墙开机自启动 systemctl disable firewalld.service 2. 删除本机jdk并配置sun公司的jdk  将windows下载好的jdk压缩包复制粘贴到linux服务器中（不要使用拖拽，否则解压会出错），然后执行下面命令  # 删除本机的openjdk rpm -qa|grep java rpm -e --nodeps xxxxxx（要删除的包名） # 解压下载的 jdk tar zxvf 压缩包地址 -C 解压地址  配置环境变量 /etc/profile  export JAVA_HOME=/usr/java/jdk1.8.0_191 (等号右边为解压的jdk目录) export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH  使配置的环境变量生效（谨慎设置，否则系统会无法正常启动 source /etc/profile  3. 下载hadoop压缩包并解压   下载地址：https://hadoop.apache.org/releases.html（本人下载的是2.7.7的二进制版本，需要的话可以下载源码在服务器自行编译）\n  解压命令：tar -zxvf hadoop-2.7.7.tar.gz /usr/hadoop/\n  修改解压后的hadoop的etc目录下的相关配置：\n hadoop-env.sh：加入jdk环境变量：  # The java implementation to use. export JAVA_HOME=/usr/java/jdk1.8.0_191  core-size.xml：  \u0026lt;configuration\u0026gt; \u0026lt;!-- 指定Hadoop使用的文件系统schema（URI），HDFS的老大（NameNode）所在的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://node-1:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定hadoop运行时产生的存储目录 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/hadoop/tmp\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;  hdfs-site.xml:  \u0026lt;configuration\u0026gt; \u0026lt;!-- 指定HDFS副本数量 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;2\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定HDFS的秘书节点的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.secondary.http-address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;node-2:50090\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;  mapred-site.xml：  \u0026lt;configuration\u0026gt; \u0026lt;!-- 指定mapreduce运行在yarn上，默认是local模拟 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;  yarn-site.xml：  \u0026lt;configuration\u0026gt; \u0026lt;!-- 指定YARN的老大（ResourceManager）的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.hostname\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;node-1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- NodeManager上运行的附属服务。需要配置成mapreduce_shuffle，才可MapReduce程序默认值 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;  slaves：(默认为localhost)  \u0026lt;!-- 指定从节点所在的主机名，启动DataNode --\u0026gt; node-1 node-2 node-3  /etc/profile：修改系统环境变量(终端按G可以到达文件末尾)  export HADOOP_HOME=/usr/hadoop/hadoop-2.7.7 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin # 修改完后执行命令 `resource /etc/profile`使得环境变量生效   4. 同步到其他服务器  使用远程命令将配置文件发送到各个服务器上：  scp -r /usr/hadoop/hadoop-2.7.7/ root@node-2:/usr/hadoop/(r表示递归传递)   将环境变量文件发送到其他服务器：  scp -r /etc/profile root@node-2:/etc/ 发送完成后执行source /etc/profile    5. 格式化NameNode  首次启动HDFS时，需要对其进行格式化操作 格式化本质是进行文件系统的初始化操作，创建一些自己所需的文件 格式化操作成功后，后续不要再进行格式化，否则可能导致集群启动失败 注意：需要在root权限或者sudo权限下执行格式化命令，否则会格式异常  hdfs namenode -format 6. 启动集群  启动hdfs集群(在hadoop的sbin目录下执行下面命令)  start-dfs.sh  启动yarn集群(在hadoop的sbin目录下执行下面命令)  start-yarn.sh  使用jps命令查看是否启动成功   7. 测试集群是否可用   集群启动成功后可打开下面网址（node-1为主节点所在主机名）：\n 打开hdfs网址（NameNode所在服务器）：http://node-1:50070 打开yarn网址（ResourceManager所在服务器）：http://node-1:8088    初试hdfs：（执行如下命令）\n  # 创建hello文件夹 hdfs dfs -mkdir /hello # 上传文件到hadoop hdfs dfs -put /指定文件或者文件夹 （或者 hadoop fs -put /指定文件或者文件夹 ） # 将文件下载到本地 hdfs dfs -get /hadoop中存储的文件的目录 (或者 hadoop fs -get /hadoop中存储的文件的目录) # 获取hadoop文件目录下的文件 hdfs dfs -ls /指定文件或者文件夹 （或者 hadoop fs -ls /指定文件或者文件夹 ）  初试mapreduce：(测试文件在hadoop安装目录/share/hadoop/mapreduce/下)  # mapreduce执行圆周率运算 hadoop jar hadoop-mapreduce-examples-2.7.7.jar pi 20 50 Hadoop Shell 命令  http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html   二、问题与解决 BUG  配置成功后如果出现ping不通其他服务器的情况，则检查下服务器的网络是否已经连接 在/etc/hosts加入IP地址和主机名的映射的时候，除了加入IP 主机名不要修改其他地方，否则可能出现ping不通其他服务器的情况 环境变量配置错误，把整个三个虚拟机的系统搞炸了！！垃圾BD  解决方法：在登陆界面处control + alt + F2进入终端，以root方式登陆，然后通过命令/usr/bin/vi /etc/profile将错误的环境变量删除，然后reboot重启系统即可恢复（差点绝望） 参考：https://blog.csdn.net/ysy950803/article/details/60777802    ","id":39,"section":"posts","summary":"一、搭建步骤 1. 服务器（Linux）环境准备 修改系统文件需要root权限，可通过以下命令获取 su root （回车后输入密码） 配置服务器的主机名称（Cen","tags":["大数据"],"title":"Hadoop平台的搭建","uri":"https://PI-KA-CHU.github.io/2019/03/hadoop%E5%B9%B3%E5%8F%B0%E7%9A%84%E6%90%AD%E5%BB%BA/","year":"2019"},{"content":"一、集群与分布式的区别  小饭店原来只有一个厨师，切菜洗菜备料炒菜全干。后来客人多了，厨房一个厨师忙不过来，又请了个厨师，两个厨师都能炒一样的菜，这两个厨师的关系是集群。为了让厨师专心炒菜，把菜做到极致，又请了个配菜师负责切菜，备菜，备料，厨师和配菜师的关系是分布式，一个配菜师也忙不过来了，又请了个配菜师，两个配菜师关系是集群\n  https://blog.csdn.net/bluishglc/article/details/5483162   二、redis集群搭建过程（Linux环境）   redis官方文档：（建议跟着文档走） https://redis.io/topics/cluster-tutorial\n  大佬博客： https://www.zybuluo.com/phper/note/195558\n  redis集群搭建前提，修改redis.conf配置文件，找到以下配置信息并做如下图修改：   redis集群至少需要开启六个redis节点，复制六个redis文件夹，在配置文件redis.conf中修改不同的端口号，分别进入src目录，执行命令./redis.server ../redis.conf，带配置文件启动redis。\n    在阿里云服务器（个人使用的服务器）中开启端口号，如6379/6384（6379-6384），开启后还需要开启集群总线端口16379/16384 （16379-16384）,若未开启，在使用公网Ip进行集群搭建的时候会报错。   进入其中一个redis的src文件夹中，执行命令./redis-cli -a password --cluster create xxx.xx.xxx.xx:6379 xxx.xx.xxx.xx:6380 xxx.xx.xxx.xx:6381 xxx.xx.xxx.xx:6382 xxx.xx.xxx.xx:6383 xxx.xx.xxx.xx:6384 --cluster-replicas 1创建集群，出现提示信息，输入yes后没有报异常则创建成功（如果已经创建过集群，需要先将每个redis的src目录下的nodes.conf删除，否则会有异常）。\n  创建成功后，可在redis的src目录下执行命令./redis-cli -a password --cluster fix xxx.xx.xxx.xx:6379进行检查。(下图表示创建成功)   如果是重新创建集群，即使删除nodes.conf文件后可能创建的集群所有的slot没有被覆盖，需要在其中一个redis的src目录下执行命令./redis-cli -a password --cluster fix xxx.xx.xxx.xx:6379进行修复。\n   三、异常与解决 SpringBoot使用redis集群出现的异常   SpringBoot连接redis：\n https://blog.csdn.net/lx1309244704/article/details/80696235 https://www.jianshu.com/p/b75e0d45b5e2    异常1：在SpringBoot进行Redsi集群连接的时候报ERR This instance has cluster support disabled\n  解决：Redis配置文件没有开启集群模式，在redis.conf中找到cluster-enabled yes并将其注释去掉。https://blog.csdn.net/z960339491/article/details/80521851\n  异常2：SpringBoot2.x连接redis集群时出现CLUSTERDOWN The cluster is down或者Node .. is not empty.Either the node already knows other nodes (check with CLUSTER NODES) or contains some key in database 0.，一般出现在创建过集群然后重新创建的时候报的异常\n  解决：需要删除redis的src目录下的nodes.conf文件，然后重新创建集群。\n   Linux服务器搭建redis集群时出现的异常   异常1：使用SpringBoot2.x（lettuce redis客户端）整合redis，报异常：can not connect to 127.0.0.1：6379...，连接不上集群\n  解决：在进行集群创建的时候，使用./redis-cli -a password --cluster create xxx.xx.xxx.xx:6379 xxx.xx.xxx.xx:6380 xxx.xx.xxx.xx:6381 xxx.xx.xxx.xx:6382 xxx.xx.xxx.xx:6383 xxx.xx.xxx.xx:6384 --cluster-replicas 1进行创建的时候，需要用其公网IP进行创建\n  异常2：进行redis集群创建的时候（公网IP创建），报Waiting for the cluster to join ...\n  解决：开放其集群通讯端口，如集群使用的是6379-6384，则还需要再开放16379-16384端口，在阿里云防火墙进行开放，否则会一直无法创建\u0026ndash;https://blog.csdn.net/Truong/article/details/52531103\n  异常3：使用redis-cli --cluster check 127.0.0.1:6379进行集群检查后，检查到[ERR] Not all 16384 slots are covered by nodes.（可能出现在集群有所更换之后）\n  解决：在redis的src目录下运行 ./redis-cli -a password --cluster fix 172.168.63.201:7001\u0026ndash;https://blog.csdn.net/vtopqx/article/details/50235891 （redis5.0后使用redis-cli \u0026ndash;cluster check 127.0.0.1:6379等）\n  异常4：配置从节点redis的redis.conf文件的replicaof指令后创建集群，报replicaof directive not allowed in cluster mode异常\n  解决：如果是创建集群的话，需要注释掉replicaof的配置（本人为replica配置处配置了redis密码，这似乎使得主从复制成功了）\n   参考文献   redis集群相关指令-1： https://www.cnblogs.com/ivictor/p/9768010.html\n  redis集群相关指令-2： http://weizijun.cn/2016/01/08/redis%20cluster%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7redis-trib-rb%E8%AF%A6%E8%A7%A3/\n  redis集群详解： http://russellluo.com/2018/07/redis-replication-demystified.html\n   ","id":40,"section":"posts","summary":"一、集群与分布式的区别 小饭店原来只有一个厨师，切菜洗菜备料炒菜全干。后来客人多了，厨房一个厨师忙不过来，又请了个厨师，两个厨师都能炒一样的菜","tags":["redis","spring-boot"],"title":"redis集群搭建","uri":"https://PI-KA-CHU.github.io/2019/02/redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","year":"2019"},{"content":"前言   replica-master模式就是slave-master模式，查了一下好像是slave-master名称遭到太多人反对，作者被迫进行了修改。\n  配置了主从模式后重启相应配置的redis服务器，从redis会异步复制主redis的数据，开启AOF可以实现数据的持久性（将数据写入磁盘）\n  下面主从模式的配置是建立在已经启动redis-cluster的前提上进行修改配置，集群搭建详情参考#52 。\n  主从、哨兵和集群的关系：\n 主从：数据备份及读写分离 哨兵：高可用，主挂了哨兵可以选主切换 集群：数据 hash 分片， 解决单台机器资源的上限的问题，分散压力。     redis的复制   Redis使用异步复制，异步从站到主站确认处理的数据量。\n  主设备可以有多个从设备。\n  复制在从属端也很大程度上是非阻塞的。当slave正在执行初始同步时，假设您在redis.conf中配置了Redis，它可以使用旧版本的数据集处理查询。\n  复制可用于可伸缩性，以便为只读查询提供多个从站（例如，可以将慢速O（N）操作卸载到从站），或者仅用于提高数据安全性和高可用性。\n  可以使用复制来避免让主服务器将完整数据集写入磁盘的成本：典型的技术是配置主服务器redis.conf以避免持久存储到磁盘，然后连接配置为不时保存的从服务器，或者已启用AOF。但是，必须小心处理此设置，因为重新启动的主服务器将以空数据集开始：如果从服务器尝试与其同步，则从服务器也将被清空。\n   修改replica（从者redis）的redis.conf配置文件（有密码还需要配置replica连接master的密码） # 设置master的IP及端口 replicaof [masterip] [port] # replica需要关闭集群的启动模式，否则会报异常 cluster-enable no  重启replica的redis   进入redis的src目录下，执行./redis-cli -a [password] -h [ip] -p [port] shutdown关闭redis\n  使用./redis-server ../redis.conf命令重启redis服务器\n   参考文献  官方文档： https://redis.io/topics/replication   ","id":41,"section":"posts","summary":"前言 replica-master模式就是slave-master模式，查了一下好像是slave-master名称遭到太多人反对，作者被迫进行","tags":["redis"],"title":"redis的主从模式配置","uri":"https://PI-KA-CHU.github.io/2019/02/redis%E7%9A%84%E4%B8%BB%E4%BB%8E%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/","year":"2019"},{"content":"获取git仓库 git init //初始化本地仓库（会自动创建一个名为 .git 的子目录） git clone [URL] //拉取远程代码到本地 git提交代码的基本操作 git add . // 提交所有修改的文件 git commit -m \u0026#34;提交的版本说明\u0026#34; // 将版本提交到本地仓库 git push // 将代码 常见异常及解决 操作： 使用git pull命令，git pull = git fetch + merge\n异常：\nPull is not possible because you have unmerged files. Please, fix them up in the work tree, and then use \u0026#39;git add/rm \u0026lt;file\u0026gt;\u0026#39; as appropriate to mark resolution, or use \u0026#39;git commit -a\u0026#39;. 解决：\ngit fetch origin // 拿到了远程所有分支的更新 git reset --hard origin/master // 完全舍弃你没有提交的改动 git pull // 拉取远程项目 修改.gitignore文件忽略上传的文件  下面的结果是：忽略除了src、bin文件夹及pom.xml文件的其他所有文件，及除了这些其他文件不会被跟踪上传  /* # 忽略所有文件 !/src # 忽略除了src文件夹的其他文件 !/bin # 忽略除了bin文件夹的其他文件 !/pom.xml #忽略除了pom.xml文件的其他文件 添加目标git项目作为submodule到public目录中 git submodule add https://github.com/PI-KA-CHU/PI-KA-CHU.github.io.git public 问题： \u0026#39;public\u0026#39; already exists in the index 解决： 1. git ls-files --stage public 2. 看到 160000开头的目录 3. git rm --cached public 4. 重新添加 重置本地修改 # 忽略本地修改，使用远方分支 git reset --hard origin/master ","id":42,"section":"posts","summary":"获取git仓库 git init //初始化本地仓库（会自动创建一个名为 .git 的子目录） git clone [URL] //拉取远程代码到本地 git提交代码的基本操作 git add . // 提交所有修改","tags":["开发工具"],"title":"git命令的使用及常见的问题","uri":"https://PI-KA-CHU.github.io/2019/02/git%E5%91%BD%E4%BB%A4%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E5%B8%B8%E8%A7%81%E7%9A%84%E9%97%AE%E9%A2%98/","year":"2019"},{"content":"分布式锁实现流程图  一、@Scheduled的使用及其redis的分布式锁创建   创建新类，使用@Scheduled注解注释需要定时启动的方法\n  单机版的定时任务直接将业务代码写入@Scheduled注解的方法中即可，集群版的定时任务需要加分布式锁，防止同一时间所有集群都启动定时任务，造成资源的浪费。\n  此处使用分布式的双重防死锁，在使用redis实现分布式锁的同时，加入判断防止系统在创建了分布式锁后，因为手动关闭或者意外关闭了服务器，造成没有成功执行释放锁的代码，从而形成死锁。\n  双重防死锁：使用redis实现分布式锁，并且为创建的键赋值为System.currentTimeMillis() + RedisConst.APPLICATION.TASK_LOCK_TIME，即当前系统时间加上有效时间，若分布式锁已经存在，则使用if (time != null \u0026amp;\u0026amp; System.currentTimeMillis() \u0026gt; Long.valueOf(time))判断是否过期，若过期，则可以正常获取到分布式锁并使用。\n  @Component public class ApplicationCloseTask { @Autowired FormExecuteService formExecuteService; private Logger log = LoggerFactory.getLogger(this.getClass()); /** * 定时更新未处理且已经过期的个人申请单，将其设置为过期状态 * 定时任务每七天执行一次 * * @author 曾博佳 * @since 2019-02-23 */ @Scheduled(cron = \u0026#34;0 0 0 */7 * ?\u0026#34;) public void closeApplication(){ log.info(\u0026#34;定时任务正常启动：{}\u0026#34;,new SimpleDateFormat(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;).format(new Date())); //设置分布式锁，若不存在，将其值设置为{当前时间 + 有限时间}，返回true boolean setnxResult = RedisPoolUtil.setnx(RedisConst.APPLICATION.TASK_CLOSE_LOCK, String.valueOf(System.currentTimeMillis() + RedisConst.APPLICATION.TASK_LOCK_TIME)); if(setnxResult){ updateApplicationState(); }else{ //如果分布式锁已过期或者为永久(出现死锁)，则可以获取分布式锁 String time = RedisPoolUtil.get(RedisConst.APPLICATION.TASK_CLOSE_LOCK); if (time != null \u0026amp;\u0026amp; System.currentTimeMillis() \u0026gt; Long.valueOf(time)){ log.info(\u0026#34;获取分布式锁{} 成功，ThreadName：{}\u0026#34;,RedisConst.APPLICATION.TASK_CLOSE_LOCK,Thread.currentThread().getName()); String time2 = RedisPoolUtil.getset(RedisConst.APPLICATION.TASK_CLOSE_LOCK, String.valueOf(System.currentTimeMillis() + RedisConst.APPLICATION.TASK_LOCK_TIME)); // time2为null表示集群应用的锁已经其他进程执行并删除，从正常逻辑上此时可以获取到锁 // 如果time不等于time2并且time2不等于null，说明time2已经被其他进程获取并修改 if (time2 == null || StringUtils.equals(time,time2)){ log.info(\u0026#34;获取分布式锁{} 成功，ThreadName：{}\u0026#34;,RedisConst.APPLICATION.TASK_CLOSE_LOCK,Thread.currentThread().getName()); updateApplicationState(); }else{ log.info(\u0026#34;分布式锁：{} 获取失败\u0026#34;,RedisConst.APPLICATION.TASK_CLOSE_LOCK); } }else{ log.info(\u0026#34;分布式锁：{} 获取失败\u0026#34;,RedisConst.APPLICATION.TASK_CLOSE_LOCK); } } } /** * 执行更新application状态的代码 */ private void updateApplicationState(){ log.info(\u0026#34;分布式锁获取成功，执行关闭订单程序\u0026#34;); //将已经过期的申请更新为过期状态 formExecuteService.updateAllApplicationState(new Date(), NormalConst.APPLICATION.EFFECTIVE_TIME); RedisPoolUtil.del(RedisConst.APPLICATION.TASK_CLOSE_LOCK); log.info(\u0026#34;分布式锁删除成功\u0026#34;); } }  二、SpringBoot Schedule的cron的使用   在线corn生成器：http://www.pppet.net/\n  @Scheduled(cron = \u0026quot;0 0 0 */7 * ?\u0026quot;)表示每七天执行一次，其中的0 0 0 */7 * * ?分别表示秒、分、时、日、月、周、年。\n  常见的corn表达式：\n      0 15 10 * * ? * 每天10点15分触发     0 15 10 * * ? 2017 2017年每天10点15分触发   0 * 14 * * ? 每天下午的 2点到2点59分每分触发   0 0/5 14 * * ? 每天下午的 2点到2点59分(整点开始，每隔5分触发)   0 0/5 14,18 * * ? 每天下午的 2点到2点59分、18点到18点59分(整点开始，每隔5分触发)   0 0-5 14 * * ? 每天下午的 2点到2点05分每分触发   0 15 10 ? * 6L 每月最后一周的星期五的10点15分触发   0 15 10 ? * 6#3 每月的第三周的星期五开始触发      ","id":43,"section":"posts","summary":"分布式锁实现流程图 一、@Scheduled的使用及其redis的分布式锁创建 创建新类，使用@Scheduled注解注释需要定时启动的方法 单机","tags":["","spring-boot","redis"],"title":"SpringBoot定时任务及其redis的分布式锁创建","uri":"https://PI-KA-CHU.github.io/2019/02/springboot%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E5%8F%8Aredis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%88%9B%E5%BB%BA/","year":"2019"},{"content":" REST全称是Representational State Transfer，中文意思是表述性状态转移，指的是一组架构约束条件和原则， 如果一个架构符合REST的约束条件和原则，我们就称它为RESTful架构。\n 特征：  每个URI代表一种资源 客户端和服务器之间，传递这种资源的某种表现层 客户端通过HTTP动词，对服务器端资源进行操作，实现表现层状态转化 URL中通常不出现动词，只有名词 自定义一个占位，可以把摸棱两可的资源请求进行区分,如设计了API为/{keyId}和/{productName}，正常情况下会出现无法识别的异常，可以修改为key/{keyId}和product/{productName}  设计方式  URI的设计技巧  使用_或-来让URI可读性更好，如：如：http://www.oschina.net/news/38119/oschina-translate-reward-plan 使用/来表示资源的层级关系，如：/git/git/commit/e3af72cdafab5993d18fae056f87e1d675913d08 使用?用来过滤资源，如：/pulls?state=closed用来表示git项目中已经关闭的推入请求 ,或;可以用来表示同级资源的关系，如： /blocksha1/sha1.h/compare/e3af72cdafab5993d18fae056f87e1d675913d08;bd63e61bdf38e872d5215c07b264dcc16e4febca    统一资源接口   按照HTTP方法的语义来暴露资源，接口将会拥有安全性和幂等性的特性，例如GET和HEAD请求都是安全的， 无论请求多少次，都不会改变服务器状态。而GET、HEAD、PUT和DELETE请求都是幂等的，无论对资源操作多少次， 结果总是一样的，后面的请求并不会产生比第一次更多的影响。\n  HEAD（获取某个资源的头部信息）\n  GET（获取资源） 安全且幂等 获取表示 变更时获取表示（缓存） 200（OK） - 表示已在响应中发出 204（无内容） - 资源有空表示 301（Moved Permanently） - 资源的URI已被更新 303（See Other） - 其他（如，负载均衡） 304（not modified）- 资源未更改（缓存） 400 （bad request）- 指代坏请求（如，参数错误） 404 （not found）- 资源不存在 406 （not acceptable）- 服务端不支持所需表示 500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务端当前无法处理请求\n  POST（创建资源） 不安全且不幂等 使用服务端管理的（自动产生）的实例号创建资源 创建子资源 部分更新资源 如果没有被修改，则不过更新资源（乐观锁） 200（OK）- 如果现有资源已被更改 201（created）- 如果新资源被创建 202（accepted）- 已接受处理请求但尚未完成（异步处理） 301（Moved Permanently）- 资源的URI被更新 303（See Other）- 其他（如，负载均衡） 400（bad request）- 指代坏请求 404 （not found）- 资源不存在 406 （not acceptable）- 服务端不支持所需表示 409 （conflict）- 通用冲突 412 （Precondition Failed）- 前置条件失败（如执行条件更新时的冲突） 415 （unsupported media type）- 接受到的表示不受支持 500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务当前无法处理请求\n  PUT（更新资源） 不安全但幂等 用客户端管理的实例号创建一个资源 通过替换的方式更新资源 如果未被修改，则更新资源（乐观锁） 200 （OK）- 如果已存在资源被更改 201 （created）- 如果新资源被创建 301（Moved Permanently）- 资源的URI已更改 303 （See Other）- 其他（如，负载均衡） 400 （bad request）- 指代坏请求 404 （not found）- 资源不存在 406 （not acceptable）- 服务端不支持所需表示 409 （conflict）- 通用冲突 412 （Precondition Failed）- 前置条件失败（如执行条件更新时的冲突） 415 （unsupported media type）- 接受到的表示不受支持 500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务当前无法处理请求\n  DELETE（删除资源） 不安全但幂等 200 （OK）- 资源已被删除 301 （Moved Permanently）- 资源的URI已更改 303 （See Other）- 其他，如负载均衡 400 （bad request）- 指代坏请求 404 （not found）- 资源不存在 409 （conflict）- 通用冲突 500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务端当前无法处理请求\n  使用方式   GET：http://www.birjemin.com/api/user # 获取列表\n  POST：http://www.birjemin.com/api/user # 创建用户\n  PUT：http://www.birjemin.com/api/user/{id} # 修改用户信息\n  DELETE：http://www.birjemin.com/api/user/{id} # 删除用户信息\n  过滤信息   用于补充规范一些通用字段\n  ?limit=10：指定返回记录的数量\n  ?offset=10：指定返回记录的开始位置\n  ?page=2\u0026amp;per_page=100：指定第几页，以及每页的记录数\n  ?sortby=name\u0026amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序\n  ?state=close：指定筛选条件\n  接口返回示例  {\u0026quot;code\u0026quot;: 200,\u0026quot;message\u0026quot;: \u0026quot;获取成功\u0026quot;,\u0026quot;succ\u0026quot;: true,\u0026quot;data\u0026quot;: [] }  参考文献  https://baijiahao.baidu.com/s?id=1591221393853082871\u0026amp;wfr=spider\u0026amp;for=pc\u0026amp;isFailFlag=1 http://www.runoob.com/w3cnote/restful-architecture.html  ","id":44,"section":"posts","summary":"REST全称是Representational State Transfer，中文意思是表述性状态转移，指的是一组架构约束条件和原则， 如果一个架构符合R","tags":[""],"title":"RESTful学习","uri":"https://PI-KA-CHU.github.io/2019/02/restful%D1%A7%CF%B0/","year":"2019"},{"content":"1. 创建一个简单类  使用@RestControllerAdvice注册组件，使用@ExceptionHandler捕获异常 该类只捕获Controller层的异常，其他方法体的异常不予捕获 捕获到异常后会返回相应方法体的返回值  /** * 全局异常处理，仅对Controller层的异常有效 * * @author 曾博佳 * @since 2019-02-01 */ @RestControllerAdvice public class GlobalExceptionHandler { private Logger log = LoggerFactory.getLogger(this.getClass()); /** * 统一捕获运行时异常 * * @author 曾博佳 * @since 2019-02-01 * @param e 异常信息 * @return 返回异常错误码及信息 */ @ExceptionHandler(Exception.class) public BaseReturnDto handleError(Exception e){ log.error(\u0026#34;服务器异常\u0026#34;,e); return new BaseReturnDto(ReturnCodeEnum.SERVER_ERROR.getCode(),e.getMessage()); } /** * 统一捕获404未找到异常 * * @author 曾博佳 * @since 2019-02-01 * @param e 异常信息 * @return 返回404码及异常信息 */ @ExceptionHandler(NoHandlerFoundException.class) public BaseReturnDto handler404Error(Exception e){ log.error(\u0026#34;页面未找到\u0026#34;,e); return new BaseReturnDto(ReturnCodeEnum.NOT_FOUND.getCode(),e.getMessage()); } }  2. 全局异常捕获类的注解介绍  @RestControllerAdvice = @ResponseBody + @ControllerAdvice：是SpringBoot整合后的注解 @ResponseBody：将返回结果以JSON格式输出 @ControllerAdvice：是一个组件注解，它允许实现类通过类路径扫描被自动检测到。@ControllerAdvice注解的类可以包含带有@ExceptionHandler、@InitBinder和@ModelAttribute注解的方法，@ExceptionHandler用于异常的捕获，可以指定捕获的异常类型。   ","id":45,"section":"posts","summary":"1. 创建一个简单类 使用@RestControllerAdvice注册组件，使用@ExceptionHandler捕获异常 该类只捕获Contro","tags":["","spring-boot"],"title":"SpringBoot实现全局异常的捕获","uri":"https://PI-KA-CHU.github.io/2019/02/springboot%E5%AE%9E%E7%8E%B0%E5%85%A8%E5%B1%80%E5%BC%82%E5%B8%B8%E7%9A%84%E6%8D%95%E8%8E%B7/","year":"2019"},{"content":" 一、Session的共享问题  Session的共享可以通过redis的操作及spring session进行解决，从而实现单点登陆，redis操作可以直接通过调用template的方法进行数据的存取，但是其对业务具有一定的侵入性，使用spring session对session的存储进行封装，可以通过正常的HttpSession操作进行代码编写，其支持使用Redis、Mongo、JDBC、Hazelcast来存储Session，可以通过指定不同的存储方式进行存储，此处使用的是redis\n  二、Spring-Session实现 Maven依赖 \u0026lt;!-- 引入redis依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--spring session的redis相关依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.session\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-session-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.3.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 配置application.properties spring.session.store-type=redis 在启动类中加入@EnableRedisHttpSession 注解 @EnableRedisHttpSession @SpringBootApplication public class LostBackSysApplication extends SpringBootServletInitializer{ @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) { return application.sources(LostBackSysApplication.class); } public static void main(String[] args) { //\tSystem.setProperty(\u0026#34;spring.devtools.restart.enabled\u0026#34;, \u0026#34;false\u0026#34;); SpringApplication.run(LostBackSysApplication.class, args); } } Spring Session的操作(与HttpSession操作无区别) @RequestMapping(value = \u0026#34;/check\u0026#34;,method=RequestMethod.POST) public BaseReturnDto checkUser(HttpSession session) { //使用Spring Session存储至redis中 session.setAttribute(‘key’,‘value’); }  三、使用过程中遇到的坑  异常：  Error starting ApplicationContext. To display the conditions report re-run your application with \u0026#39;debug\u0026#39; enabled. 2018-05-11 22:27:32.921 ERROR 5600 --- [ main] o.s.boot.SpringApplication : Application run failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name \u0026#39;enableRedisKeyspaceNotificationsInitializer\u0026#39; defined in class path resource [org/springframework/session/data/redis/config/annotation/web/http/RedisHttpSessionConfiguration.class]: Invocation of init method failed; nested exception is java.lang.NoSuchMethodError: org.springframework.data.redis.connection.RedisConnection.getConfig(Ljava/lang/String;)Ljava/util/List; at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1706) ~[spring-beans-5.0.6.RELEASE.jar:5.0.6.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(Abstr ...  解决：  一：spring session的redis依赖版本不对 二：依赖导入只需要springBoot对redis的依赖及spring session对redis的依赖两个即可，不要再导入其他spring session依赖。     参考  https://juejin.im/post/5bdd449b6fb9a04a09557a40 https://www.jianshu.com/p/e4191997da56   ","id":46,"section":"posts","summary":"一、Session的共享问题 Session的共享可以通过redis的操作及spring session进行解决，从而实现单点登陆，redis操","tags":["","spring-boot"],"title":"SpringBoot中使用Spring Session实现单点登陆","uri":"https://PI-KA-CHU.github.io/2019/02/springboot%E4%B8%AD%E4%BD%BF%E7%94%A8spring-session%E5%AE%9E%E7%8E%B0%E5%8D%95%E7%82%B9%E7%99%BB%E9%99%86/","year":"2019"},{"content":"1. 新建各个环境的配置文件夹  2. pom.xml中配置profile \u0026lt;!-- Maven多环境隔离配置 --\u0026gt; \u0026lt;project\u0026gt; \u0026lt;build\u0026gt; \u0026lt;!-- 多环境文件夹配置，根据profile选择对应的文件夹 --\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;directory\u0026gt;src/main/resources\u0026lt;/directory\u0026gt; \u0026lt;!-- 排除指定文件目录下的配置文件 --\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;config_dev/*\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;config_prod/*\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;config_beta/*\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;filtering\u0026gt;true\u0026lt;/filtering\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;!-- 通过profile指定配置的文件夹 --\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;directory\u0026gt;src/main/resources/config_${profile}\u0026lt;/directory\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;!-- 配置不同的环境 --\u0026gt; \u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;dev\u0026lt;/id\u0026gt; \u0026lt;activation\u0026gt; \u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt; \u0026lt;/activation\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;profile\u0026gt;dev\u0026lt;/profile\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;beta\u0026lt;/id\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;profile\u0026gt;beta\u0026lt;/profile\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;prod\u0026lt;/id\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;profile\u0026gt;prod\u0026lt;/profile\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt; \u0026lt;/project\u0026gt;  3. 配置application.properties  SpringBoot会根据选择的profile选择对应的配置文件：application-${profile}.properties  # Multiple environment configuration(beta,prod,dev) spring.profiles.active=@profile@  4. 配置不同环境对应文件夹中的application-${profile}.properties文件  通过springBoot的spring.profiles.include=dataBase可以导入指定的配置文件（多个用逗号隔开），指定的配置文件名需要以application-${profile}.properties进行命名    5. 通过Idea选择profile进而自动配置对应的环境配置  此处分为dev-开发环境，prod-线上环境，beta-测试环境   ","id":47,"section":"posts","summary":"1. 新建各个环境的配置文件夹 2. pom.xml中配置profile \u0026lt;!-- Maven多环境隔离配置 --\u0026gt; \u0026lt;project\u0026gt; \u0026lt;build\u0026gt; \u0026lt;!-- 多环境文件夹配置，根据profile选择对应","tags":["","spring-boot","开发工具"],"title":"Idea中SpringBoot + maven进行环境隔离","uri":"https://PI-KA-CHU.github.io/2019/02/idea%E4%B8%ADspringboot--maven%E8%BF%9B%E8%A1%8C%E7%8E%AF%E5%A2%83%E9%9A%94%E7%A6%BB/","year":"2019"},{"content":"一、实现静态变量的注入   参考：\n SpringBoot的常见注解：https://blog.csdn.net/fxbin123/article/details/80387668 https://blog.csdn.net/RogueFist/article/details/79575665 https://my.oschina.net/u/2617082/blog/1924530    问题：\n最近使用封装的工具类时遇到了无法正常自动注入的问题，在工具类中对静态成员变量使用@Autowired进入自动注入，虽然编译正常，但是在运行的时候会报java.lang.NullPointerException: null异常。\n  原因：\n在Springframework里，我们是不能@Autowired一个静态变量，使之成为一个Spring bean的。因为当类加载器加载静态变量时，Spring上下文尚未加载。所以类加载器不会在bean中正确注入静态类，并且会失败。\n  解决：( @Component用于将类注册到Spring中,注册完记得重新打包一下项目war包)\n  方法一：\n通过@Autowired注解构造函数的方式进行注入：Spring扫描到AutowiredTypeComponent的bean，然后赋给静态变量component。\n@Component public class TestClass { private static AutowiredTypeComponent component; @Autowired public TestClass(AutowiredTypeComponent component) { TestClass.component = component; } // 调用静态组件的方法 public static void testMethod() { component.callTestMethod()； } }   方法二：\n给静态组件加setter方法，并在这个方法上加上@Autowired：Spring能扫描到AutowiredTypeComponent的bean，然后通过setter方法注入\n@Component public class TestClass { private static AutowiredTypeComponent component; @Autowired public void setComponent(AutowiredTypeComponent component){ TestClass.component = component; } // 调用静态组件的方法 public static void testMethod() { component.callTestMethod()； } }      二、过滤器的使用   参考：\n Servlet3.0中@WebFilter的新特性： https://blog.csdn.net/u012334071/article/details/42131943    问题：\n 没有使用web.xml文件，不知如何进行监听器的注册 过滤器对/*进行过滤时登陆会出现死循环过滤的情况    解决：\n 对监听用的类使用@WebFilter注解进行注册 将静态资源及登陆界面和登陆方法进行过滤    代码示例：\n@WebFilter(filterName = \u0026#34;loginFilter\u0026#34;,urlPatterns = {\u0026#34;/*\u0026#34;}) public class LoginFilter implements Filter { private Logger log = LoggerFactory.getLogger(this.getClass()); private static final String COOKIE_NAME = \u0026#34;LB_USERID\u0026#34;; private static final Long COOKIE_TIME = 60 * 30L; @Override public void init(FilterConfig filterConfig) throws ServletException { } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { HttpServletRequest req = (HttpServletRequest)request; HttpServletResponse resp = (HttpServletResponse)response; StringBuffer URL = req.getRequestURL(); log.debug(\u0026#34;Request URL : {}\u0026#34;,URL); //项目的根URL地址 String path = req.getContextPath(); String basePath = request.getScheme() + \u0026#34;://\u0026#34; + request.getServerName() + \u0026#34;:\u0026#34; + request.getServerPort() + path + \u0026#34;/\u0026#34;; //登陆界面及静态资源不进行过滤 if(URL.indexOf(\u0026#34;/index\u0026#34;) \u0026gt; -1 || URL.indexOf(\u0026#34;/static\u0026#34;) \u0026gt; -1 || URL.indexOf(\u0026#34;/login/check\u0026#34;) \u0026gt; -1){ chain.doFilter(request,response); return; } Cookie[] cookies = req.getCookies(); if (cookies != null \u0026amp;\u0026amp; cookies.length != 0){ try { String loginToken = CookieUtil.readLoginToken(req); if(StringUtils.isNotEmpty(loginToken)){ //刷新redis中缓存的有效时间 RedisPoolUtil.expire(loginToken,COOKIE_TIME); //刷新客户端Cookie的有效时间 CookieUtil.refreshLoginToken(req,resp); //递交给下一个过滤器（若没有则结束） chain.doFilter(request,response); return; } } catch (NullPointerException e) { log.error(\u0026#34;redis 中的缓存过期\u0026#34;,e); } } log.info(\u0026#34;用户未登录，跳转至登陆页面\u0026#34;); resp.sendRedirect(basePath + \u0026#34;index\u0026#34;); } @Override public void destroy() { } }   ","id":48,"section":"posts","summary":"一、实现静态变量的注入 参考： SpringBoot的常见注解：https://blog.csdn.net/fxbin123/article/d","tags":["","spring-boot"],"title":"SpringBoot2.0静态变量注入及过滤器的使用","uri":"https://PI-KA-CHU.github.io/2019/01/springboot2.0%E9%9D%99%E6%80%81%E5%8F%98%E9%87%8F%E6%B3%A8%E5%85%A5%E5%8F%8A%E8%BF%87%E6%BB%A4%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8/","year":"2019"},{"content":"一、redis单机版 写在前面  选中代码块，按control + alt + t：快捷tryCatch 选中类名，按control + shift + t：快捷创建Junit Test  redis连接的准备工作  如果是阿里云购买的服务器的话，需要开放redis的启动端口，默认端口为6379 打开redis根目录下的redis.conf文件，将bind 127.0.0.1注释掉，即改为# bind 127.0.0.1，否则只能进行本地连接，从而报异常 找到redis.conf文件中的protected-mode yes，将其改为protected-mode no，关闭保护模式，否则会报连接被终止的异常 重启redis，重启的使用应携带conf文件进行重启，在redis的src文件下执行./redis-server ../redis.conf重启redis，直接重启配置文件并不会生效  redis相关配置  依赖包引入  \u0026lt;!-- 引入redis依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- redis依赖commons-pool 这个依赖一定要添加 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-pool2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  配置springBoot的配置文件（application.properties或application.yml），此处配置的是application.properties  # redis相关配置 # redis数据库索引 spring.redis.database=0 # redis服务器地址 spring.redis.host=120.78.151.65 # redis服务器连接端口 spring.redis.port=6379 # redis服务器连接密码 spring.redis.password= # 连接池最大连接数（负值表示没有限制） spring.redis.lettuce.pool.max-active=8 # 连接池最大阻塞等待时间（负值表示无限制） spring.redis.jedis.pool.max-wait=-1 # 连接池中的最大空闲连接 spring.redis.lettuce.pool.max-idle=8 # 连接池中的最小空闲连接 spring.redis.lettuce.pool.min-idle=0 # 连接超时时间（毫秒） spring.redis.timeout=5000  创建RedisConfig.java文件，配置自定义redisTemplate，解决因默认序列化JdkSerializationRedisSerializer导致的字符显示不正常的问题。  import com.fasterxml.jackson.annotation.JsonAutoDetect; import com.fasterxml.jackson.annotation.PropertyAccessor; import com.fasterxml.jackson.databind.ObjectMapper; import org.springframework.boot.autoconfigure.AutoConfigureAfter; import org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer; import org.springframework.data.redis.serializer.StringRedisSerializer; @Configuration @AutoConfigureAfter(RedisAutoConfiguration.class) public class RedisConfig { /** * 配置自定义redisTemplate * @return */ @Bean public RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate\u0026lt;String, Object\u0026gt; template = new RedisTemplate\u0026lt;\u0026gt;(); template.setConnectionFactory(redisConnectionFactory); //使用Jackson2JsonRedisSerializer来序列化和反序列化redis的value值 Jackson2JsonRedisSerializer serializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper mapper = new ObjectMapper(); mapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); mapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); serializer.setObjectMapper(mapper); template.setValueSerializer(serializer); //使用StringRedisSerializer来序列化和反序列化redis的key值 template.setKeySerializer(new StringRedisSerializer()); template.setHashKeySerializer(new StringRedisSerializer()); template.setHashValueSerializer(serializer); template.afterPropertiesSet(); return template; } }  创建Junit测试实例进行Junit测试,Idea编译器中选中类名，control + shift + t快捷创建  import org.junit.Test; import org.junit.runner.RunWith; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.test.context.junit4.SpringRunner; import java.util.concurrent.TimeUnit; @RunWith(SpringRunner.class) @SpringBootTest public class RedisConfigTest { private Logger log = LoggerFactory.getLogger(this.getClass()); @Autowired private RedisTemplate redisTemplate; @Test public void redisTemplate() { // redis存储的键名 String key = \u0026#34;number\u0026#34;; //如果不存在则设置值，存在的话则不设置 redisTemplate.opsForValue().setIfAbsent(key, 1); //设置哈希值map值 redisTemplate.opsForHash().put(\u0026#34;map\u0026#34;, \u0026#34;place\u0026#34;, \u0026#34;北京\u0026#34;); //获取到哈希值 String place = (String) redisTemplate.opsForHash().get(\u0026#34;map\u0026#34;,\u0026#34;place\u0026#34;); log.info(\u0026#34;[获取到的哈希值] - {}\u0026#34;,place); //进行值的增加和减少（需要为Integer） redisTemplate.opsForValue().increment(\u0026#34;number\u0026#34;,-10); //log中的“{}”为占位符 String value = (String) redisTemplate.opsForValue().get(\u0026#34;school\u0026#34;); log.info(\u0026#34;[redis中 获取到的值] - [{}]\u0026#34;,value); //设置有效时间为10秒的键值 redisTemplate.opsForValue().set(\u0026#34;flag\u0026#34;,\u0026#34;10\u0026#34;,10, TimeUnit.SECONDS); String flag = (String) redisTemplate.opsForValue().get(\u0026#34;flag\u0026#34;); log.info(\u0026#34;[获取到的有效时间为10秒的flag] - [{}]\u0026#34;,flag); } }  Redis整合遇到的坑 问题一：\n SpringBoot整合Redis后连接Redsi出现超时问题  解决：\n 网上的redis的相关配置中都是spring.redis.timeout=0，超时时间不能设置为0，可以设置为5000（根据实际情况设置）。  问题二：\n 修改redis的conf文件后重启redis配置没有生效，一直报连接被主机中的软件中止。  org.springframework.data.redis.RedisSystemException: Redis exception; nested exception is io.lettuce.core.RedisException: java.io.IOException: 你的主机中的软件中止了一个已建立的连接。 ... Caused by: java.io.IOException: 你的主机中的软件中止了一个已建立的连接。 at sun.nio.ch.SocketDispatcher.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288) at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1108) at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:345) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:886) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.lang.Thread.run(Thread.java:748) 解决：\n https://blog.csdn.net/Agly_Clarlie/article/details/52251746 在启动redis的时候，连同配置文件一起启动./redis-server redis.conf    ","id":49,"section":"posts","summary":"一、redis单机版 写在前面 选中代码块，按control + alt + t：快捷tryCatch 选中类名，按control + shift + t：快捷创建Junit","tags":["","spring","redis"],"title":"SpringBoot整合redis","uri":"https://PI-KA-CHU.github.io/2019/01/spring%E6%95%B4%E5%90%88redis/","year":"2019"},{"content":"一、redis的基础命令   info命令：\n # Server：查看系统信息，包括redis的版本信息，使用的端口和process_id和依赖的gcc版本等 # Clients：客户端连接数量等 # Memory：内存的使用情况 # Persistence：持久化备份的参数信息 # State：redis状态信息 # Replication：主从同步的相关数据 # Keyspace：显示已有的空间及空间内的key的数量，在redis客户端通过select index选择相应的空间，类似于选择数据库表，不同的表中存储着不同的数据  在redis.conf文件中可以进行空间数量的设置，默认值为16：即可以使用的空间是0-15     flushdb命令：清除当前keyspace空间的所有键值\n  flushall命令：清除所有keyspace空间的所有键值\n  dbsize命令：当前keyspace的键的数量\n  save命令：人工触发的对redis的持久化\n  quit命令：退出当前redis客户端的连接\n   二、redis的键命令  set key value：设置键值 del key：删除键，成功则返回1，失败则返回0 exists key：判断键是否存在，存在则返回1，不存在则返回0 ttl key：time to leave，返回的是键的过期时间，单位时间是秒，若返回-1，则代表该键不会过期，返回-2表示该键不存在 expire key time：设置键的过期时间，如expire a 10表示设置键a的过期时间为10秒 type key：返回键的类型，如string，hash等 randomkey：返回随机的key（键） rename key1 key2 ：将键名进行重命名，如key a b则将键a的名改为键b，若替换的键名存在，则会将已有的键进行覆盖，即将b的键值覆盖，得到的是key：b，value：a renamenx key1 key2：加了nx结尾的rename命令会进行一定的逻辑判断，该命令会判断该替换的键是否存在，如果存在则返回0，表示替换失败，若不存在则返回1，表示替换成功   三、redis的数据结构 String字符串  setex key time value：创建键的时候为键设置其有效时间，单位时间为秒，psetex设置的时间单位为毫秒 getrange key start end：获取字符串的某一片段（start到end的间字段） getset key value：为key赋上新的值，返回被替换之前的值。 mset key value key value：进行多个键值的设置 mget key1 key2 key3：进行多个键值的获取，获取到键key1,key2,key3的值 strlen key：返回键值的长度 msetnx key value key value：进行批量的键设置，需要设置的键不存在才会赋值成功，该命令具有原子性，及要么全部设置成功返回1，要么都是失败返回0。 incr key：将类型的Integer的键值进行加一操作 incrby key number：为键值一次性加上number的数，如incr 1 100，则表示为键为1的值增加100. decr key：将类型的Integer的键值进行减一操作 decrby key number：为键值一次性减去number的数 append key str：为key的值加上字符串str   Hash哈希  hset key filed value：设置哈希键值 hexists key field：判断键是否存在 hget key field：获取键中filed所对应的值 hgetall key：获取键中所有的field和对应的value hkeys key：获取键中 所有的filed值 hvals key：获取键中所有的value值 hlen key：获取键中filed的数量 hmset key field name field name：批量设置键中的field和value hmget key field field：批量获取键中field的value hdel key field field：删除键中的多个field及其值 hsetnx key field value：若存在这个field，则设置失败返回0，否则返回1   List列表  lpush key value value：将多个value添加到key中，添加的过程是将value从链表头部一个一个插入，及最后一个加入的是在链表的头部 llen key：获取key的数量 lrange key startIndex stopIndex：获取key列表指定下标范围内的元素 lindex index：获取指定下标的元素值 lpop key：移除key列表的首个（头）元素 rpop key：移除key列表的最后（末尾）元素   Set集合  sadd key value value：将多个value加入key集合中 scard key：返回集合中的数量 smembers key：返回集合中的所有成员 sdiff key1 key2：返回key1与key2的差值，即key1减去ke1和key2中共同存在的元素 sinter key1 key2：返回key1与key2的交集 sunion key1 key2：返回key1与key2的并集 srandmember key1 number：返回集合中的随机的number个数 sismember key member：判断member是否为集合key中的元素，是则返回1，不是则返回0 srem key value value：移除集合中的多个元素 spop key：随机移除并返回移除的元素   sortedset有序集合  zadd key score member score member：为有序集合添加多个成员元素，score为member的值，且集合按score的值进行排序 zcard key：返回集合中成员的数量 zscore key member：返回member元素的score zcount key min max：返回score在min到max范围内的member数量 zrank key member：返回member的下标 zrange key min max：返回score在min到max范围内的所有member zrange key min max withscores：返回score在min到max范围内的所有member及其对应的score  ","id":50,"section":"posts","summary":"一、redis的基础命令 info命令： # Server：查看系统信息，包括redis的版本信息，使用的端口和process_id和依赖的gcc","tags":["redis"],"title":"Redis命令操作","uri":"https://PI-KA-CHU.github.io/2019/01/redis%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/","year":"2019"},{"content":"一、redis的安装（Linux环境）  下载需要的redis版本：http://download.redis.io/releases/ ，将redis压缩包解压并进入其根目录tar -zxvf redis-4.0.8.tar.gz -C /home/bigDataPresentationSys/ ，执行make命令进行编译。  如果出现各种坑爹问题，参考这篇：https://blog.csdn.net/jy0902/article/details/19248299 坑爹一：注意允许的用户，我是在root用户下才运行成功的（应该跟权限有关） 坑爹二：尽量用官方稳定版本的，减少入一些奇奇怪怪的坑     执行make test命令检测是否编译成功，在执行命令的时候可能会出现You need tcl 8.5 or newer in order to run the Redis test的错误，通过yum install tcl进行tcl的安装并重新编译即可。   二、redis的基本操作   redis服务的启动及关闭\n 进入redis根目录下的src文件夹，然后运行./redis-server启动redis服务端，键盘按control + C关闭服务，或者输入命令kill -9 PID（PID为启动redis的PID），或者使用客户端的命令——./redis-cli -h 127.0.0.1 shutdown进行关闭      redis的持久化及及基本的数据操作\n 通过redis-cli的shutdown的话redis会进行持久化（存储到磁盘上），即下次启动数据还在，若用control + C关闭redis，即存储的数据会消失。在client执行save命令的话，可以进行人工的持久化操作 redis的src目录下执行./redis-cli启动客户端（redis-server已启动的情况，执行命令ping，若返回pong则连接成功），通过set key value进行数据的设置，通过get key进行相应键值的获取，通过keys *获取所有的key     redis修改启动的端口号\n 通过执行指定执行的端口进行修改：  ./redis-server --port 6380：启动redis服务 ./redis-cli -p 6380：启动redis客户端 ./redis-cli -p 6380 shutdown：关闭redis服务   通过执行指定的配置文件进行启动：  修改根目录下的redis-conf文件，将port（端口）进行修改，执行命令 ./redis-server ../redis-conf，即可根据配置文件进行启动       redis修改启动密码\n 在redis根目录下执行sudo vim redis-conf命令进行配置文件编辑，通过/ + requirepass(搜索的单词)进行搜索，n可以匹配下一个，shift + n可以匹配上一个，找到位置后修改密码，即redis客户端启动的时候需要输入密码才能启动，如./redis-cli -a 1991111     ","id":51,"section":"posts","summary":"一、redis的安装（Linux环境） 下载需要的redis版本：http://download.redis.io/releases/ ，将re","tags":["redis"],"title":"Redis安装及使用","uri":"https://PI-KA-CHU.github.io/2019/01/redis%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8/","year":"2019"},{"content":"一、前言  一直使用Eclipse进行java开发的我最近发现一个界面及其炫酷的编译器Idea，尝试着使用Idea进行开发，记录下出现的一些问题\n 二、Idea导入Eclipse项目（SpringBoot） 1. bug：\n 开始导入的时候是直接在打开项目文件（错误的打开方式），然后一步一步进行配置 ，出现了No mapping found for HTTP request with URI [/] in DispatcherServlet with name 'dispatcherServlet'的问题。  2. 解决：\n  百度谷歌查了各种办法，捣鼓了大半天都没解决，最后把整个项目删除了重新导入，正常了！ 问题出在导入方式有误，导致配置文件没有正常配置。\n  首先在主界面导入项目Import Project，进入后选择 Eclipse（如果是Eclipse项目的话），导入后运行即可正常访问。\n   三、Idea实现热部署（4种）  参考：https://www.cnblogs.com/jcook/p/6910238.html    修改服务器配置，使得IDEA窗口失去焦点时，更新类和资源（只在debug启动模式下有效）\n 点击Idea上方的Run菜单栏，点击Edit Configuration 在Deployment栏目中点击右边的 \u0026lsquo;+\u0026rsquo; 号选择相应的war包 将On Updatae action和On frame deactivation设置为Update classes and resources       使用springloaded jar包\n 下载jar包并导入项目：https://github.com/spring-projects/spring-loaded 启动应用时添加VM启动参数：-javaagent:/home/lkqm/.m2/repository/org/springframework/springloaded/1.2.7.RELEASE/springloaded-1.2.7.RELEASE.jar -noverify      使用spring-boot-devtools提供的开发者工具\n 在springBoot项目中加入如下依赖：  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-devtools\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  如果是Idea编译器，则需要启动Idea的自动编译，否则不会生效：https://blog.csdn.net/u012190514/article/details/79951258      使用Jrebel插件实现热部署(该插件14天免费试用)\n 点击Idea左上角File -\u0026gt; Setting -\u0026gt; Plugin，搜索JReble for Intellij， 选中安装即可。       最后三种方法是基于类加载机制来实现热加载的，因此你修改完成代码后必须重新编译当前代码，才能触发热部署，Eclipse默认就支持了自动编译，而在Intellij IDEA中默认是关闭了自动编译的，可以按照如下2步设置开启：\n control + alt + S打开设置页面，选择Build,Execut, Deployment -\u0026gt; Compiler 勾选中左侧的Build Project automatically control + shift + alt + /打开页面，选择Registry -\u0026gt; 勾选compiler.automake.allow.when.app.running即可     四、Idea常见问题及解决   解决Idea中报没有加载到Bean的错误：\n 报错：Could not autowire. No beans of 'xxxx' type found 解决： https://blog.csdn.net/u012453843/article/details/54906905    Idea进行Debug操作：\n  https://blog.csdn.net/qq_27093465/article/details/64124330\n  设置断点并运行程序   F7为进入详细方法，及如果改行程序是某一个方法，会进入到方法里面\n  F8为直接运行一行代码，即不进入方法中，只运行单行代码\n  F9为跳到下一个断点，若无断点，则结束debug\n    ","id":52,"section":"posts","summary":"一、前言 一直使用Eclipse进行java开发的我最近发现一个界面及其炫酷的编译器Idea，尝试着使用Idea进行开发，记录下出现的一些问题","tags":["","开发工具"],"title":"Idea的使用及常见问题","uri":"https://PI-KA-CHU.github.io/2019/01/idea%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/","year":"2019"},{"content":"一、进程的定义及特征  定义：\n 多道程序设计技术的引入，实现了资源的共享和程序的并发执行。为了描述并发执行的特点，引入了进程的概念。进程是可高并发执行的程序在一个数据集合上运行的过程，具有动态性、并发行、独立性、异步性、和结构特征。进程管理包括进程控制、进程同步、进程通信和进程调度。 同一数据的不同操作之间、不同数据的同一操作之间存在着前趋关系。但不同数据的不同操作之间可考虑并发执行。\n 特征：\n  动态性——是进程最基本的特性。进程有一定的生命期，由创建而产生，由调度而执行，因得不到资源而暂停执行，由撤消而消亡。而程序是一组有序指令的集合，是静态实体。\n  并发性——多个进程在一段时间间隔内同时运行。\n  独立性——进程实体是一个能独立运行的基本单位，也是系统中独立获得资源和独立调度的基本单位。\n  异步性——进程按各自独立的、不可预知的速度向前推进。\n  结构特征——进程实体由程序段、数据段和进程控制块PCB组成。\n   进程控制块： 进程控制块是进程实体的一部分，它记录了操作系统所需的、用于描述进程情况及控制进程运行所需的所有信息。 进程控制块的作用是使一个在多道程序环境下不能独立运行的程序（含数据），成为一个能独立运行的基本单位，一个能和其他进程并发执行的进程。 PCB是进程存在的唯一标志。创建新进程时建立一个PCB，结束进程时回收PCB。 PCB经常被系统访问，应该常驻内存。 PCB的内容： 进程标识符信息——外部标识符、内部标识符（唯一整数）。 处理机状态信息 进程调度信息——进程的状态、优先级等。 进程控制信息——程序和数据地址、同步机制、资源清单等。 PCB的组织方式： 链表方式——相同状态的PCB，链接成一个队列。 索引方式——建立索引表\n  二、进程的控制 1. 操作系统的内核 本质  内核是计算机硬件的第一层扩充软件，由与硬件紧密相关的模块以及运行频率较高的模块组成，常驻内存，以提高OS的运行效率。  支撑功能   中断处理：是内核中最基本的功能，它是整个操作系统赖以活动的基础。内核只对中断进行“有限的处理”，然后转由有关进程继续处理。\n  时钟管理\n  原语操作：原语本身是由若干条指令构成，用于完成一定功能的一个过程。原语是一个不可分割的原子操作。\n  资源管理功能  进程管理、存储器管理和设备管理。   2. 进程的创建、终止、阻塞和唤醒 进程的创建  进程图——是描述进程家族关系的有向树。有向边表示了进程的创建关系，及父子关系（并不能说明前趋关系），子进程可以继承父进程所拥有的资源，父进程撤销时必须同时撤销所有子进程。  引起创建进程的事件 进程的创建——创建原语  申请空白PCB、为进程分配资源、初始化PCB、插入就绪队列  进程的阻塞与唤醒  原因——请求系统服务、启动某种操作、新数据未到达、无新工作 阻塞——当出现上述时间，进程无法继续执行，进程通过阻塞原语block把自己阻塞，是进程自身的一种主动行为。 唤醒——当阻塞事件结束，由发现者进程调用唤醒原语将阻塞进程的唤醒，是一种被动行为。  进程的终止  正常结束、异常结束、外界干预   三、进程间的通信 进程通信   共享存储器系统：\n  基于共享数据结构的通信方式\n  基于共享存储区的通信方式\n    消息传递系统：\n  直接通信方式：进程间直接进行数据交互\n  间接通信方式：进程间通过实体进行数据交互\n    管道通信：\n 共享文件的通信方式    竞争条件  两个或多个进程读写某些共享数据，而最后的结果取决于进程运行的精确时序，称为竞争条件。  三个需要处理的问题 如何避免竞争条件  临界区避免竞争条件  把对共享资源进行访问的程序片段称作临界区域(critical region)或临界区(critical section)\n  实现互斥访问（三种）   睡眠和唤醒实现互斥\n  信号量实现互斥\n  忙等待的互斥：（五种）\n 屏蔽中断（Disabling interrupts）：每个进程在刚刚进入临界区后立即屏蔽所有中断（包括时钟中断），并在离开前再打开所有中断。   锁变量（Lock variables）：违反了条件1   严格轮换法（Strict alternation）：违反了条件3   P解法（Peterson’s solution）：   TSL指令：    忙等待的互斥的缺点：   忙等待，浪费CPU\n  优先级反转问题：H进程优先级高占用着CPU，L进程优先级低且处于临界区中，因获取不到CPU无法从临界区出来。\n   四、生产者与消费者的问题  题目需求：\n 睡眠和唤醒进行互斥的实现：\n 存在严重的竞争条件：\n 信号量的定义：\n 使用信号量进行互斥的实现：（解决丢失的wakeup() ）\n","id":53,"section":"posts","summary":"一、进程的定义及特征 定义： 多道程序设计技术的引入，实现了资源的共享和程序的并发执行。为了描述并发执行的特点，引入了进程的概念。进程是可高并发","tags":["操作系统"],"title":"进程与线程","uri":"https://PI-KA-CHU.github.io/2019/01/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/","year":"2019"},{"content":"一、操作系统的概念  操作系统是一组控制和管理计算机硬件资源和软件资源、合理组织计算机的工作流程，方便用户使用的程序的集合。 操作系统是裸机上的第一层软件，是对硬件功能的首次扩充。   （计算机系统的层次图）  二、操作系统（Operating System，简称OS）的作用   OS作为用户与计算机硬件系统之间的接口 用户在OS的帮助下能够方便、快捷、安全、可靠地操纵计算机硬件和运行自己的程序。\n  OS作为计算机系统资源的管理者 OS的主要功能是针对处理器、存储器、I / O设备以及**信息（数据和程序）**这四类资源进行有效的管理。\n  OS用作扩充机器 没有软件配置的计算机系统称为裸机，OS在裸机上分别覆盖了I / O设备管理软件、文件管理软件等，通常把覆盖了软件的机器成为扩充机器或虚机器\n   三、操作系统的发展历史  人工操作方式→脱机输入输出技术→批处理技术→分时、实时系统→通用操作系统→微机操作系统→网络操作系统→分布式操作系统\n  脱机输入输出技术 为解决人工操作阶段存在的人机矛盾以及CPU与I/O速度不匹配的矛盾，引入脱机输入输出技术。主机CPU只与高速的输入输出设备打交道，从而有效地减少了CPU等待低速设备输入输出的时间   批处理技术 批处理技术是指计算机对一批作业自动进行处理的一种技术。 早期的计算机系统为了充分利用系统资源，通常把一批作业以脱机输入方式输入到磁带上，并在系统中配置监督程序，依次将作业装入内存，控制磁带上的作业自动地、一个接一个地进行处理，这样就形成了早期的单道批处理系统。\n  多道程序设计技术 为进一步改进单道批处理系统中CPU和内存利用率较低的问题，引进多道程序设计技术。多道程序设计技术同时将多个作业放入内存并允许作业交替执行，共享系统中的资源。宏观上并行，微观上串行。 多道程序设计技术能有效提高系统的吞吐量和改善资源利用率，但是为了协调内存中运行的多道程序，应妥善解决处理机分配、内存分配、设备分配、文件安全、作业组织的问题。为解决上述问题而设置的一组软件就形成了操作系统。\n   四、操作系统的分类   单用户操作系统\n  批处理操作系统\n  单道批处理系统：\n 把一批作业以脱机方式输入到磁带上，在系统中配上监督程序，在它的控制下使这批作业能自动地一个接一个地顺序处理。对作业的处理是成批进行的、且内存中始终只保持一道作业。\n  多道批处理系统：\n 引入多道批处理的目的： 提高CPU利用率 提高内存和I / O设备的利用率 增加系统的吞吐量\n  分时操作系统\n  实时操作系统\n  其它操作系统\n   五、操作系统的特征及功能 特征： 并发、共享、虚拟、异步性\n功能：\n 进程控制 进程同步 进程通信 进程调度  ","id":54,"section":"posts","summary":"一、操作系统的概念 操作系统是一组控制和管理计算机硬件资源和软件资源、合理组织计算机的工作流程，方便用户使用的程序的集合。 操作系统是裸机上的第","tags":["操作系统"],"title":"操作系统引论","uri":"https://PI-KA-CHU.github.io/2019/01/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%BC%95%E8%AE%BA/","year":"2019"},{"content":"分支限界法的性质   广度优先或最小耗费（最大效益）优先的方式搜索解空间树\n  先入先出队列式分支限界\n  优先队列式分支限界\n   分支限界法和回溯法的关系   搜索方式\n  解题目标\n   分支限界法例子 0-1背包问题：（队列式分支限界法）\n题目：\n 0-1背包问题定义如下：\nn=3, c=5, w={2,3,2}, v={2,2,3} 通过完成下面的表格，给出用队列式分支限界法解决这个问题时的解题过程 最优值和最优解分别是什么？\n 答案：\n 答题步骤：\n画出解空间树（左1右0） 初始化根节点 按照广度优先的方式入队（只入队没有超背包容量且剩余价值比bestv大的节点） 按照队列先进先出原则 遇到叶子节点则更新bestv及bestx      循环次数 出队结点 入队结点 队列元素 bestv bestx     初始   A A 0 [0,0,0]   1 A B, C B, C       2 B D, E C, D, E       3 C F, G D, E, F, G       4 D I E,F,G,I       5 E J,K F,G,I,J,K       6 F L,M G,I,J,K,L,M       7 G N,O I,J,K,L,M,N,O       8 I   J,K,L,M,N,O 4 [1,1,0]   9 J   K,L,M,N,O 5 [1,0,1]   10 K   L,M,N,O       11 L   M,N,O       12 M   N,O       13 N   O       14 O   空         0-1背包问题：（优先队列式分支限界法）\n问题：\n 0-1背包问题定义如下： n=3, c=5, w={2,3,2}, v={2,2,3}\n通过完成下面的表格，给出用（优化后的）优先队列式分支限界法解决这个问题时的解题过程， 优先级为背包中物品单位重量的价值(假设重量为0时单位重量为的价值为0） 最优值和最优解分别是什么？\n 答案：\n 解题步骤： 基本步骤跟队列式分支限界法一致，在元素入队后，计算入队节点单位价值，并按照节点从到到小出队。 只入队没有超背包容量且剩余价值比bestv大的节点\n    循环次数 出队结点 入队结点 队列元素 bestv bestx     初始   A(0) A 0 [0,0,0]   1 A B(1),C(0) B,C       2 B D(4/5),E(1) C,D,E       3 E J(5/4),K(1) C,D,J,K       4 J   C,D,K 5 [1,0,1]   5 K   C,D       6 D   C       7 C   空        ","id":55,"section":"posts","summary":"分支限界法的性质 广度优先或最小耗费（最大效益）优先的方式搜索解空间树 先入先出队列式分支限界 优先队列式分支限界 分支限界法和回溯法的关系 搜索方式","tags":[],"title":"分支限界法","uri":"https://PI-KA-CHU.github.io/2019/01/%E5%88%86%E6%94%AF%E9%99%90%E7%95%8C%E6%B3%95/","year":"2019"},{"content":"参考  https://blog.csdn.net/zolalad/article/details/11848739  ","id":56,"section":"posts","summary":"参考 https://blog.csdn.net/zolalad/article/details/11848739","tags":[],"title":"时间空间复杂度","uri":"https://PI-KA-CHU.github.io/2019/01/%E6%97%B6%E9%97%B4%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/","year":"2019"},{"content":"一、分治法 分治法思想   将要解决的较大规模的问题不断地分割成更小规模的子问题，直到能够很容易地得到子问题的解。\n  对小规模的问题进行求解\n  将小问题的解合并为一个更大规模的问题的解，由子问题逐步求出原来问题的解。\n  分治法是自顶向下的分解问题\n   https://www.zhihu.com/question/27363814   分治法的使用条件 1. 原问题可分割成k个子问题，1\u0026lt;k\u0026lt;=n\n2. 这些子问题都可解\n3. 可利用这些子问题求出原问题的解\n 分治法的例子 棋盘覆盖问题：\n https://www.jianshu.com/p/97b09ef06735   归并排序：\n #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; void Merge(int sourceArr[],int tempArr[], int startIndex, int midIndex, int endIndex) { int i = startIndex, j=midIndex+1, k = startIndex; while(i!=midIndex+1 \u0026amp;\u0026amp; j!=endIndex+1) { if(sourceArr[i] \u0026gt; sourceArr[j]) tempArr[k++] = sourceArr[j++]; else tempArr[k++] = sourceArr[i++]; } while(i != midIndex+1) tempArr[k++] = sourceArr[i++]; while(j != endIndex+1) tempArr[k++] = sourceArr[j++]; for(i=startIndex; i\u0026lt;=endIndex; i++) sourceArr[i] = tempArr[i]; } //内部使用递归 void MergeSort(int sourceArr[], int tempArr[], int startIndex, int endIndex) { int midIndex; if(startIndex \u0026gt;= endIndex) return; // 解决一个元素的问题 midIndex = (startIndex + endIndex) / 2; //中间点作为分割点 MergeSort(sourceArr, tempArr, startIndex, midIndex); // 分治解决问题1 MergeSort(sourceArr, tempArr, midIndex+1, endIndex); // 分治解决问题2 Merge(sourceArr, tempArr, startIndex, midIndex, endIndex); //合并a[left:mid], a[mid+1, right] } int main(int argc, char * argv[]) { int a[8] = {50, 10, 20, 30, 70, 40, 80, 60}; int i, b[8]; MergeSort(a, b, 0, 7); for(i=0; i\u0026lt;8; i++) printf(\u0026#34;%d \u0026#34;, a[i]); printf(\u0026#34;\\n\u0026#34;); return 0; }  二、递归 递归算法的定义   直接或间接地调用自身的算法称为递归算法\n  用函数自身给出定义的函数称为递归函数\n  使用被定义对象的自身来为其下定义称为递归定义\n  边界条件和递归方法是递归函数的二要素\n   递归举例 Fibonacci数列（斐波那契数）：\n//第n个Fibonacci数可递归地计算如下： int fibonacci(int n) { if (n \u0026lt;= 1) return 1; return fibonacci(n-1)+fibonacci(n-2); }  Ackerman（阿克曼函数）：\n Ackerman函数是一个双递归函数，当一个函数以及它的一个变量是由函数自身定义时，称为双递归函数。   int ackerman(int n,int m){ int F; if(n==1 \u0026amp;\u0026amp; m==0) F = 2; else if(n==0 \u0026amp;\u0026amp; m\u0026gt;=0) F = 1; else if(n\u0026gt;=2 \u0026amp;\u0026amp; m==0) F = n+2; else F = ackerman(ackerman(n-1,m),m-1); return F; }  全排列：\nvoid perm(int m) { if(m==0){ for(int i=0;i\u0026lt;arr.length;i++){ System.out.print(arr[i]+\u0026#34; \u0026#34;); } System.out.println(); return; } else{ for(int i=0;i\u0026lt;arr.length;i++){ if(arr[i]==0){ arr[i] = m; perm(m-1); arr[i] = 0; } } } }  整数划分  找到正整数n的划分个数 将最大加数不大于m的划分个数记作q(n,m)\n  https://www.cnblogs.com/hoodlum1980/archive/2008/10/11/1308493.html   /** 例如正整数6有如下11种不同的划分： 6； 5+1； 4+2，4+1+1； 3+3，3+2+1，3+1+1+1； 2+2+2，2+2+1+1，2+1+1+1+1； 1+1+1+1+1+1。 **/ unsigned long GetPartitionCount(int n, int max) { if (n == 1 || max == 1) return 1; else if (n \u0026lt; max) return GetPartitionCount(n, n); else if (n == max) return 1 + GetPartitionCount(n, max-1); else return GetPartitionCount(n,max-1) + GetPartitionCount(n-max, max); }  Hanoi塔问题  设A,B,C是3个塔座 开始时，在塔座A上有一叠共n个圆盘，这些圆盘自下而上，由大到小地叠在一起 各圆盘从小到大编号为1,2,…,n 现要求将塔座A上的这一叠圆盘移到塔座B上，并仍按同样顺序叠置\n规则1：每次只能移动1个圆盘 规则2：任何时刻都不允许将较大的圆盘压在较小的圆盘之上 规则3：在满足移动规则1和2的前提下，可将圆盘移至A,B,C中任一塔座上\n void hanoi(int n, int a, int b, int c) { if (n \u0026gt; 0) { hanoi(n-1, a, c, b); move(a,b); hanoi(n-1, c, b, a); } } ","id":57,"section":"posts","summary":"一、分治法 分治法思想 将要解决的较大规模的问题不断地分割成更小规模的子问题，直到能够很容易地得到子问题的解。 对小规模的问题进行求解 将小问题的解","tags":[],"title":"递归与分治","uri":"https://PI-KA-CHU.github.io/2019/01/%E9%80%92%E5%BD%92%E4%B8%8E%E5%88%86%E6%B2%BB/","year":"2019"},{"content":"动态规划的基本步骤 1. 分析最优子结构\n 找出最优解的性质，并刻画其结构特征  2. 建立递归关系\n 递归地定义最优值  3. 计算最优值\n 以自底向上的方式计算出最优值（分治相反）  4. 构造最优解\n 根据计算最优值时得到的信息  动态规划：\n 把小问题的解存储起来，大问题的最优解一定是小问题的最优解得出，而分治法则不同。   动态规划的基本要素   最优子结构\n  重叠子问题（子问题独立则用分治法）\n   动态规划例子 矩阵连乘  矩阵的维数分别为： A1: 5☓2，A2: 2☓4，A3: 4☓3，A4: 3☓5 给出用动态规划算法解决该问题时矩阵m(n行，n列)和s(n行，n列)的值 给出矩阵A1A2A3A4连乘的最小数乘次数以及计算顺序（用加括号的方式表示）\n  m[1][4]=104：表示A1A2A3A4连乘的最小值为104 s[1][4]=1：表示A1A2A3A4连乘的最小值的划分为A1(A2A3A4) A1(A2A3)= m[1][3] ：m[1][1] + m[2][3] + A1乘（A2A3连乘后的行列）的值   0-1背包问题  问题：假设背包容量为5。共有3种物品，其重量w={1, 2, 3}，价值v={6, 10, 2}，选择哪几个物品装入背包中，使得背包中物品的价值最大？ 要求： 给出用动态规划法解决上面背包问题时的矩阵m(n+1行，C+1列)及向量x(n行，1列) 给出最优值（背包中物品的最大价值）及最优解（放入背包中的物品）\n  最长公共子序列  求序列X={1, 2, 4, 6}和Y={2, 6, 4, 8}的最长公共子序列长度以及最长公共子序列 并给出用动态规划解决问题时矩阵c(m+1行，n+1列)和b(m行，n列)\n  解题步骤： ps：某元素的对角线位为”1“号位，上面为”2“号位，左边为”3“号位\n  比较某位置的行和列元素，若相等则取1号位置（对角线）的值 + 1，若不相等，则取2号位（上面）和3号位（左边）的最大值（2号和3号位的相等则默认取2号的）。\n  第二张表则填取的是几号位的，最后从表的右下角开始，根据是从几号位的获取的值逆着走，如2则往上走，3则往左走，最后得到的路径中是1的所对应的值就是最长公共子序列。\n    最大字段和  求序列{5, 2, -4, -8, 3, 4, 1}的最大字段和及相应的最大字段 给出用动态规划法求最大字段和的向量b(1行，n列)\n  答题步骤：\n b的值为该位置前面的数字的总和，若为负数，则填0 找到最大的值，则取其位置到前面b的值为0的位置的所有对应的数字      序列 5 2 -4 -8 3 4 1     b 5 7 3 0 3 7 8     最大子段和：8 最大子段{3, 4, 1}\n  ","id":58,"section":"posts","summary":"动态规划的基本步骤 1. 分析最优子结构 找出最优解的性质，并刻画其结构特征 2. 建立递归关系 递归地定义最优值 3. 计算最优值 以自底向上的方式计算出最优值（","tags":[],"title":"动态规划","uri":"https://PI-KA-CHU.github.io/2019/01/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","year":"2019"},{"content":"回溯法的本质   具有限界函数的深度优先搜索法\n  扩展结点：一个正在产生儿子的结点称为扩展结点\n  活结点：一个自身已生成但其儿子还没有全部生成的节点称做活结点\n  死结点：一个所有儿子已经产生的结点称做死结点\n   回溯法的例子 0-1背包问题：\n 一个0-1背包问题定义如下： n=3, c=7, w={4,3,2}, v={3,4,3} （这个问题可以用回溯法解决，可以用约束函数和限界函数剪枝以提高搜索效率。）\n给出： 搜索该解空间树时，扩展结点访问过程 最优值和最优解\n剪枝的条件：\n 目前已有的重量加下个节点的重量超过背包容量，则剪去 剩余的物品的价值补比bestv大，则剪去  更新bestv的条件：\n 当前拓展节点为叶子节点，则更新bestv和bestx    答案：\n扩展结点访问过程如下：初始bestv=0，下表中的 r 表示剩余的物品的价值，若剩余的价值不比bestv大，则剪去改节点。\n    t 扩展结点 sw sv 剪枝函数     1 A 0 0 sw+w[t]=0+4\u0026lt;=C   2 B 4 3 sw+w[t]=4+3\u0026lt;=C   3 D 7 7 sw+w[t]=7+2\u0026gt;C，剪去H,r=0,r+sv=0+7\u0026gt;bestv   4 I 7 7 叶子结点，sv\u0026gt;bestv，更新bestv,bestv=sv=7, bestx=[1,1,0]   2 B 4 3 r=3,r+sv=3+4\u0026lt;=bestv，剪去E   1 A 0 0 r=7,r+sv=7\u0026lt;=bestv，剪去C     最优值为7，即背包中物品最大价值为7 最优解为[1,1,0]，即物品1，物品2放入背包中\n  5皇后问题：\n 使用回溯法解决5皇后问题时，x=[5, 3, 1, 4, 2]是该问题的一个解\n问题： 假设搜索从t=1, x[t]=5开始，直到找到x =[5, 3, 1, 4, 2]时结束 按照算法的搜索顺序，给出满足约束条件时，搜索深度t和x[t]的所有值\n  答案： 简写成(t, x[t])的形式： (1,5), (2,1), (3,4), (2,2), (3,4), (4,1), (5,3), (2,3), (3,1), (4,4), (5,2)\n ","id":59,"section":"posts","summary":"回溯法的本质 具有限界函数的深度优先搜索法 扩展结点：一个正在产生儿子的结点称为扩展结点 活结点：一个自身已生成但其儿子还没有全部生成的节点称做活","tags":[],"title":"回溯法","uri":"https://PI-KA-CHU.github.io/2019/01/%E5%9B%9E%E6%BA%AF%E6%B3%95/","year":"2019"},{"content":"贪心算法的本质  做出在当前看来最好的选择 自顶向下地解决问题   贪心算法基本要素  最优子结构 贪心选择   贪心算法的例子 活动安排问题：\n 利用贪心算法，解决下列活动安排问题\n 给出代表所选择活动的集合A的值（布尔值） 说明使用该算法最多能安排几个活动 活动的序号分别是什么      活动序号 1 2 3 4 5 6 7 8 9     开始时间 0 1 5 6 7 8 10 14 15   结束时间 5 3 12 8 13 15 14 20 18     解题思路：\n 按照活动结束的时间进行非减排序 根据所排顺序及活动时间是否冲突创建布尔（boolean）表并填入boolean值 根据布尔表得出安排的活动    普通背包问题：\n 利用贪心算法，解决下列背包问题：\n背包容量为5，共有4个物品，其重量w={2, 4, 6, 1}，价值v={5, 6, 3, 3} 。 给出解决问题的步骤 给出最优值（背包中物品的总价值）和最优解（放入背包中每个物品的比例）\n  1. 解题步骤：\n  按v/w={2.5, 1.5, 0.5, 3}对物品进行排序结果为(编号): {4, 1, 2, 3}\n  依次将物品放入背包中 x[4]=1, 剩余容量c=5-1=4 x[1]=1, 剩余容量c=4-2=2 x[2]=0.5, 剩余容量c=2-4*0.5=0 X[3]=0, c =0 （这一步可省略）\n  2. 放入背包中的物品总价值为：5+0.5*6+0+3=11 3. 放入背包中的物品为 x = [1, 0.5, 0, 1]，即编号为1和4的物品全部放入背包中，编号为2的物品的1/2放入背包中，编号为3的物品不放入背包中\n  最优装载问题：\n 利用贪心算法，解决下列最优装载问题\n共有3个集装箱，其重量为w={2, 4, 1}，集装箱的载重量为5 给出解决问题的步骤以及结果（轮船上最大集装箱数量以及装入轮船上集装箱的编号）\n  1. 解题步骤：\n  按重量对集装箱从小到大进行排序结果为(编号): {3, 1, 2}\n  依次将集装箱装上轮船 x[3]=1, 剩余容量c=5-1=4 x[1]=1, 剩余容量c=4-2=2 w[2]\u0026gt;c, 循环结束\n  2. 装上轮船集装箱数量为：2 3. 装上轮船的集装箱为 x = [1,0,1]，即编号为1和3的集装箱装上轮船\n 构造哈夫曼树：\n 假设某文件共含有6个字符，其出现的频率如下：\n    字符 A B C D E F     频率 8 5 6 12 3 1     根据该频率构造哈夫曼树时，优先队列的出队顺序是什么？ 给出最终构造出的哈夫曼树\n 采用贪心算法构造哈夫曼树时，可采用优先队列实现最小频度子树的选择 规定构造哈夫曼树时，左子树的频度\u0026lt;右子树的频度     Dijkstra算法（迪杰斯特拉算法）：\n  题目：用Dijkstra算法计算下图从源顶点1到其它顶点间最短路径，并给出中间过程（ u，S及dist）\n   答案：\n    迭代 u S dist[2] dist[3] dist[4] dist[5]     初始 - {1} 5 inf 30 inf   1 2 {1,2} - 10 30 inf   2 3 {1,2,3} - - 30 20   3 5 {1,2,3,5} - - 28 -   4 4 {1,2,3,5,4}            ","id":60,"section":"posts","summary":"贪心算法的本质 做出在当前看来最好的选择 自顶向下地解决问题 贪心算法基本要素 最优子结构 贪心选择 贪心算法的例子 活动安排问题： 利用贪心算法，解决下列","tags":[],"title":"贪心算法","uri":"https://PI-KA-CHU.github.io/2019/01/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/","year":"2019"},{"content":"一、依赖环境安装 操作系统： CentOS 7.3 安装版本： nginx-1.14.2\n安装gcc：\nyum install gcc-c++\n安装其他依赖：\nyum install -y pcre pcre-devel yum install -y zlib zlib-devel yum install -y openssl openssl-devel  二、Nginx安装 官网下载地址  http://nginx.org/en/download.html   nginx下载包的解压  Windows下载完后通过MobaXtem终端模拟器将安装包上传到服务器（CentOS），通过以下命令进行解压：  tar -xzvf 压缩包文件地址 -C 解压缩到的文件夹\n nginx的编译安装  进入nginx目录，执行如下命令：  # --prefix可以指定安装目录 ./configure --prefix=/usr/java/nginx/nginx-1.14.2/ make \u0026amp; make install  nginx的启动   编译成功后，nginx根目录下会出现sbin文件夹，进入sbin文件夹并运行nginx：./nginx\n  查看启动是否成功\n  # 启动完毕查看是否启动成功 ps -ef | grep nginx  三、Nginx负载均衡的配置 1. 创建vhost文件夹，用于添加负载均衡的相关配置：\n2. 进入vhost文件夹，创建*.conf文件 （如myconf.conf），加入如下配置：\n#将到120.78.151.65的请求负载到不同端口的tomcat上，weight为其权重 upstream 120.78.151.65{ server 120.78.151.65:18080 weight=10; server 120.78.151.65:9080 weight=1; } server{ listen 80; #监听的端口号 autoindex on; server_name 120.78.151.65 #监听的服务器 access_log /usr/java/nginx/nginx-1.14.2/logs/access.log combined; index index.jsp index.html index.htm index.php; #将端口监听到的请求转发到如下服务器，通过上面进行负载均衡 location / { proxy_pass http://120.78.151.65; } } 3. 在nginx的conf文件夹下的nginx.conf中加入如下代码include vhost/*.conf，将vhost的配置文件导入：\n4. 进入sbin文件夹\n  关闭nginx： ./nginx -s stop\n  重启nginx： ./nginx -s reload\n  查看启动是否成功： ./nginx -t\n   四、Nginx安装的异常及解决： 异常 nginx: [alert] could not open error log file: open() \u0026#34;/usr/local/nginx/logs/error.log\u0026#34; failed (2: No such file or directory) 2019/01/10 19:08:56 [emerg] 6996#0: open() \u0026#34;/usr/local/nginx/logs/access.log\u0026#34; failed (2: No such file or directory) 原因  nginx/目录下没有logs文件夹  解决  在nginx根目录下运行如下命令：  mkdir logs chmod 700 logs 创建完logs文件夹后重新启动nginx   五、Nginx安装及负载均衡配置（Windows环境下） 安装与配置  基本步骤与Linux环境相同，如果需要修改本地域名重定向用于测试负载均衡，打开C:\\Windows\\System32\\drivers\\etc目录下的hosts文件，加入127.0.0.1 www.test.com即可将www.test.com重定向到本地  遇到的坑  安装nginx后需要重启电脑后配置的文件才能生效，否则nginx不会进行跳转   参考文献   https://www.cnblogs.com/xxoome/p/5866475.html\n  https://www.jianshu.com/p/deeb4bb5cedb\n  https://www.jianshu.com/p/320a48fcef57\n  ","id":61,"section":"posts","summary":"一、依赖环境安装 操作系统： CentOS 7.3 安装版本： nginx-1.14.2 安装gcc： yum install gcc-c++ 安装其他依赖： yum install -y pcre pcre-devel yum install -y zlib zlib-devel yum install -y openssl openssl-devel 二、Nginx安装 官网下载地址 http://nginx.org/en/download.html ng","tags":[""],"title":"Nginx的安装及负载均衡（Linux环境）","uri":"https://PI-KA-CHU.github.io/2019/01/nginx%E7%9A%84%E5%AE%89%E8%A3%85%E5%8F%8A%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1linux%E7%8E%AF%E5%A2%83/","year":"2019"},{"content":"UML的体系结构 UML建模规则  名字：任何一个UML成员都必须包含一个名字。 作用域：UML成员所定义的内容起作用的上下文环境。某个成员在每个实例中代表一个值，还是代表这个类元的所有实例的一个共享值，由上下文决定。 可见性：UML成员能被其他成员引用的方式。 完整性：UML成员间互相连接的合法性和一致性。 运行属性：UML成员在运行时的特性。  UML建模的原则  被省略：即模型本身是完备的，但在图上某些属性被隐藏起来，以简化表达； 不完全：即在设计过程中某些元素可以暂时不存在； 不一致：即在设计过程中暂时不保证设计的完整性。   UML常用的9种模型图  按照视图进行划分：  （4 + 1 视图）  按照静态图及动态图进行划分：  1. 用例图（UseCase Diagrams）  用于描述一组用例、参与者及它们之间的连接关系，即什么角色做了什么事。 从角色使用系统的角度描述系统中的信息，查看系统的功能，而不描述该功能的内部实现。 用例图包括系统用例图及业务用例图，系统用例图是通过系统的使用用例，业务用例图是没有系统参与时的使用用例。  （系统用例图）\n 椭圆：用例，是用户与计算机之间的一次典型交互作用。 人形：参与者（外部执行者）是指用户在系统中扮演的角色。  (旅游系统) (ATM系统)  2. 类图（Class Diagrams）  用于描述一组类，接口，协作及它们间的静态关系（即系统的对象结构，显示构成系统的对象类，以及那些对象类间的关系），阐明了系统的静态结构。 类是一组具有相同属性，操作，关系和语义的对象的描述，其中对类和属性进行描述时的一个重要细节是它的可见性。  （类图）\n 类图用矩形框表示，它的属性和方法操作分别列在分格中。 类之间可以多种方式链接，关联，泛化，依赖，实现，聚合，组合，各种关系的强弱顺序： 泛化 = 实现 \u0026gt; 组合 \u0026gt; 聚合 \u0026gt; 关联 \u0026gt; 依赖 类框之间的关系通过连线来表示，不同关系用连线上和连线端头的修饰符来区别。 方法修饰符 “-” 表示私有属性private，内部可见，类内部操作才可存取， “+” 表示公有属性public ，可以被系统中其他操作查看和使用。“#”表示 protected  ，类中操作存取，也可被子类使用。  （大学类图）  3. 对象图（Object DIagrams）  对象图描述的是参与交互的各个对象在交互过程中某一时刻的状态，其中所使用的符号与类图中的符号几乎完全相同，区别仅在于对象图的对象名带有下划线，而且类与类之间关系的所有实例都要画出来。  （汽车类图） （汽车对象图）  4. 时序图（Sequence Diagrams）（顺序图，序列图）  顺序图表示对象间传送消息的时间顺序，阐明对象间交互过程以及在系统执行过程中的某一具体时刻将会发送什么事件。 是一种强调时间顺序的交互图，将用例表达的需求，转化为进一步，更加正式层次的精细表达。用例常常被细化为一个或者多个序列图。  （时序图）\n  对象： 传送事件的对象，如下面餐厅点餐时序图中的人形及方框内的对象。\n  对象生命线： 对象下的一条虚线，表示一个对象在一段时间内存在。\n  控制焦点： 每个对象下面有一个小矩形条，它与对象的生命线相重叠，事务的请求传递和响应返回在矩形条中进行。\n  同步消息： 需要等待上一步执行完，才可进行下一步操作   异步消息： 异步发送消息，不需等待   注释： 对某一操作过程进行注释   约束： 对传输的数据进行约束   （餐厅点餐时序图）  5. 协作图（Collaboration Diagrams）   协作图是一种交互图，它强调收发信息的对象的组织结构 ，描述的对象间的协作关系（与写作图相似），显示对象间的动态合作关系 。\n  协作图是表示一个类操作的实现，可以说明类操作中用到的参数和局部变量及操作中的永久链，当实现一个行为时，消息编号对应了程序中嵌套调用结构和信息传递过程。\n   （张三取款协作图） （就餐者，服务生和厨师间关系的协作图）   协作图与顺序图的区别：\n   两者同构，可以相互转换，顺序图强调时间顺序，协作图强调组织结构及对象间的协作。\n  在多数情况下，协作图主要用来对单调的，顺序的控制流建模，但它也可以用来对包括迭代和分支的控制流程进行建模。\n  如果强调时间和顺序，则使用顺序图；如果强调上下级关系（对象间的协作），则选择协作图。\n   6. 状态图（Statechart Diagrams）   状态视图是一个类对象所经历的所有历程的模型图，状态由对象的各个状态和连接这些状态的变迁（事件和条件）组成。\n  每个状态对一个对象在其生命周期中满足某种条件的一个时间段建模，当一个事件发生时，它会触发状态间的变迁，导致对象从一种状态转化到另一种新的状态，与变迁相关的活动执行时，变迁也同时发生。\n  在UML中,状态图可用来对一个对象按事件排序的行为建模（建模特定对象的动态行为,说明对象的生命周期）\n  （张三取款状态图） （电梯系统运行的状态图）  7. 活动图（Activity Diagrams）   活动图是状态图的一种特殊情况，其中几乎所有或大多数状态都处于活动状态，而且几乎所有或大多数变迁都是由源状态中活动的完成而触发的。\n  活动图本质上是一种流程图，它描述从活动到活动的控制流，而交互图强调的是对象到对象的控制流。活动图显示了系统的流程，可以是工作流，也可以是事件流。\n  活动图是一种表述过程基理、业务过程以及工作流的技术。它可以用来对业务过程、工作流建模，也可以对用例实现甚至是程序实现来建模。\n  （带泳道的开户的活动图） （喝饮料活动图，同一个地方需要多个活动时，加实体长方条）  8. 构件图（Component Diagrams）（组件图）   用于描述一组(构)件之间的组织和依赖关系，用于建模系统的静态实现视图(可以显示程序代码如何分解成模块，相当于哪些程序代码负责哪个功能模块)。组(构)件用虚线连接,表示组(构)件间的相关性。\n  组(构)件可以是可执行程序、库、表、文件和文档等，它包含了逻辑类或者逻辑类的实现信息，因此逻辑视图和实现视图之间存在映射关系。\n  组(构)件间也存在依赖关系，利用它可方便地分析一个组(构)件的变化会给其他组(构)件带来怎样的影响。\n  （ATM系统构件图） （ATM客户机的C++组建图） ATM客户机的C++组建图解析：\n  在C++组件图中，每个类有自己的体文件和头文件，框图中每个类映射自己的组件。如，显示类映射ATM显示组件，阴影组件称为包体，表示C++中显示类的体文件(.cpp)。\n  无阴影组件称为包规范，表示C++类的头文件(.H)。组件ATM.exe是**任务规范,**表示处理线程(可执行程序)。\n  组(构)件间的相关性：如，读卡机类与显示类相关，即必须有显示类才能编译读卡机类。编译所有类后，即可创建可执行文件ATMClient.exe。\n   8. 部署图（Deployment Diagrams） 部署(配置)图用于定义系统中软硬件的物理体系结构。可显示实际的计算机和设备(节点，立方体图形)以及它们间的连接关系，也可显示连接的类型及组(构)件间的依赖性。\n在节点内部，放置可执行组(构)件和对象以显示节点与可执行软件单元的对应关系。\n（ATM系统部署图）   参考文献  https://zhuanlan.zhihu.com/p/44518805 http://wenku.uml.com.cn/document.asp?fileid=16458\u0026amp;partname=UML  ","id":62,"section":"posts","summary":"UML的体系结构 UML建模规则 名字：任何一个UML成员都必须包含一个名字。 作用域：UML成员所定义的内容起作用的上下文环境。某个成员在每个实","tags":[" "],"title":"UML期末复习","uri":"https://PI-KA-CHU.github.io/2019/01/uml/","year":"2019"},{"content":"软件测试的知识点   思维导图（部分）：\n   软件测试的定义：\n   软件是一个集合，包括三部分：文档，程序和数据\n  软件测试就是为了发现错误而审查软件文档，检查软件数据和执行程序代码的过程，其目的在于在软件交付前分发现缺陷并协助相关部门定位，解决缺陷，最后交付一个高质量软件给客户。\n  从广义上讲，软件测试是指软件产品生存周期内的所有检查、评审和确认活动。如设计评审、文档审查、单元测试、集成测试、系统测试、验收测试等。\n  软件测试中称找缺陷的过程为找Bug。Bug表示电脑系统或程序中隐藏的错误、缺陷和问题。\n    软件测试的分类：\n 1. 黑盒测试：\n 黑盒测试又叫做功能测试、数据驱动测试或基于需求规格说明书的功能测试。该测试类型注重于测试软件的功能性需求。测试工程师无需了解程序代码内部结构，完全模拟软件产品的最终用户使用该软件，检查软件产品是否达到了用户的需求。  2. 白盒测试：\n 白盒测试又称为结构测试、逻辑驱动测试或基于程序代码内部构成的测试。  3. 灰盒测试：\n 灰盒测试是前两种测试的集合，一方面考虑程序代码的功能性表现，另一方面又要考虑程序代码内部结构。像我们的功能测试，自动化功能测试就采用了灰盒测试的方法。  4. 静态测试：\n 静态测试，顾名思义，就是静态的、不执行被测对象程序代码而寻找缺陷的过程。通俗的讲，静态测试就是用眼睛看，阅读程序代码、文档资料等，与需求规格说明书中的客户需求进行比较，找出程序代码中设计不合理以及文档资料有错误的地方。 一般在企业、公司里召开正规的评审会，通过评审的方式，找出文档资料、程序代码中存在缺陷的地方，并加以修改。  5. 动态测试：\n  动态测试即为实际的执行被测试对象代码，输入事先设计好的测试用例，检查程序代码运行的结果与测试用例中设计的预期结果之间是否有差异，判定实际结果与预期结果是否一致，从而检查程序的正确性、可靠性和有效性，并分析系统运行效率和健壮性等性能状况。\n  动态测试由四部分组成：设计测试用例、执行测试用例、分析比较输出结果、输出测试报告。\n  6. 手动测试：\n 手动测试是最传统的测试方法。它是测试人员设计测试用例并且执行测试用例，然后根据实际的结果去和预期的结果相比较并记录测试结果，最终输出测试报告的测试活动。  7. 自动化测试：\n  自动化测试就是利用一些测试工具，模拟用户的业务使用流程，让它们自动运行来查找缺陷。也可以编写一些代码，设定特定的测试场景，来自动寻找缺陷。\n  自动化测试的优点是能够很快、很广泛的查找缺陷，同时可以做很多重复性的工作，在回归测试阶段，可以利用QuickTest\n  自动化测试工具：HP的QuickTest Professional、Jmeter，LoadRunner（可用于压力测试）,微软的WAS，IBM的Rational等。\n    按结构与内部实现：\n  黑盒测试 白盒测试   按是否执行程序分类：\n  静态测试 动态测试   按执行过程分类：\n  单元测试 集成测试 确认测试 系统测试 验收测试   软件评审  代码评审  参考文献：  https://wenku.baidu.com/view/1353eb5e284ac850ac024291.html?re=view  ","id":63,"section":"posts","summary":"软件测试的知识点 思维导图（部分）： 软件测试的定义： 软件是一个集合，包括三部分：文档，程序和数据 软件测试就是为了发现错误而审查软件文档，检查软","tags":[" "],"title":"软件测试复习","uri":"https://PI-KA-CHU.github.io/2019/01/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%A4%8D%E4%B9%A0/","year":"2019"},{"content":"一、Tomcat的安装  单个Tomcat的安装（包括基本JDK，Mysql环境搭建）具体可参考这边笔记#20 ，现在需要将多个Tomcat安装到服务器上（下面安装的是两个），按照单个安装那样，通过Linux命令将Tomcat安装到不同目录下：   二、Tomcat的端口的修改  修改不同Tomcat的端口以避免同时启动时的冲突，第一个Tomcat可以不做修改，保持默认端口即可，多个Tomcat的端口可以按照+1000的形式进行添加，比如默认端口为8080，则第二个Tomcat的端口可设置为9080，方便记忆。\n 1. 命令行打开的server.xml文件：（第二个Tomcat）\nsudo vim /usr/java/tomcat2/apache-tomcat-8.5.35/conf/server.xml\n2. 修改三个位置的端口号：（只修改port属性的端口即可）\n 默认为8005，用于监听Tomcat的关闭   默认为8080，用于监听客户端请求，如果发送的是https 请求. 就将请求转发到8443 端口.，加入URIEncoding=“UTF-8”可以防止Tomcat乱码   默认为8009，用于接收其他服务器转发过来的请求   三、系统环境变量的修改   为了防止多个Tomcat冲突问题，将安装的Tomcat路径都加入系统环境变量中，并通过修改Tomcat的server.xml配置其启动路径。\n  CentOS目录下/etc/profile文件，在其最底部加入下面代码，下面代码的等号右边都是自己安装的Tomcat的路径：\n  （CATALINA_BASE，CATALINA_HOME，TOMCAT_HOME是Tomcat的默认启动路径，如果想要修改其启动路径，需要在Tomcat的server.xml文件中进行配置）\n#set tomcat environment export CATALINA_BASE=/usr/java/tomcat1/apache-tomcat-8.5.35 export CATALINA_HOME=/usr/java/tomcat1/apache-tomcat-8.5.35 export TOMCAT_HOME=/usr/java/tomcat1/apache-tomcat-8.5.35 export CATALINA_2_BASE=/usr/java/tomcat2/apache-tomcat-8.5.35 export CATALINA_2_HOME=/usr/java/tomcat2/apache-tomcat-8.5.35 export TOMCAT_2_HOME=/usr/java/tomcat2/apache-tomcat-8.5.35  修改完成并保存后需要更新配置，可以使用下面命令进行更新，或者重启服务器：  source /etc/profile\n 四、Tomcat启动路径的修改  用linux的vim命令编辑第二个Tomcat（第一个Tomcat使用默认路径即可，可以不做修改）安装目录下的bin/catalina.sh文件，或者直接用模拟器内置的Text编辑器打开，找到如下位置（红框内的为需要加入的代码）：\n   文件打开命令：  sudo vim /usr/java/tomcat2/apache-tomcat-8.5.35/bin/catalina.sh\n加入代码（等号右边为上一步配置的相应Tomcat的环境变量，修改后保存）：  CATALINA_BASE=$CATALINA_2_BASE CATALINA_HOME=$CATALINA_2_HOME  五、巨坑回顾及学习成果 1. 填坑总结：\n  如果要修改Tomcat端口，需要先关闭tomcat，再修改端口，否则可能出现端口一直被占用的情况，此时只能重启服务器（个人方法）。\n  进行集群tomcat配置的时候，需要修改防火墙入站规则，即加入允许访问的端口（阿里云服务器可以在控制台的入站规则里修改），否则一直出现503！！！\n  2. 学习成果：\n  netstat -an | grep 8080：可以查看端口的占用情况\n  sudo vim server.xml：以默认权限（管理员）修改文件\n  echo $CATALINA_BASE：可以查看环境变量是否生效\n  reboot：重启服务器\n  source /etc/profile：更新配置文件，使得配置生效\n  ","id":64,"section":"posts","summary":"一、Tomcat的安装 单个Tomcat的安装（包括基本JDK，Mysql环境搭建）具体可参考这边笔记#20 ，现在需要将多个Tomcat安装到","tags":[""],"title":"单机部署Tomcat集群","uri":"https://PI-KA-CHU.github.io/2019/01/%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2tomcat%E9%9B%86%E7%BE%A4/","year":"2019"},{"content":"   列表： 有序，可变 [ 'crunchy frog' , 'ram bladder' , 'lark vomit' ]\n  元组： 有序，不可变 ( 'crunchy frog' , 'ram bladder' , 'lark vomit' )\n  字典： 无序，可变 { 'x' : 'crunchy frog' , 'y' : 'ram bladder' , 'z' : 'lark vomit' }\n  集合： 无序，可变 { 'crunchy frog' , 'ram bladder' , 'lark vomit' }\n  字符串： 有序，不可变 “ crunchy frog ram bladder lark vomit ”\n  range： 有序，不可变 range(10)、range(2, 5)、range(5, 2, -1)\n   转化：\n 其他格式转化为list格式：   其他格式转化为元组格式：   其他格式转化为字符串格式：   map语法 ： map(function, iterable)，下面的图中是迭代把i值从nt转化为str，再把list转化为字符串\n  其他格式转化为字典格式：   enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标。（使其能够转化为字典）\n  zip(arr) 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。zip(*arr)是将打包好的重新回到未打包之前的元组列表，相当于转置。 如果各个迭代器的元素个数不一致，则 返回列表长度与最短的对象相同 ，利用 * 号操作符，可以将元组解压为列表。\n  其他格式转化为集合格式：   python的有序序列切片  切片的格式及使用：\n  切片使用2个冒号分隔的3个数字来完成\n  第一个数字表示切片开始位置，默认值为0\n  第二个数字表示切片截止（但不包含）位置，默认值为列表长度len(S)\n  第三个数字表示切片的步长，默认值为1\n  ","id":65,"section":"posts","summary":"列表： 有序，可变 [ 'crunchy frog' , 'ram bladder' , 'lark vomit' ] 元组： 有序，不可变 ( 'crunchy frog' , 'ram bladder' , 'lark vomit' ) 字典： 无序，可变 { 'x' : 'crunchy frog' , 'y' : 'ram bladder' , 'z' : 'lark vomit' } 集合： 无序，可变 {","tags":[],"title":"Python期末复习","uri":"https://PI-KA-CHU.github.io/2019/01/python%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/","year":"2019"},{"content":" JDBC操作数据库的一般步骤  加载正确的数据驱动程序 Class.forName(\u0026quot;com.mysql.jdbc.Driver\u0026quot;) 定义所要连接数据库的地址 String url = \u0026quot;jdbc:mysql://localhost:3306/dbname?SSL=false\u0026quot; 建立与数据库的连接 Connection conn = DriverManager.getConnection(url,username,password); 创建语句对象 Statement stmt = conn.createStatement(); 声明SQL语句，并将该语句通过Statement对象提交给服务器进行执行  String sql = \u0026#34;SELECT name FROM t_student\u0026#34;; ResultSet rs= stmt.executeQuery(sql);  对查询结果进行分析  while(rs.next()){ name = rs.getString(\u0026#34;name\u0026#34;); phone = rs.getString(\u0026#34;sex\u0026#34;); }  关闭打开的资源  rs.close(); stmt.close(); conn.close();  Statement的三种类型 Statement：常规语句\n  普通的不带参的查询SQL\n  支持批量更新和删除\n  PreparedStatement：预置语句\n  可变参数的SQL，编译一次，执行多次，效率高\n  安全性好，有效防止SQL注入等问题\n  支持批量更新和删除\n  PreparedStatement stmt = connection.prepareStatement(\u0026#34;insert into test values(\u0026#39;?,?\u0026#39;)\u0026#34;); stmt.setString(1,”first value”); stmt.setString(2,”second value”); stmt.executeUpdate(); CallableStatement：可调用语句\n  继承自PreparedStatement，支持带参数的SQL操作；\n  支持调用服务器上的存储过程，提供了对输入/输出参数（IN OUT）的支持\n  // 创建有输入输出参数的存储过程： create procedure getNameById(in cid int,out return_name varchar(20)) select name from usertbl where id = cid; CallableStatement cstmt = conn.prepareCall(\u0026#34;{call getNameById(?,?)}\u0026#34;); // 输入输出变量都要以?表示 cstmt.setInt(1, 20); // 给第一个问号赋值为20 // 注册输出参数 cstmt.registerOutParameter(2, Types.CHAR); // 给作为输出变量的第二个问号注册，并制定其为char型 cstmt.execute(); // 执行 call getNameById() String name = cstmt.getString(2); // 取存储过程中的第二个变量的值  网络服务可以采用技术来提高系统性能  负载均衡： 如nginx 缓存技术： 如redis 连接池技术： 如数据库连接池c3p0，hikari 容错机制   数据库连接池的工作机制   数据库连接池在初始化时将创建一定数量的数据库连接放到连接池中，这些数据库连接的数量是有最小数据库连接数决定的。\n  无论这些数据库连接是否被使用，连接池都将一直保证至少拥有这么多的连接数量。\n  连接池的最大数据库连接数量限定了这个连接池能占有的最大连接数。\n  当应用程序向连接池请求的连接数超过最大连接数量时，这些请求将被加入到等待队列中。\n   数据库中事务理解：（JAVA中对事务的处理）   原子性： 原子性是指事务中的操作要么全做，要么都不做，保证数据库的一致性\n  一致性： 一致性是指数据库在事务在操作前后的数据都必须满足业务规则约束。\n  隔离性： 隔离性是数据库允许多个并发食物同时对齐数据进行读写和修改的能力，隔离性防止了多个事务并发执行时由于交叉执行而导致的数据库不一致。\n  持久性：持久性表示为：事务处理结束后，对数据库的修改就是永久的，即便系统故障也不会丢失。\n  ","id":66,"section":"posts","summary":"JDBC操作数据库的一般步骤 加载正确的数据驱动程序 Class.forName(\u0026quot;com.mysql.jdbc.Driver\u0026quot;) 定义所要连接数据库的地址 String url = \u0026quot;jdbc:mysql://localhost:3306/dbname?SSL=false\u0026quot; 建立与数据库的连接 Connection conn = DriverManager.getConnection(url,username,password); 创建语句对象 Statement stmt = conn.createStatement(); 声明SQL","tags":["mysql"],"title":"JDBC期末复习","uri":"https://PI-KA-CHU.github.io/2019/01/jdbc%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/","year":"2019"},{"content":" 前言  特别推荐（web.xml详解）： https://blog.csdn.net/believejava/article/details/43229361   Servlet的生命周期  当servlet被部署在应用服务器中（应用服务器中用于管理Java组件的部分被抽象成为容器）以后，由容器控制servlet的生命周期。 除非特殊制定，否则在容器启动的时候，servlet是不会被加载的，servlet只会在第一次请求的时候被加载和实例化。 servlet一旦被加载，一般不会从容器中删除，直至应用服务器关闭或重新启动。但当容器做内存回收动作时，servlet有可能被删除。也正是因为这个原因，第一次访问servlet所用的时间要大大多于以后访问所用的时间。   Servlet的工作原理  当客户端向服务器发送请求一个servlet时，服务器收到后首先到容器中检索与请求匹配的Servlet实例是否存在。若不存在，则Servlet容器负责 加载和实例化 出该servlet的实例对象，接着容器框架负责调用 init() 方法进行一些相关的初始化工作，然后Servlet容器继续调用其 service() 方法。若存在，则直接调用该servlet实例的service()方法。然后判断请求的类型，调用doGet或doPost等方法。   Tomcat中的JSP引擎就是一个Servlet程序，负责解释执行JSP页面 处理流程： 1）客户通过浏览器向服务器端的JSP页面发送请求。 2）容器接受到客户请求后，会检查JSP文件对应编译后的Servlet文件（.class）是否存在。如果不存在，则跳转到第4）步，否则执行下一步。 3）容器检查JSP页面是否有更新（修改），没有更新，则跳转到第5）步，否则执行下一步。 4）容器将JSP文件转换为Servlet类源文件（.java）。（此步会检查和捕获JSP语法错误） 5）容器将Servlet源文件（.java）编译成相应的字节码（.class）文件。（会检查和捕获Java语法错误） 6）容器将字节码（.class）文件加载到内存。 7）容器实例化Servlet（调用构造函数），然后调用初始化方法（jspInit()）初始化Servlet。到此为止，Servlet对象真正成为一个Servlet，准备就绪，可以处理客户的请求了。 8）容器创建一个新的线程来运行Servlet并运行Servlet的_jspService()方法处理客户的请求。 9）Servlet生成并向客户返回一个响应（或把请求转发给另一个Web应用组件处理）。\n 通常，一个Servlet在容器中只有一个实例，每当有请求来到时，则分配一条线程进行处理。 在处理请求时：    Servlet容器会创建一个请求对象ServletRequst，其中封装了用户请求的信息，以便处理客户端请求，此外还会创建一个响应对象ServletResponse，用于响应客户端请求，想客户端返回数据。\n  然后Servlet容器把创建好的ServletRequst和ServletResponse对象传给用户所请求的Servlet。\n  Servlet利用ServletResponse包含的数据和自身的业务逻辑处理请求，并把处理好的结果写在ServletResponse中，最后Servlet容器把响应结果传给用户。\n   JavaBean学习 什么是javaBean，如何进行交互：\n JavaBean是一种用JAVA语言编写的可重用的组件，用户可以使用javaBean进行业务逻辑及数据库操作的打包，其他开发者可以通过Jsp，Servlet，其他javaBean，Applet等对javaBean进行操作，用户可以认为JavaBean提供了一种随时随地的复制和粘贴的功能，而不用关心任何改变。  \u0026lt;jsp:useBean id=\u0026#34;loginUser\u0026#34; class=\u0026#34;com.po.Users\u0026#34; scope=\u0026#34;page\u0026#34;\u0026gt;\u0026lt;/jsp:useBean\u0026gt; \u0026lt;jsp:useBean id=\u0026#34;userDao\u0026#34; class=\u0026#34;Dao.UserDao\u0026#34;\u0026gt;\u0026lt;/jsp:useBean\u0026gt; \u0026lt;jsp:setProperty name=\u0026#34;beanName\u0026#34; property=\u0026#34;propertyName\u0026#34; value=\u0026#34;\u0026#34;\u0026gt; Bean的三大特性：\n 独立性 可重用性 状态可保存性  javaBean的三大核心部分：\n 属性（properties） 方法（method） 事件（event）   事件为JavaBean组件提供了一种发送通知给其他组件的方法。在AWT事件模型中，一个事件源可以注册事件监听器对象。当事件源检测到发生了某种事件时，它将调用事件监听器对象中的一个适当的事件处理方法来处理这个事件。\n javaBean的三大构件：\n  Session bean： 会话构件，是短暂的对象，运行在服务器上，并执行一些应用逻辑处理，它由客户端应用程序建立，其数据需要自己来管理。分为无状态和有状态两种。\n  Entity bean： 实体构件，是持久对象，可以被其他对象调用。在建立时指定一个唯一标示的标识，并允许客户程序，根据实体bean标识来定位beans实例。多个实体可以并发访问实体bean，事务间的协调由容器来完成。\n  MessageDriven Bean： 消息构件，是专门用来处理JMS（Java Message System）消息的规范（EIB2.0）。JMS是一种与厂商无关的API，用来访问消息收发系统，并提供了与厂商无关的访问方法，以此来访问消息收发服务。JMS客户机可以用来发送消息而不必等待回应。\n   其他的常见问题  Session用哪些方法设置超时时间？  优先级：Servlet中API设置 \u0026gt; 程序/web.xml设置 \u0026gt; Tomcat/conf/web.xml设置\n //方式一：在web.xml中设置session-config \u0026lt;session-config\u0026gt; \u0026lt;session-timeout\u0026gt;2\u0026lt;/session-timeout\u0026gt; \u0026lt;/session-config\u0026gt; //方式二：在Tomcat的/conf/web.xml中session-config，默认值为30分钟 \u0026lt;session-config\u0026gt; \u0026lt;session-timeout\u0026gt;30\u0026lt;/session-timeout\u0026gt; \u0026lt;/session-config\u0026gt; //方式三：在servlet中设置 HttpSession session = request.getSession(); session.setMaxInactiveInterval(60);//单位为秒  session存取对象分别用什么方法？  request.getSession.getAttribute() request.getSession.setAttribute()  Web.xml文件的位置？  WebContent/WEB-INF/  创建一个监听类，需要实现什么接口？  监听不同的类型的类需要实现不同的接口，如HttpSessionListener  Response对象的什么方法可以将cookie写入会话中重写URL接口？  response.encodeRedirectURL(“”)  Filter控制生命周期的方法是哪三个？  init()，doFilter()，destroy() ","id":67,"section":"posts","summary":"前言 特别推荐（web.xml详解）： https://blog.csdn.net/believejava/article/details/43229361 Servlet的生命周期 当servlet被部署在应用服务器中（应用服务器中用于管理Java组件的部分被","tags":[""],"title":"Servlet及JavaBean期末复习","uri":"https://PI-KA-CHU.github.io/2019/01/servlet%E5%8F%8Ajavabean%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/","year":"2019"},{"content":" JSP的工作机制  https://www.jianshu.com/p/93736c3b448b   基本工作流程： 浏览器向服务器请求JSP页面，服务器收到该请求后，检查JSP文件内容是不是为第一次访问（是的话则创建servlet），检车JSP文件内容是否被修改（是的话则新建servlet），如果是，则这个JSP文件会在服务器端的JSP 引擎作用下转化为一个servlet类的java源代码，然后会在java编译器的作用下转化为一个字节码文件，并装载到JVM解释运行。   JSP的三种脚本元素：  声明（declaration）：用于在JSP中声明合法的变量和方法 \u0026lt;%! 代码内容 %\u0026gt; 表达式（expression）：计算该表达式，将其结果转换成字符串插入到输入中\u0026lt;%=name %\u0026gt; 脚本（Scriplets）：直接编写一行或多行代码 \u0026lt;% 代码块 %\u0026gt;   JSP的三种指令元素  page指令：用于声明JSP页面的属性等  \u0026lt;%@ page 2 [ language=\u0026#34;java\u0026#34; ] 3 [ extends=\u0026#34;package.class\u0026#34; ] 4 [ import=\u0026#34;{package.class | package.*}, ...\u0026#34; ] 5 [ session=\u0026#34;true | false\u0026#34; ] 6 [ buffer=\u0026#34;none | 8kb | sizekb\u0026#34; ] 7 [ autoFlush=\u0026#34;true | false\u0026#34; ] 8 [ isThreadSafe=\u0026#34;true | false\u0026#34; ] 9 [ info=\u0026#34;text\u0026#34; ] 10 [ errorPage=\u0026#34;relative_url\u0026#34; ] 11 [ isErrorPage=\u0026#34;true | false\u0026#34; ] 12 [ contentType=\u0026#34;mimeType [ ;charset=characterSet ]\u0026#34; | \u0026#34;text/html ; charset=ISO-8859-1\u0026#34; ] 13 [ pageEncoding=\u0026#34;characterSet | ISO-8859-1\u0026#34; ] 14 [ isELIgnored=\u0026#34;true | false\u0026#34; ] 15 %\u0026gt;  include指令：通过include指令将包含的源代码添加到页面中  静态包含：把其它资源包含到当前页面中。 \u0026lt;%@ include file=\u0026#34;/include/header.jsp\u0026#34; %\u0026gt; 动态包含： \u0026lt;jsp:include page=\u0026#34;/include/header.jsp\u0026#34;\u0026gt;\u0026lt;/jsp:include\u0026gt; 原则：能用静态就不要用动态  taglib指令：用于引入外部tag标签库后者自定义tag标签库`  taglib指令有两个属性，uri为类库的地址，prefix为标签的前缀 \u0026lt;%@ taglib uri=\u0026#34;http://java.sun.com/jsp/jstl/core\u0026#34; prefix=\u0026#34;c\u0026#34;%\u0026gt;  JSP的四个作用域(page，request，session，application)   page： 当前访问的页面内有效，关闭页面重新打开或刷新后变量或对象重置；\n  request： 变量或对象存在于一次完整HTTP请求与响应期间，完成后被释放。当跳转到其他的JSP页面就失效了。\n  session： HTTP会话从开始（浏览器打开）到结束这段时间，只要将数据存入session对象，数据的范围就是session\n  application： 服务器从启动带停止这一段时间，application作用域上的信息传递是通过ServletContext实现的。\n   JSP的六种常见行为动作   \u0026lt;jsp:include \u0026gt; ：动态包含\n  \u0026lt;jsp:forward \u0026gt; ：请求转发\n  \u0026lt;jsp:param \u0026gt; ：设置请求参数\n  \u0026lt;jsp:useBean \u0026gt; ：创建一个对象\n  \u0026lt;jsp:setProperty \u0026gt;： 给指定的对象属性赋值\n  \u0026lt;jsp:getProperty \u0026gt; ：取出指定对象的属性值\n   JSP九大内置对象 out：out.write(\u0026quot;⽂字内容\u0026quot;);\n 类型：Javax.servlet.jsp.JspWriter 作⽤：主要⽤来向客户端输出数据 作⽤域：page。也就是说，每个⻚⾯都有⼀个⾃⼰的out对象。 重要⽅法：print()/println()/write() 向客户端⻚⾯输出数据  request：\n 类型：Javax.servlet.http.HttpServletRequest 描述：来⾃客户端的请求经Servlet容器处理后，由request对象进⾏封装。注：客户端和服务器的⼀ 次通信就是⼀次请求（发送请求或得到相应）。 作⽤域：request。说明，这次请求结束后，它的⽣命周期 就结束了。  request.getRequestDispatcher(\u0026#34;list.jsp\u0026#34;).forward(request,response) 转发(通过代码的⽅式进⾏转发) request.setCharacterEncoding(\u0026#34;UTF-8\u0026#34;) 对请求数据重新编码 getParameter(key) 获取提交表单的数据 getParameterValues(key) 获取提交表单的⼀组数据 request.setAttribute(key,object) 设置请求对象的属性 request.gettAttribute(key) 获取请求对象的属性 response：\n 类型：Javax.servlet.http. HttpServletResponse 描述：它封闭了JSP 的响应，然后被发送到客户端以响应客户的请求。 作⽤域：page  response.sendRedirect(\u0026#34;⻚⾯\u0026#34;)：⻚⾯跳转。注意，之前的forward是转发，这⾥是跳转，注意区分。 response.setCharacterEncoding(\u0026#34;gbk\u0026#34;)：设置响应编码 session：\n  会话的创建过程： 如果是第⼀次接触“会话”这个概念，需要重复⼀下。说⽩了，客户端与服务器之间可能需要不断地进行 数据交互（请求与相应），这个过程就可以理解为⼀段会话。Tomcat默认的会话时间为30分钟，这段时间内如果没有交互，会话结束；下次客户端⼀旦发送请求，重新创建会话。当客户端第⼀次发送请求的时候，才会创建⼀个会话。session的⽣命周期⽐request⻓\n  session创建的详细过程： 当客户端第一次访问JSP文件的时候，Servlet容器（Tomcat）会创建session，如果不是第一次访问，则会检索是否存在此session，存在的话则使用，不存在的话则重新创建，此时为新会话。\n  类型：Javax.servlet.http.HttpSession\n  描述：表示⼀个会话，⽤来保存⽤户信息，以便跟踪每个⽤户的状态。（不要⽤来保存业务数据，request）\n  定义：是指在⼀段时间内客户端和服务器之间的⼀连串的相关的交互过程。\n  作⽤域：session。\n  /* 会话结束的条件之⼀： 服务器关闭 会话过期（⼀段会话时间默认为30分钟） ⼿动终⽌会话 【举例】为保持⽤户登录的状态，我们可以把⽤户的数据信息保存在session中。 */ session.getid()：取得session的id号.id由tomcat⾃动分配。 session.isnew()：判断session时候是新建的 session.setMaxInactiveInterval(1000*60*30)：设置当前会话失效时间(ms) 。Tomcat默认的会话时间为30分钟。 session.invalidate()：初始化当前会话对象(⼀般在推出的时候使⽤，可以删除当前会话的数据) session.setAttribute(key,object)：往当前会话中设置⼀个属性 session.getAttribute(key)：获取当前会话中的⼀个属性 session.removeAttribute(key)：删除当前会话中的属性 pageContext：\n 类型：javax.servlet.jsp.PageContext 描述：本JSP的⻚⾯上下⽂。 作⽤域：page 注：上下⽂的理解：上下⽂可以联系到当前⻚⾯所有的信息。 我们先来回顾⼀下原始的request.jsp代码：  //通过PageContext上下⽂对象获取当前⻚⾯的其他内置对象 pageContext.getRequest(); pageContext.getResponse(); pageContext.getSession(); pageContext.getOut(); String path = request.getContextPath( application：\n这个对象的⽣命周期是最⻓的。服务器启动的时候就会创建application对象。从服务器存在到服务器 终⽌，都⼀直存在，且只保留⼀个对象，所有⽤户共享⼀个application。不是很常⽤。\n 类型：javax.servlet.ServletContext 描述：从servlet配置对象获得的servlet上下⽂ 作⽤域：application  //⼀个应⽤程序只有⼀个application对象 //在服务器启动时创建，到服务器关闭时销毁 //所有客户端共享⼀份 String serverPath = application.getContextPath();//获取当前应⽤程序的路径 //向application对象添加数据 application.setAttribute(\u0026#34;\u0026#34;, \u0026#34;\u0026#34;); config：\n 类型：javax.servlet.ServletConfig 描述：本JSP的 ServletConfig 作⽤域：page 注：代表配置对象，基本⽤不到。  page：\n 类型：java.1ang.Object 描述：实现处理本⻚当前请求的类的实例（javax.servlet.jsp.HttpJspPage），转换后的Servlet类 本身 作⽤域：page  exception：\n  JSP常⻅错误状态码： 403：禁⽌访问。⽐如IP地址被拒绝，站点访问被拒绝等 404：找不到。没有找到⽂件或⽬录 500：服务器由于遇到错误⽽不能完成该请求，Web服务器太忙\n  类型：java.lang.Exception\n  描述：本JSP⻚⾯的异常对象\n  作⽤域：page\n   JSP的自定义标签 自定义标签库方式：\n  tag方式，引用的时候通过tagdir属性指定.tag文件的位置，例如 \u0026lt;%@ taglib prefix=\u0026quot;x\u0026quot; tagdir=\u0026quot;/WEB-INF/tags\u0026quot; %\u0026gt;\n  tld方式，通过uri属性指定.tld文件的位置，需要在web.xml中jsp-config配置taglib \u0026lt;%@ taglib prefix=\u0026quot;c\u0026quot; tagdir=\u0026quot;http://java.sun.com/jsp/jstl/core\u0026quot; %\u0026gt;\n   自定义标记的方式：\n  简单标记，不处理体内容： 需实现javax.servlet.jsp.tagext.Tag接口，这个接口实现javax.servlet.jsp.tagext.TagSupport\n  处理体内容的标记： 需要实现javax.servlet.jsp.tagext.BodyTag接口，这个接口实现java.servlet.jsp.tagext.BodyTagSupport\n  使用步骤：\n 引入javax.servlet-api，jsp-api，jstl三个包 新建标签库描述文件，放在webapp/WEB-INF/tld目录下 标签库描述文件格式：http://book.51cto.com/art/201004/193459.htm   动态include与静态include的区别   静态include： \u0026lt;%@include file=\u0026quot;include.html\u0026quot; %\u0026gt; 静态include主要是对静态页面的引入，不会检查所包含文件的变化，同时编译时只生成一个class文件（先包含include，后编译）\n  动态include： \u0026lt;jsp:include file=\u0026quot;include.html\u0026quot; /\u0026gt; 动态include主要是对动态页面的引入，它会检查所引入页面的变化，如果所包含的资源在请求间发生变化，则下次请求此资源时，将包含新的内容。编译后会生成多个class文件。（先编译include的文件，再包含）\n   JSP的常见问题   JSP如何把页面的编码格式设为utf-8： \u0026lt;%@ page pageEncoding=”utf-8” %\u0026gt;\n  JSP通常使用什么内置对象实现对用户会话的跟踪： session\n  taglib用什么方法接收参数，用什么方法发送参数？ 接受参数（EL表达式）：${param.name} 发送参数：\u0026lt;jsp:param name=\u0026quot;\u0026quot; value=\u0026quot;\u0026quot;/\u0026gt;\n   参考文献：\n https://www.cnblogs.com/zhangyinhua/p/7637399.html#_label1  ","id":68,"section":"posts","summary":"JSP的工作机制 https://www.jianshu.com/p/93736c3b448b 基本工作流程： 浏览器向服务器请求JSP页面，服务器收到该请求后，检查JSP文件内容是不是为第一次访问（是的话则创建serv","tags":[""],"title":"JSP期末复习","uri":"https://PI-KA-CHU.github.io/2019/01/jsp%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/","year":"2019"},{"content":"一、Maven环境隔离简介 基本类别  本地开发环境（local） 开发环境（dev） 测试环境（beta） 线上环境（prod）  解决的问题  避免人工修改配置信息导致的问题 轻松分环境编译，打包，部署 \u0026hellip;\u0026hellip;  环境隔离的依赖   Spring Profile：\n  a. Spring可使用Profile决定程序在不同环境下执行情况，包含配置、加载Bean、依赖等。 Spring的Profile一般项目包含：dev(开发), test(单元测试), qa(集成测试), prod(生产环境)。由spring.profiles.active属性决定启用的profile。\n  b. SpringBoot的配置文件默认为 application.properties(或yaml，此外仅以properties配置为说明)。不同Profile下的配置文件由application-{profile}.properties管理，同时独立的 Profile配置文件会覆盖默认文件下的属性。\n    Maven Profile： Maven同样也有Profile设置，可在构建过程中针对不同的Profile环境执行不同的操作，包含配置、依赖、行为等。 Maven的Profile由 pom.xml 的标签管理。每个Profile中可设置：id(唯一标识), properties(配置属性), activation(自动触发的逻辑条件), dependencies(依赖)等。\n   二、相关配置  pom.xml中，在build（dependencies）同级的地方加入：  //分别代表开发（dev），测试（beta）和线上（prod）环境 \u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;dev\u0026lt;/id\u0026gt; \u0026lt;activation\u0026gt; \u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt; \u0026lt;/activation\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;profile\u0026gt;dev\u0026lt;/profile\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;beta\u0026lt;/id\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;profile\u0026gt;beta\u0026lt;/profile\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;prod\u0026lt;/id\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;profile\u0026gt;prod\u0026lt;/profile\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt;  在classpath下分别创建三个环境需要的application-${profile}.propertices：   在application.propertices中配置指定的环境：  //配置动态的profile注入（profile的值由maven修改） spring.profiles.active=@profile@   参考文章：  https://www.jianshu.com/p/948c303b2253 https://yq.aliyun.com/articles/670083 http://tengj.top/2017/02/28/springboot2/ https://www.jianshu.com/p/ad1b8157feaf  ","id":69,"section":"posts","summary":"一、Maven环境隔离简介 基本类别 本地开发环境（local） 开发环境（dev） 测试环境（beta） 线上环境（prod） 解决的问题 避免人工修改","tags":["","spring-boot"],"title":"Maven + SpringBoot实现环境隔离","uri":"https://PI-KA-CHU.github.io/2019/01/maven--springboot%E5%AE%9E%E7%8E%B0%E7%8E%AF%E5%A2%83%E9%9A%94%E7%A6%BB/","year":"2019"},{"content":"工具下载   下载地址： https://www.seleniumhq.org/download/\n  Selenuim使用firefox时所需的驱动： https://github.com/mozilla/geckodriver/releases/tag/v0.9.0\n  基本操作  将相关依赖包导入Eclipse中，导入完成后即可使用，下面是登陆QQ邮箱的代码：\n //配置启动Firefox时所需的驱动 System.setProperty(\u0026#34;webdriver.gecko.driver\u0026#34;, \u0026#34;C:\\\\Users\\\\pc\\\\Desktop\\\\桌面文件夹\\\\作业\\\\软件测试\\\\selenium-jav\\\\geckodriver.exe\u0026#34;); //配置本地Firefox的地址 System.setProperty(\u0026#34;webdriver.firefox.bin\u0026#34;, \u0026#34;E:\\\\Program Files (x86)\\\\Firefox\\\\firefox.exe\u0026#34;); WebDriver driver = new FirefoxDriver(); // 设置查找元素最大时间 Timeouts tos = driver.manage().timeouts(); tos.implicitlyWait(5, java.util.concurrent.TimeUnit.SECONDS); driver.get(\u0026#34;https://mail.qq.com\u0026#34;); Thread.sleep(3000); //跳转到登陆模块 driver.switchTo().frame(\u0026#34;login_frame\u0026#34;); //获取账号输入框 WebElement username = driver.findElement(By.name(\u0026#34;u\u0026#34;)); username.sendKeys(usernameValue); //获取密码输入框 WebElement password = driver.findElement(By.id(\u0026#34;p\u0026#34;)); password.sendKeys(passwordValue); //模拟点击登陆按钮 WebElement button = driver.findElement(By.id(\u0026#34;login_button\u0026#34;)); button.click(); //跳转到上一层界面 driver.switchTo().defaultContent(); //下面是独立密码输入，如果没有的话可以不用 WebElement password2 = driver.findElement(By.id(\u0026#34;pp\u0026#34;)); password2.sendKeys(\u0026#34;******\u0026#34;); //模拟点击登陆按钮 WebElement button2 = driver.findElement(By.id(\u0026#34;btlogin\u0026#34;)); button2.click(); //获取指定标签的text内容 WebElement logo = driver.findElement(By.id(\u0026#34;useraddr\u0026#34;)); String text = logo.getText(); System.out.println(text); if(\u0026#34;529709737@qq.com\u0026#34;.equals(text)) { System.out.println(\u0026#34;页面正确\u0026#34;); //关闭浏览器 driver.close(); } 附录： //通过xpath获取（百度）标签元素： elm = driver.findElement(By.xpath(\u0026#34;//div[@id=\u0026#39;1\u0026#39;]/h3/a\u0026#34;)); //处理Alert弹出框： Alert alert = driver.switchTo().alert(); //点击确认 alert.accept(); //点击取消按钮 alert.dismiss() //需要输入文本的弹出框的处理 alert.sendKeys(keyValue) //获取弹出框的文本内容 alert.getText() /* 要在多个窗口之间进行切换，首先必须获取每个窗口的唯一标识符（句柄）， 通过WebDriver对象的getWindowHandles()方法，可以获取所有打开窗口的标 识符，并将其以集合的形式返回 */ String[] handles = new String[driver.getWindowHandles().size()]; driver.getWindowHandles().toArray(handles); /* 通过Options对象对测试进行设置，设置内容包括Cookie、超时时间和浏览器窗口 Timeouts对象包含三种方法： 1. implicitlyWait：设置脚本在查找元素时的最大时间 2. pageLoadTimeout：页面操作超时时间 3. setScriptTimeout：设置脚本异步执行时间 */ Timeouts timeouts = driver.manage().timeouts(); Timeouts.implicitlyWait(30, java.util.concurrent.TimeUnit.SECONDS); ","id":70,"section":"posts","summary":"工具下载 下载地址： https://www.seleniumhq.org/download/ Selenuim使用firefox时所需的驱动： https://github.com/mozilla/geckodriver/releases/tag/v0.9.0 基本操作 将相关依赖包导入Eclipse中，导入完成后即可使用，下面是登","tags":["","开发工具"],"title":"Java Selenium的基本使用","uri":"https://PI-KA-CHU.github.io/2019/01/java-selenium%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","year":"2019"},{"content":"工具下载   官网下载： http://jd.benow.ca/\n  官网包括Eclipse插件，Intellij插件及独立的图形实用程序——JD-GUI，此处下载的是JD-GUI\n  工具的使用  解压后运行，将编译后的.class文件打开即可展现反编译后的代码，maven项目的.class文件可以从项目的target文件夹中找到。\n ","id":71,"section":"posts","summary":"工具下载 官网下载： http://jd.benow.ca/ 官网包括Eclipse插件，Intellij插件及独立的图形实用程序——JD-GUI，此处下载的是JD-GUI 工具的使用","tags":["","开发工具"],"title":"JAVA反编译工具","uri":"https://PI-KA-CHU.github.io/2018/12/java%E5%8F%8D%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7/","year":"2018"},{"content":"Lombok的简介及安装   Lombok能够简化代码的复杂度，例如对dao层或者bean的类进行getter，setter，构造方法，toString方法和equal等方法的进行简化，使用注解即可实现，只要程序实现了该API，就能在javac（编译）运行时得到调用，通过注解在编译时生成新的语法树，再进而转换为字节码。\n  Lombok插件的下载：https://projectlombok.org/download （windows环境下）下载插件后，双击lombok.jar进行安装，选择自己的编译器（这里是Eclipse）后install导入即可。\n  Lombok的maven依赖：https://projectlombok.org/setup/maven 将maven依赖复制黏贴到项目pom.xml依赖中即可\n  Lombok的常见注解及使用   @Getter/@Setter：实现get和set方法\n  @ToString：实现类toString方法\n  @EqualsAndHashCode：实现hashCode和equals方法\n  @Data：实现@ToString，@EqualsAndHashCode， @Getter在所有领域，@Setter所有非final字段，以及 @RequiredArgsConstructor方法\n  @Slf4j：实现日志创建 private static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(LogExample.class);\n  可使用@Getter(of = {\u0026ldquo;id\u0026rdquo;,\u0026ldquo;name\u0026rdquo;})来实现对指定元素构造及@Getter(exclude = {\u0026ldquo;id\u0026rdquo;,\u0026ldquo;name\u0026rdquo;})除去选中元素构造\n  官方详细文档：https://projectlombok.org/features/all\n  使用：在所需注解类上使用以上注解即可，使用前需导入插件及jar包（或者maven依赖）\n  ","id":72,"section":"posts","summary":"Lombok的简介及安装 Lombok能够简化代码的复杂度，例如对dao层或者bean的类进行getter，setter，构造方法，toStr","tags":["开发工具"],"title":"Lombok的安装及使用","uri":"https://PI-KA-CHU.github.io/2018/12/lombok%E7%9A%84%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8/","year":"2018"},{"content":"Command Line Tools工具包下载\n Command Line Tools for Xcode：https://developer.apple.com/download/all/?q=Command%20Line%20Tools  MAC添加环境变量  https://www.jianshu.com/p/acb1f062a925 https://zhuanlan.zhihu.com/p/25976099  MAC常见命令  su  # 切换至root用户 su - root  pwd：查看当前路径  ","id":73,"section":"posts","summary":"Command Line Tools工具包下载 Command Line Tools for Xcode：https://developer.apple.com/download/all/?q=Comm","tags":["开发工具"],"title":"Lombok的安装及使用","uri":"https://PI-KA-CHU.github.io/2018/12/mac%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/","year":"2018"},{"content":"slf4j简介  slf4j是一套包装Logging 框架的界面程式，以外观模式实现。可以在软件部署的时候决定要使用的 Logging 框架，目前主要支援的有Java Logging API、log4j及logback等框架。slf4j必须与其他日志库配合才能使用，一般需要将抽象层（例如slf4j-api-xx.jar）+中间层（例如slf4j-log4j12）+实现层（例如log4j）这三层都配置好才能保证SLF4J正常运行。\n   maven依赖 \u0026lt;!-- slf4j日志依赖包 --\u0026gt; \u0026lt;!-- 抽象层依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.8.0-beta0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- 中间层和实现层依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-log4j12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.8.0-beta0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  log4j.properties配置   log4j支持两种文件类型的配置：XML格式的文件和properties属性文件，一般是将文件放置于src/main/resources/目录下。\n  配置中主要包括三个方面：日志的输出位置Appenders（控制台，文件，数据库等），日志的输出样式Layouts（HTML样式、自由指定样式等），日志根配置Logger（日志记录的最低等级等）；\n  详细配置信息参考：https://www.jianshu.com/p/ccafda45bcea\n   问题及解决   异常：java.lang.NoClassDefFoundError: org.slf4j.LoggerFactory 找了一个上午，以为是版本冲突或maven导入的jar包不完整，怎么改都不成功，绝望脸T_T，最后终于找到了解决方法，需要将maven依赖部署到程序集中。   解决方法： 右键项目—\u0026gt;“ Properties ”—\u0026gt;选择“ Deployment Assembly ”—\u0026gt;点击 “ add ” —\u0026gt;\u0026quot; Java Build Path Entries \u0026ldquo;—\u0026gt;\u0026rdquo; Maven Dependencies \u0026quot;   其他问题1：：如果创建servlet后或者改servlet的类名后出现servlet无法访问到的问题（404），将项目clean下后即可正常访问\n  其他问题2：：如果mave-update后jre一直回退到1.5版本，可以在pom中加入下面配置进行设置，版本号对应自己的jdk版本\n  \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;source\u0026gt;1.8\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;1.8\u0026lt;/target\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt;  参考网址：https://zhuanlan.zhihu.com/p/32475568\n","id":74,"section":"posts","summary":"slf4j简介 slf4j是一套包装Logging 框架的界面程式，以外观模式实现。可以在软件部署的时候决定要使用的 Logging 框架，目前主要支援的有Ja","tags":["","log"],"title":"slf4j + log4j日志框架配置","uri":"https://PI-KA-CHU.github.io/2018/12/slf4j--log4j%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6%E9%85%8D%E7%BD%AE/","year":"2018"},{"content":"JSTL依赖包下载（本人使用1.1.2版本）  https://archive.apache.org/dist/jakarta/taglibs/standard/binaries/  使用流程：   将依赖包中lib文件夹下的jstl和standard的jar包导入项目\n  在JSP头部引入taglib相关的地址：\n  \u0026lt;%@ taglib uri=\u0026#34;http://java.sun.com/jsp/jstl/core\u0026#34; prefix=\u0026#34;c\u0026#34;%\u0026gt; //基本taglib \u0026lt;%@ taglib prefix=\u0026#34;sql\u0026#34; uri=\u0026#34;http://java.sun.com/jsp/jstl/sql\u0026#34;%\u0026gt; //数据库操作相关taglib \u0026lt;%@ taglib prefix=\u0026#34;fn\u0026#34; uri=\u0026#34;http://java.sun.com/jsp/jstl/functions\u0026#34; %\u0026gt; //相关函数taglib 基本taglib的用法总结  - taglib操作数据库，使用${ }可以引用taglib定义的标签的变量名： \u0026lt;sql:setDataSource var=\u0026#34;myDB\u0026#34; driver=\u0026#34;com.mysql.jdbc.Driver\u0026#34; url=\u0026#34;jdbc:mysql://localhost:3306/test\u0026#34; user=\u0026#34;root\u0026#34; password=\u0026#34;19970121\u0026#34; /\u0026gt; \u0026lt;sql:query dataSource=\u0026#34;${myDB}\u0026#34; var=\u0026#34;totalList\u0026#34;\u0026gt;SQL语句\u0026lt;/sql:query\u0026gt; \u0026lt;sql:query dataSource=\u0026#34;${myDB}\u0026#34; var=\u0026#34;list\u0026#34;\u0026gt;${sql}\u0026lt;/sql:query\u0026gt; /**************************************************************************************************/ - 使用taglib基本操作： /* 设置某一变量的值 */ \u0026lt;c:set var=\u0026#34;sql\u0026#34; value=\u0026#34;SELECT id FROM t_student WHERE username=\u0026#39;曾\u0026#39;\u0026#34;\u0026gt;\u0026lt;/c:set\u0026gt; /* IF语句的使用（只有条件成立才会执行里面的程序）*/ \u0026lt;c:if test=\u0026#34;${authorName ne null and !empty authorName}\u0026#34;\u0026gt; ${sql.concat(\u0026#34;AND a.author LIKE \u0026#39;\u0026#34;).concat(authorName).concat(\u0026#34;%\u0026#39;\u0026#34;) } \u0026lt;/c:if\u0026gt; /* forEach的使用（list为执行sql语句后的var）*/ \u0026lt;c:forEach var=\u0026#34;item\u0026#34; items=\u0026#34;${list.rows}\u0026#34; begin=\u0026#34;0\u0026#34; end=\u0026#34;10\u0026#34;\u0026gt;\t\u0026lt;tr\u0026gt; \u0026lt;th scope=\u0026#34;row\u0026#34;\u0026gt;${item.id}\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;${item.imageUrl}\u0026#34; class=\u0026#34;td-img\u0026#34;\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;${item.title}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;${item.author}\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/c:forEach\u0026gt; /* choose的使用（类似于switch）*/ \u0026lt;c:choose\u0026gt; \u0026lt;c:when test=\u0026#34;${(total % number) eq 1}\u0026#34;\u0026gt; \u0026lt;c:set var=\u0026#34;pageT\u0026#34; value=\u0026#34;${total/number}\u0026#34;\u0026gt;\u0026lt;/c:set\u0026gt; \u0026lt;/c:when\u0026gt; \u0026lt;c:when test=\u0026#34;${(total % number) eq 2}\u0026#34;\u0026gt; \u0026lt;c:set var=\u0026#34;pageT\u0026#34; value=\u0026#34;${total/number}\u0026#34;\u0026gt;\u0026lt;/c:set\u0026gt; \u0026lt;/c:when\u0026gt; \u0026lt;c:otherwise\u0026gt; \u0026lt;c:set var=\u0026#34;pageT\u0026#34; value=\u0026#34;${total/number + 1}\u0026#34;\u0026gt;\u0026lt;/c:set\u0026gt; \u0026lt;/c:otherwise\u0026gt; \u0026lt;/c:choose\u0026gt; ","id":75,"section":"posts","summary":"JSTL依赖包下载（本人使用1.1.2版本） https://archive.apache.org/dist/jakarta/taglibs/standard/binaries/ 使用流程： 将依赖包中lib文件夹下的jstl和standard的jar包导入项目 在JSP头部引","tags":[""],"title":"Java-Taglib总结","uri":"https://PI-KA-CHU.github.io/2018/12/java-taglib%E6%80%BB%E7%BB%93/","year":"2018"},{"content":" 索引对查询有着重要的作用，对索引的认识是进行数据库性能提升的起点，数据库的优化一般是从两个方面，第一个是对SQL语句进行优化，第二是优化数据库表的设计包括索引的建立，这里主要记录第二种的相关学习，当数据库数据量很大的时候，索引能够极大的提高查询速度，本人现在面临的是千万条数据的数据表，遍历所有数据需要花上几分钟的时间，一般的查询都需要几十秒，这个肯定是不能接受的，建立索引后的查询速度为一秒左右，极大的提高了性能。  建表原则   越小的数据类型通常更好：越小的数据类型通常在磁盘、内存和CPU缓存中都需要更少的空间，处理起来更快。\n  简单的数据类型更好：整型数据比起字符，处理开销更小，因为字符串的比较更复杂。在MySQL中，应该用内置的日期和时间数据类型，而不是用字符串来存储时间；以及用整型数据类型存储IP地址。\n  尽量避免NULL：应该指定列为NOT NULL，除非你想存储NULL。在MySQL中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值。\n   标识符选择   选择合适的标识符是非常重要的。选择时不仅应该考虑存储类型，而且应该考虑MySQL是怎样进行运算和比较的。一旦选定数据类型，应该保证所有相关的表都使用相同的数据类型。\n  整型：通常是作为标识符的最好选择，因为可以更快的处理，而且可以设置为AUTO_INCREMENT。\n  字符串：尽量避免使用字符串作为标识符，它们消耗更好的空间，处理起来也较慢。而且，通常来说，字符串都是随机的，所以它们在索引中的位置也是随机的，这会导致页面分裂、随机访问磁盘，聚簇索引分裂（对于使用聚簇索引的存储引擎）。\n   索引入门  参考： http://www.cnblogs.com/hustcat/archive/2009/10/28/1591648.html  ","id":76,"section":"posts","summary":"索引对查询有着重要的作用，对索引的认识是进行数据库性能提升的起点，数据库的优化一般是从两个方面，第一个是对SQL语句进行优化，第二是优化数据","tags":["mysql"],"title":"MySQL索引的建立及优化","uri":"https://PI-KA-CHU.github.io/2018/12/mysql%E7%B4%A2%E5%BC%95%E7%9A%84%E5%BB%BA%E7%AB%8B%E5%8F%8A%E4%BC%98%E5%8C%96/","year":"2018"},{"content":" 百度，豆丁和道客巴巴都有不少优秀的文档，但大部分下载都需要积分或者充钱，下面推荐的是一款免费下载的，不过下载的是PDF和text文件，没有原版的wps，如果代码这些的话就不建议啦，正常纯文字文档的话还是很有用的。\n 下载方式  软件地址（亲测无毒）   http://www.bingdian001.com/?p=30  网盘链接   https://pan.baidu.com/s/1R-PFbP7-siKOa0pq8E8U-g 密码：mo34  使用步骤  解压后打开Fish.exe 打开需要下载的文档并复制其地址 将地址黏贴到工具后点击下载就可以啦  参考网址： https://www.jianshu.com/p/64f49e2e9f1a\n","id":77,"section":"posts","summary":"百度，豆丁和道客巴巴都有不少优秀的文档，但大部分下载都需要积分或者充钱，下面推荐的是一款免费下载的，不过下载的是PDF和text文件，没有原","tags":["实用工具"],"title":"免费文档下载工具","uri":"https://PI-KA-CHU.github.io/2018/12/%E5%85%8D%E8%B4%B9%E6%96%87%E6%A1%A3%E4%B8%8B%E8%BD%BD%E5%B7%A5%E5%85%B7/","year":"2018"},{"content":"参考：\n https://www.jianshu.com/p/104aa6e2342d https://www.jianshu.com/p/2315dda64ad2  Java反射的原理  Java反射机制可以让我们在编译期（Compile Time）之外的运行期（Runtime）获得任何一个类的字节码。包括接口、变量、方法等信息。还可以让我们在运行期实例化对象，通过调用get/set方法获取变量的值。  Java反射的功能：  可以判断运行时对象所属的类 可以判断运行时对象所具有的成员变量和方法 通过反射甚至可以调用private的方法 生成动态代理 Java反射的功能，一句话总结就是：反射用于在运行时检测和修改某个对象的结构及其行为。   实现Java反射的类  Class：它表示正在运行的Java应用程序中的类和接口 Field：提供有关类或接口的属性信息，以及对它的动态访问权限 Constructor：提供关于类的单个构造方法的信息以及对它的访问权限 Method：提供关于类或接口中某个方法信息 注：Class类是Java反射中最重要的一个功能类，所有获取对象的信息(包括：方法/属性/构造方法/访问权限)都需要它来实现   ","id":78,"section":"posts","summary":"参考： https://www.jianshu.com/p/104aa6e2342d https://www.jianshu.com/p/2315dda64ad2 Java反射的原理 Java反射机制可以让我们在编译期（Compile Time）之外的运行期（Runtime）获得任何一个类的字节码","tags":[""],"title":"JAVA反射技术","uri":"https://PI-KA-CHU.github.io/2018/12/java%E5%8F%8D%E5%B0%84%E6%8A%80%E6%9C%AF/","year":"2018"},{"content":"创建流程  创建Maven项目，设置Maven架构类型为maven-archetype-quickstart：   创建项目后，修改其pom.xml文件，如下：  \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.zeng\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;myspring-boot\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;war\u0026lt;/packaging\u0026gt; \u0026lt;name\u0026gt;myspring-boot\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.apache.org\u0026lt;/url\u0026gt; \u0026lt;!-- spring Boot的基本组件依赖 --\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.0.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- 单元测试依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.jayway.jsonpath\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;json-path\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- 数据库依赖包 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Jpa依赖包，用于对数据库的操作 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--WEB支持--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--用于编译jsp--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.tomcat.embed\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tomcat-embed-jasper\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Spring-boot实现热部署支持包 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-devtools\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.github.biezhi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;wechat-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Mybatis支持 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mybatis.spring.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- 打包成war包时需要移除内置tomcat --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- servlet的api依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;javax.servlet-api\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- 文件上传依赖包 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-fileupload\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-fileupload\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.3.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-io\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-io\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- 在打包war时引入第三包jar包 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-war-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;webResources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;directory\u0026gt;src/main/webapp\u0026lt;/directory\u0026gt; \u0026lt;targetPath\u0026gt;WEB-INF/lib/\u0026lt;/targetPath\u0026gt; \u0026lt;includes\u0026gt; \u0026lt;include\u0026gt;**/*.jar\u0026lt;/include\u0026gt; \u0026lt;/includes\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/webResources\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt;  在項目的/resources下配置application.properties（主要包括数据库jdbc，热部署，MyBatis映射文件地夹设置等），如下：  ###### data config start ###### spring.jpa.properties.hibernate.hbm2ddl.auto=update spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL5InnoDBDialect spring.jpa.show-sql= true spring.datasource.hikari.connectionInitSql=SET NAMES utf8mb4 COLLATE utf8mb4_unicode_ci; ###### DB config end ###### spring.datasource.url=jdbc:mysql://127.0.0.1:3306/lost_back?useSSL=false spring.datasource.username=root spring.datasource.password=19970121 spring.datasource.driver-class-name=com.mysql.jdbc.Driver ###### Mapper config mybatis.mapper-locations=classpath:mapper/*.xml # Page default prefix directory server.servlet.jsp.init-parameters.development=true spring.mvc.view.prefix=/WEB-INF/jsp/ # Response page default suffix spring.mvc.view.suffix=.jsp # Configuring hot deployment spring.devtools.restart.enabled: true # Set the restart directory do not restart the folder WEB-INF/** spring.devtools.restart.exclude: WEB-INF/** spring.thymeleaf.cache=false ","id":79,"section":"posts","summary":"创建流程 创建Maven项目，设置Maven架构类型为maven-archetype-quickstart： 创建项目后，修改其pom.xml文","tags":["","spring-boot"],"title":"SpringBoot创建及配置（一）","uri":"https://PI-KA-CHU.github.io/2018/12/springboot%E5%88%9B%E5%BB%BA%E5%8F%8A%E9%85%8D%E7%BD%AE/","year":"2018"},{"content":"一、部署流程   阿里云服务器购买（学生优惠）：https://promotion.aliyun.com/ntms/act/campus2018.html 购买成功后可以在阿里云服务器控制台上管理自己的服务器，设置自己的登陆账号和密码。  使用终端模拟器MobaXterm访问远程服务器，点击左上角的session，后点击SSH进行连接，连接地址为你的公网网址。  连接成功后在MoBaXterm上传jdk及tomcat，并配置环境变量，参考如下：\n    http://www.cnblogs.com/yijialong/p/9606265.html\n  https://www.cnblogs.com/yuanbo123/p/5819564.html  配置成功后，需要在阿里云控制台那里设置防火墙（入站规则） ，比如开放3306端口供Mysql数据库，开放8080供Tomcat使用（tomcat默认是8080端口），不开放tomcat端口的话访问到的是503错误   二、问题与解决  使用SpringBoot的jar包进行部署时发生了一堆无法描述的事情，最终改用war包进行打包部署，以下是遇到的一些坑：\n  坑爹的tomcat无法启动：jdk异常：   https://blog.csdn.net/kaikaitang/article/details/80888664  在项目根目录下进行maven的war打包（使用maven的war打包时需要使用Eclipse的JDK若使用的是JRE则需要修改过来）：   mvn clean package -Dmaven.test.skip=true（后面部分为忽略test文件）   配置完jdk和Tomcat并成功打包war包后，还需要配置数据库，我的是MySQL，否则即使war包成功部署了，也无法正常访问项目（404），具体操作可根据此：\n  Mysql配置及安装： http://www.cnblogs.com/yijialong/p/9606265.html\n  Mysql编码格式修改： https://blog.csdn.net/qq_28039297/article/details/76686022\n    一切准备就绪后，可以使用MobaXterm进行远程终端连接，通过该工具将war包上传到Tomcat的webapps目录下，运行tomcat就可以在公网进行访问了。（若无法正常访问，可查看tomcat日志文件寻找异常错误，在其logs文件夹中）\n  使用navicat将本地数据库导成.sql文件后导入服务器端数据库出现的坑爹问题，navicat在导出时默认不是65001 (UTF-8)，导入的时候要用Current Windows Codepage编码格式进行导入，否则会出现navicat显示正常，但是使用web访问时会出现乱码的问题\n  ","id":80,"section":"posts","summary":"一、部署流程 阿里云服务器购买（学生优惠）：https://promotion.aliyun.com/ntms/act/campus2018.","tags":["部署工具","spring-boot"],"title":"阿里云部署SpringBoot","uri":"https://PI-KA-CHU.github.io/2018/11/%E9%98%BF%E9%87%8C%E4%BA%91%E9%83%A8%E7%BD%B2springboot/","year":"2018"},{"content":"Junit单元测试在SSM中的使用已经有简单介绍：#4，基本的使用是一致的，但是在配置上有所不同：\n 依赖包：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.jayway.jsonpath\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;json-path\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 注释配置：（其中LostBackSysApplication为自己的启动项，即main函数中启动的那个类）\n@RunWith(SpringJUnit4ClassRunner.class) @SpringBootTest(classes = LostBackSysApplication.class) public class FormsDaoTest { @Autowired private FormsDao formsDao; @Before public void setUp() throws Exception { } @Test public void testInsertApplicationFrom() { ApplicationFormDto applicationFormDto = new ApplicationFormDto(); applicationFormDto.setId(UUID.randomUUID().toString()); applicationFormDto.setUserID(\u0026#34;ed8e29c5-4684-4fae-967b-271c71d217ac\u0026#34;); applicationFormDto.setPickedItemID(\u0026#34;6ad6152b-e7d5-42d4-8939-53b3ce79da8b\u0026#34;); applicationFormDto.setCreateTime(\u0026#34;2018-11-21\u0026#34;); applicationFormDto.setDescription(\u0026#34;这个是我的\u0026#34;); formsDao.insertApplicationFrom(applicationFormDto); } } ","id":81,"section":"posts","summary":"Junit单元测试在SSM中的使用已经有简单介绍：#4，基本的使用是一致的，但是在配置上有所不同： 依赖包： \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.jayway.jsonpath\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;json-path\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 注释配置：（其中","tags":["","spring-boot"],"title":"SpringBoot整合Junit4","uri":"https://PI-KA-CHU.github.io/2018/11/springboot%E6%95%B4%E5%90%88junit4/","year":"2018"},{"content":" 参考： https://blog.csdn.net/kenhins/article/details/51084815\n三大范式  为了建立冗余较小、结构合理的数据库，设计数据库时必须遵循一定的规则。在关系型数据库中这种规则就称为范式。范式是符合某一种设计要求的总结。要想设计一个结构合理的关系型数据库，必须满足一定的范式。    第一范式(确保每列保持原子性)： 第一范式是最基本的范式。如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式，如地址字段，可以拆分为省，市，详细地址三个字段，便于数据的获取及查询。\n  第二范式(确保表中的每列都和主键相关)： 第二范式是在第一范式的基础上，确保数据表上的每一列都与主键相关，而不能只是主键（联合主键）的某一部分相关（主要针对于联合主键的表）。\n  第三范式(确保每列都和主键列直接相关,而不是间接相关) 第三范式是在第二范式的基础上，需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。\n  ","id":82,"section":"posts","summary":"参考： https://blog.csdn.net/kenhins/article/details/51084815 三大范式 为了建立冗余较小、结构合理的数据库，设计数据库时必须遵循一定的规则。在关系型数据库中这种规则就称为范式。范式是符合某一种设计","tags":["mysql"],"title":"数据库建表规则—三大范式","uri":"https://PI-KA-CHU.github.io/2018/11/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BB%BA%E8%A1%A8%E8%A7%84%E5%88%99%E4%B8%89%E5%A4%A7%E8%8C%83%E5%BC%8F/","year":"2018"},{"content":"SVN的forbidden异常 问题：将SVN进行relocate时报了forbidden异常，检查后地址等各个都是正确的\n解决：（首先确认自己的账号密码存在并且正确）缓存导致的问题，需要清除缓存后checkout导入： ","id":83,"section":"posts","summary":"SVN的forbidden异常 问题：将SVN进行relocate时报了forbidden异常，检查后地址等各个都是正确的 解决：（首先确认自己","tags":["疑难杂症"],"title":"SVN的forbidden异常","uri":"https://PI-KA-CHU.github.io/2018/11/svn%E7%9A%84forbidden%E5%BC%82%E5%B8%B8/","year":"2018"},{"content":"一、写在前面 学习博客：\n https://blog.csdn.net/qiyuexuelang/article/details/8861300  需要导入的包：\n commons-fileupload-1.3.1.jar； commons-io-2.4.jar； mysql-connector-java-5.1.39-bin.jar；   二、实现流程 1. 图片添加界面：add.jsp \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=utf-8\u0026#34; pageEncoding=\u0026#34;utf-8\u0026#34;%\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;jsp:useBean id=\u0026#34;DBBean\u0026#34; scope=\u0026#34;page\u0026#34; class=\u0026#34;com.zeng.lab06.DBBean\u0026#34;/\u0026gt; \u0026lt;% DBBean.getConnect(); String createSQL = \u0026#34;CREATE TABLE IF NOT EXISTS lab06(id int PRIMARY KEY AUTO_INCREMENT,title varchar(255),author varchar(255),file varchar(255))\u0026#34;; DBBean.createTable(createSQL); %\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Insert title here\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.bootcss.com/bootstrap/4.0.0/css/bootstrap.min.css\u0026#34;\u0026gt; \u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; #upload { width: 30%; height: 400px; border-width: 1px; border-style: inset; margin-top: 8%; margin-left: 35%; background-color: white; margin-top: 8%; } #form{ margin-top: 28%; margin-left: 10%; width: 80%; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;upload\u0026#34;\u0026gt; \u0026lt;form id=\u0026#34;form\u0026#34; action=\u0026#34;../UploadServlet\u0026#34; method=\u0026#34;post\u0026#34; enctype=\u0026#34;multipart/form-data\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;input-group mb-3\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; class=\u0026#34;form-control\u0026#34; placeholder=\u0026#34;title\u0026#34; aria-label=\u0026#34;title\u0026#34; aria-describedby=\u0026#34;basic-addon2\u0026#34; name=\u0026#34;title\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;input-group-append\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;input-group-text\u0026#34; id=\u0026#34;basic-addon2\u0026#34;\u0026gt;标题\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;input-group mb-3\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; class=\u0026#34;form-control\u0026#34; placeholder=\u0026#34;name\u0026#34; aria-label=\u0026#34;name\u0026#34; aria-describedby=\u0026#34;basic-addon2\u0026#34; name=\u0026#34;author\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;input-group-append\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;input-group-text\u0026#34; id=\u0026#34;basic-addon2\u0026#34;\u0026gt;发布人\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;input-group\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;custom-file\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;file\u0026#34; class=\u0026#34;custom-file-input\u0026#34; id=\u0026#34;inputGroupFile04\u0026#34; name=\u0026#34;file\u0026#34;\u0026gt; \u0026lt;label class=\u0026#34;custom-file-label\u0026#34; for=\u0026#34;inputGroupFile04\u0026#34;\u0026gt;choose image\u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;input-group-append\u0026#34;\u0026gt; \u0026lt;button class=\u0026#34;btn btn-outline-secondary\u0026#34; type=\u0026#34;submit\u0026#34;\u0026gt;Upload\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.bootcss.com/jquery/3.2.1/jquery.slim.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.bootcss.com/popper.js/1.12.9/umd/popper.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.bootcss.com/bootstrap/4.0.0/js/bootstrap.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  2. 图片上传的servlet：UploadServlet.java package com.zeng.lab06; import java.io.File; import java.io.IOException; import java.sql.ResultSet; import java.util.List; import javax.servlet.ServletContext; import javax.servlet.ServletException; import javax.servlet.annotation.WebServlet; import javax.servlet.http.Cookie; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import org.apache.commons.fileupload.FileItem; import org.apache.commons.fileupload.FileUploadException; import org.apache.commons.fileupload.disk.DiskFileItemFactory; import org.apache.commons.fileupload.servlet.ServletFileUpload; @WebServlet(\u0026#34;/UploadServlet\u0026#34;) public class UploadServlet extends HttpServlet { private static final long serialVersionUID = 1L; public UploadServlet() { super(); } protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { request.setCharacterEncoding(\u0026#34;utf-8\u0026#34;); response.setContentType(\u0026#34;text/html;charset=utf-8\u0026#34;); // 为解析类提供配置信息 DiskFileItemFactory factory = new DiskFileItemFactory(); // 创建解析类的实例 ServletFileUpload suf = new ServletFileUpload(factory); // 设置文件大小 suf.setFileSizeMax(1024 * 400); String title = \u0026#34;\u0026#34;; String author = \u0026#34;\u0026#34;; String imgUploadPath = \u0026#34;\u0026#34;; String fileName = \u0026#34;\u0026#34;; try { List\u0026lt;FileItem\u0026gt; items = suf.parseRequest(request); for (FileItem item : items) { // isFormField为true，表示这不是文件上传表单域 if (!item.isFormField()) { ServletContext sctx = getServletContext(); //返回包含给定虚拟路径的实际路径。 String path = sctx.getRealPath(\u0026#34;/upload\u0026#34;); fileName = item.getName(); fileName = fileName.substring(fileName.lastIndexOf(\u0026#34;\\\\\u0026#34;) + 1); imgUploadPath = path + \u0026#34;\\\\\u0026#34; + fileName; System.out.println(imgUploadPath + \u0026#34;=========\u0026#34;); // \u0026#34;\\\u0026#34;会被转义，需要输入\u0026#34;\\\\\\\\\u0026#34;用于表示\u0026#34;\\\u0026#34; imgUploadPath = imgUploadPath.replaceAll(\u0026#34;\\\\\\\\\u0026#34;, \u0026#34;/\u0026#34;); System.out.println(imgUploadPath + \u0026#34;=========\u0026#34;); File file = new File(imgUploadPath); if (!file.exists()) { // 创建文件所需的目录(用于保存上传的图片) file.getParentFile().mkdirs(); item.write(file); } } else { if (item.getFieldName().equals(\u0026#34;title\u0026#34;)) { title = item.getString(); } else if (item.getFieldName().equals(\u0026#34;author\u0026#34;)) { author = item.getString(); } } } //保存图片的相对路径 String rootPath = \u0026#34;/upload/\u0026#34; + fileName; String sql = \u0026#34;insert into lab06(title,author,file) \u0026#34; + \u0026#34;values(\u0026#39;\u0026#34; + title + \u0026#34;\u0026#39;,\u0026#39;\u0026#34; + author + \u0026#34;\u0026#39;,\u0026#39;\u0026#34; + rootPath + \u0026#34;\u0026#39;)\u0026#34;; DBBean.executeUpdate(sql); response.sendRedirect(\u0026#34;lab06_jsp/list.jsp\u0026#34;); } catch (FileUploadException e) { e.printStackTrace(); } catch (Exception e) { e.printStackTrace(); } } protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doGet(request, response); } }  3. 图片分页显示界面：list.jsp \u0026lt;%@page import=\u0026#34;java.sql.ResultSet\u0026#34;%\u0026gt; \u0026lt;%@page import=\u0026#34;com.zeng.lab06.DBBean\u0026#34;%\u0026gt; \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=utf-8\u0026#34; pageEncoding=\u0026#34;utf-8\u0026#34;%\u0026gt; \u0026lt;% String path = request.getContextPath(); String basePath = request.getScheme() + \u0026#34;://\u0026#34; + request.getServerName() + \u0026#34;:\u0026#34; + request.getServerPort() + path + \u0026#34;/\u0026#34;; %\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;jsp:useBean id=\u0026#34;DBBean\u0026#34; scope=\u0026#34;page\u0026#34; class=\u0026#34;com.zeng.lab06.DBBean\u0026#34; /\u0026gt; \u0026lt;% DBBean.getConnect(); String sqlTotal = \u0026#34;select count(*) from lab06\u0026#34;; ResultSet rs1 = DBBean.executeQuery(sqlTotal); int total = 0; while(rs1.next()){ //总的上传数 total = Integer.parseInt(rs1.getString(1)); } //起始下标 int index; String indexTemp = request.getParameter(\u0026#34;index\u0026#34;); if(indexTemp == null || indexTemp.equals(\u0026#34;\u0026#34;)){ index = 0; }else{ index = Integer.parseInt(indexTemp); } //每五个分一页 int number = 3; %\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.bootcss.com/bootstrap/4.0.0/css/bootstrap.min.css\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Insert title here\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; #main { width: 60%; margin-top: 5%; margin-left: 20%; border: 1px inset; } #bottom { margin-left: 38%; } .table { text-align: center; vertical-align: center; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;table class=\u0026#34;table\u0026#34;\u0026gt; \u0026lt;thead class=\u0026#34;thead-light\u0026#34;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th scope=\u0026#34;col\u0026#34;\u0026gt;#\u0026lt;/th\u0026gt; \u0026lt;th scope=\u0026#34;col\u0026#34;\u0026gt;图片\u0026lt;/th\u0026gt; \u0026lt;th scope=\u0026#34;col\u0026#34;\u0026gt;标题\u0026lt;/th\u0026gt; \u0026lt;th scope=\u0026#34;col\u0026#34;\u0026gt;发布人\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;查看详情\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;% if(index \u0026lt; 0){ index = 0; } String sql = \u0026#34;select * from lab06 limit \u0026#34; + index + \u0026#34;,\u0026#34; + number; ResultSet rs = DBBean.executeQuery(sql); String author = null; String title = null; String img = null; while (rs.next()) { img = basePath + rs.getString(4); title = rs.getString(2); author = rs.getString(3); %\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th scope=\u0026#34;row\u0026#34;\u0026gt;\u0026lt;%=rs.getString(1)%\u0026gt;\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;\u0026lt;%=img%\u0026gt;\u0026#34;\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;%=title%\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;%=author%\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;a type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-success\u0026#34; href=\u0026#34;detail.jsp? img=\u0026lt;%=img%\u0026gt;\u0026amp;title=\u0026lt;%=title%\u0026gt;\u0026amp;author=\u0026lt;%=author%\u0026gt;\u0026#34;\u0026gt;detail\u0026lt;/a\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;% } if(index + number \u0026gt; total){ index = index - number; } //要分的页数 int pageT = (total % number==0)? (total/number) : (total/number + 1); %\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;bottom\u0026#34;\u0026gt; \u0026lt;nav aria-label=\u0026#34;Page navigation example\u0026#34;\u0026gt; \u0026lt;ul class=\u0026#34;pagination\u0026#34;\u0026gt; \u0026lt;li class=\u0026#34;page-item\u0026#34;\u0026gt; \u0026lt;a class=\u0026#34;page-link\u0026#34; href=\u0026#34;list.jsp?pageT=\u0026lt;%=pageT%\u0026gt;\u0026amp;total=\u0026lt;%=total%\u0026gt;\u0026amp;index=\u0026lt;%=index-number%\u0026gt;\u0026#34; aria-label=\u0026#34;Previous\u0026#34;\u0026gt; \u0026lt;span aria-hidden=\u0026#34;true\u0026#34;\u0026gt;\u0026amp;laquo;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;sr-only\u0026#34;\u0026gt;Previous\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;% for(int i = 1;i \u0026lt;= pageT;i ++){ %\u0026gt; \u0026lt;li class=\u0026#34;page-item\u0026#34;\u0026gt;\u0026lt;a class=\u0026#34;page-link\u0026#34; href=\u0026#34;list.jsp?pageT=\u0026lt;%=i%\u0026gt;\u0026amp;total=\u0026lt;%=total%\u0026gt;\u0026amp;index=\u0026lt;%=number*(i-1)%\u0026gt;\u0026#34;\u0026gt;\u0026lt;%=i %\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;li class=\u0026#34;page-item\u0026#34;\u0026gt; \u0026lt;a class=\u0026#34;page-link\u0026#34; href=\u0026#34;list.jsp?pageT=\u0026lt;%=pageT%\u0026gt;\u0026amp;total=\u0026lt;%=total%\u0026gt;\u0026amp;index=\u0026lt;%=index+number%\u0026gt;\u0026#34; aria-label=\u0026#34;Next\u0026#34;\u0026gt; \u0026lt;span aria-hidden=\u0026#34;true\u0026#34;\u0026gt;\u0026amp;raquo;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;sr-only\u0026#34;\u0026gt;Next\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/nav\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.bootcss.com/jquery/3.2.1/jquery.slim.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.bootcss.com/popper.js/1.12.9/umd/popper.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.bootcss.com/bootstrap/4.0.0/js/bootstrap.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  4. 详情图片信息显示页面：detail.jsp \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=utf-8\u0026#34; pageEncoding=\u0026#34;utf-8\u0026#34;%\u0026gt; \u0026lt;% String path = request.getContextPath(); String basePath = request.getScheme() + \u0026#34;://\u0026#34; + request.getServerName() + \u0026#34;:\u0026#34; + request.getServerPort() + path + \u0026#34;/\u0026#34;; %\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Insert title here\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.bootcss.com/bootstrap/4.0.0/css/bootstrap.min.css\u0026#34;\u0026gt; \u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; .card{ margin-top: 5%; margin-left: 30%; } .black{ margin-left: 85%; } } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div\u0026gt; \u0026lt;div class=\u0026#34;card\u0026#34; style=\u0026#34;width: 35rem;\u0026#34;\u0026gt; \u0026lt;img class=\u0026#34;card-img-top\u0026#34; src=\u0026#34;\u0026lt;%=request.getParameter(\u0026#34;img\u0026#34;) %\u0026gt;\u0026#34; alt=\u0026#34;Card image cap\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;card-body\u0026#34;\u0026gt; \u0026lt;h5 class=\u0026#34;card-title\u0026#34;\u0026gt;标题：\u0026lt;%=request.getParameter(\u0026#34;title\u0026#34;) %\u0026gt;\u0026lt;/h5\u0026gt; \u0026lt;p class=\u0026#34;card-text\u0026#34;\u0026gt;作者：\u0026lt;%=request.getParameter(\u0026#34;author\u0026#34;) %\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;div class=\u0026#34;black\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;list.jsp\u0026#34; class=\u0026#34;btn btn-primary\u0026#34;\u0026gt;返回\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.bootcss.com/jquery/3.2.1/jquery.slim.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.bootcss.com/popper.js/1.12.9/umd/popper.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.bootcss.com/bootstrap/4.0.0/js/bootstrap.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ","id":84,"section":"posts","summary":"一、写在前面 学习博客： https://blog.csdn.net/qiyuexuelang/article/details/8861300 需要导入的包： commons-fileupload-1.3.1.jar； commons-io-2.4.jar； mys","tags":[" "],"title":"JSP + Bootstrap +  JAVABean + Servlet + Mysql实现图片上传并分页显示","uri":"https://PI-KA-CHU.github.io/2018/11/jsp--bootstrap--javabean--servlet--mysql%E5%AE%9E%E7%8E%B0%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0%E5%B9%B6%E5%88%86%E9%A1%B5%E6%98%BE%E7%A4%BA/","year":"2018"},{"content":"File文件转换为MultipartFile 文件： （可用于测试excel文件导入的接口）\nFile file1 = new File(\u0026#34;C:\\\\Users\\\\pc\\\\Desktop\\\\裁判员信息表.xls\u0026#34;); FileInputStream fis = new FileInputStream(file1); MultipartFile multipartFile = new MockMultipartFile(\u0026#34;file\u0026#34;, file1.getName(), \u0026#34;text/plain\u0026#34;,IOUtils.toByteArray(fis)); ","id":85,"section":"posts","summary":"File文件转换为MultipartFile 文件： （可用于测试excel文件导入的接口） File file1 = new File(\u0026#34;C:\\\\Users\\\\p","tags":[""],"title":"JAVA文件操作相关学习笔记","uri":"https://PI-KA-CHU.github.io/2018/10/java%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","year":"2018"},{"content":"一、Insert语句 insert into和replace into以及insert ignore用法区别   insert into表示插入数据，数据库会检查主键，如果出现重复会报错；\n  replace into表示插入替换数据，需求表中有PrimaryKey，或者unique索引，如果数据库已经存在数据，则用新数据替换，如果没有数据效果则和insert into一样；\n  insert ignore表示，如果中已经存在相同的记录，则忽略当前新数据；\n   二、日期转化为星期的SQL语句 SELECT CASE dayofweek(‘2018-12-17’) WHEN 1 THEN \u0026#39;星期日\u0026#39; WHEN 2 THEN \u0026#39;星期一\u0026#39; WHEN 3 THEN \u0026#39;星期二\u0026#39; WHEN 4 THEN \u0026#39;星期三\u0026#39; WHEN 5 THEN \u0026#39;星期四\u0026#39; WHEN 6 THEN \u0026#39;星期五\u0026#39; WHEN 7 THEN \u0026#39;星期六\u0026#39; END FROM DUAL  三、自定义排序（如中文排序）  根据自定义的排序进行数据排序获取  SELECT * FROM classes WHERE gradeId = \u0026#34;44040012\u0026#34; ORDER BY instr( \u0026#34;一班,二班,三班,四班,五班,六班,七班,八班,九班,十班,十一班,十二班\u0026#34;, className ) ","id":86,"section":"posts","summary":"一、Insert语句 insert into和replace into以及insert ignore用法区别 insert into表示插入数据，数据库会检查主键，如果出现","tags":["mysql"],"title":"MySQL语句学习","uri":"https://PI-KA-CHU.github.io/2018/10/mysql%E8%AF%AD%E5%8F%A5%E5%AD%A6%E4%B9%A0/","year":"2018"},{"content":"百度云盘资源搜索工具：  http://tansuo233.com/  百度资源链接获取：   下载谷歌扩展程序：BaiduPan Explorer（需要先翻墙）   打开百度云资源分享页面（如视频，压缩文件等），复制下载链接：   将下载链接使用其他工具下载（此处用proxyee-down）：  下载地址：https://github.com/proxyee-down-org/proxyee-down/releases  ","id":87,"section":"posts","summary":"百度云盘资源搜索工具： http://tansuo233.com/ 百度资源链接获取： 下载谷歌扩展程序：BaiduPan Explorer（需要先翻墙） 打开百度云资源分享页面（如视频，压","tags":["实用工具"],"title":"百度云盘资源搜索及高速下载","uri":"https://PI-KA-CHU.github.io/2018/10/%E7%99%BE%E5%BA%A6%E4%BA%91%E7%9B%98%E8%B5%84%E6%BA%90%E6%90%9C%E7%B4%A2%E5%8F%8A%E9%AB%98%E9%80%9F%E4%B8%8B%E8%BD%BD/","year":"2018"},{"content":"关键代码： 【配置好spring及JUnit的class（创建一次即可，后续的可以直接继承）】\n@RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration({ \u0026#34;classpath:spring/spring-dao.xml\u0026#34;, \u0026#34;classpath:spring/spring-service.xml\u0026#34; }) public class BaseTest { }  创建步骤：  右键点击想测试的类（或包）,选择New\u0026ndash;\u0026gt;Oher：  找到JUnit Test Case，创建：  选择JUnit 4 Test，输入名称(Name)，命名规则一般建议采用：类名+Test  勾选自己要测试的方法：  生成代码，在要测试的方法中写入代码  ","id":88,"section":"posts","summary":"关键代码： 【配置好spring及JUnit的class（创建一次即可，后续的可以直接继承）】 @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration({ \u0026#34;classpath:spring/spring-dao.xml\u0026#34;, \u0026#34;classpath:spring/spring-service.xml\u0026#34; }) public class BaseTest { } 创建步骤： 右键点击想测试的类","tags":[""],"title":"JUnit单元测试的使用","uri":"https://PI-KA-CHU.github.io/2018/10/junit%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E7%9A%84%E4%BD%BF%E7%94%A8/","year":"2018"},{"content":" 1.使用Cookie记录页面浏览次数（通过按钮刷新的方式）  \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=utf-8\u0026#34; pageEncoding=\u0026#34;utf-8\u0026#34;%\u0026gt; \u0026lt;% /** 功能：使用cookie记录页面的浏览次数： 步骤： 1.遍历所有cookie： 2.count用于记录浏览的次数，如果存在就为此次访问count + 1， 没有的话count默认为1，表示第一次访问。 3.计算完count值后创建cookie并把count的值赋值进去。 （cookie名相同的会自动替代原有的，不过各个属性设置要相同） **/ int count = 1; for(int i = 0;i \u0026lt; request.getCookies().length;i ++){ Cookie cookie = request.getCookies()[i]; if(cookie.getName().equals(\u0026#34;count\u0026#34;)){ count = Integer.parseInt(cookie.getValue()); count ++; } } Cookie visitCookie = new Cookie(\u0026#34;count\u0026#34;,Integer.toString(count)); response.addCookie(visitCookie); %\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;使用cookie记录每次浏览\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;阅览次数：\u0026lt;/label\u0026gt;\u0026lt;%= count %\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;!-- 如果用response.send()去跳转，需要将cookie设置为setPath(\u0026#34;/\u0026#34;)，表示cookie适用于整个项目 --\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; onclick=\u0026#34;location=\u0026#39;\u0026lt;%= request.getRequestURI() %\u0026gt;\u0026#39;\u0026#34; value=\u0026#34;刷新\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 附：\n http://www.51gjie.com/javaweb/860.html https://segmentfault.com/a/1190000004556040   2.使用session统计现在访问的人数 a.创建一个含有静态（static）值的类：计数 `public static int onlinePeople = 0;`  b.创建一个实现HttpSessionAttributeListener接口的类：监听 public class OnlineCountListener implements HttpSessionAttributeListener { //用于监测当有session的值被添加的使用 @Override public void attributeAdded(HttpSessionBindingEvent event) { System.out.println(\u0026#34;正常监测到\u0026#34; + event.getName()); if(event.getName().equals(\u0026#34;userIP\u0026#34;)) { System.out.println(\u0026#34;session值有添加IP\u0026#34;); OnlineCountInfo.addOnlinePeople(); } } //用于监测session的值被删除（或者过期）的时候 @Override public void attributeRemoved(HttpSessionBindingEvent event) { if(event.getName().equals(\u0026#34;userIP\u0026#34;)) { System.out.println(\u0026#34;session值有删除IP\u0026#34;); OnlineCountInfo.reduceOnlinePeople(); } } //用于监测session值被替换的时候 @Override public void attributeReplaced(HttpSessionBindingEvent event) { } } c.将创建的监听的类注册到web.xml里面：注册 \u0026lt;listener\u0026gt; \u0026lt;listener-class\u0026gt;com.bnuz.test.util.OnlineCountListener\u0026lt;/listener-class\u0026gt; \u0026lt;/listener\u0026gt; d.在JSP中使用session进行统计在线人数：统计 步骤：\n 在login.jsp使用loginServlet进行模拟登陆 在servlet中进行操作 跳转到successLogin.jsp页面  LoginServlet.java：\nprotected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { String username = request.getParameter(\u0026#34;username\u0026#34;); String password = request.getParameter(\u0026#34;password\u0026#34;); System.out.println(getIpAddr(request)); //模拟登陆 if(username.equals(\u0026#34;\u0026#34;) || username.trim().equals(\u0026#34;\u0026#34;)) { response.sendRedirect(\u0026#34;login.jsp\u0026#34;); }else { HttpSession session = request.getSession(); //设置session的有效时间为10秒（刷新则会重新计数），超过10秒无交互视为下线 session.setMaxInactiveInterval(10); if(session.getAttribute(\u0026#34;userIP\u0026#34;) == null) { //跳过代理，获取客户端IP String IP = getIpAddr(request); session.setAttribute(\u0026#34;userIP\u0026#34;, IP); } response.sendRedirect(\u0026#34;successLogin.jsp\u0026#34;); } } //来源：http://blog.51cto.com/drizzlewalk/479934 //越过多级代理,获取客户端真实IP public String getIpAddr(HttpServletRequest request) { String ip = request.getHeader(\u0026#34;x-forwarded-for\u0026#34;); if(ip == null || ip.length() == 0 || \u0026#34;unknown\u0026#34;.equalsIgnoreCase(ip)) { ip = request.getHeader(\u0026#34;Proxy-Client-IP\u0026#34;); } if(ip == null || ip.length() == 0 || \u0026#34;unknown\u0026#34;.equalsIgnoreCase(ip)) { ip = request.getHeader(\u0026#34;WL-Proxy-Client-IP\u0026#34;); } if(ip == null || ip.length() == 0 || \u0026#34;unknown\u0026#34;.equalsIgnoreCase(ip)) { ip = request.getRemoteAddr(); } return ip; } successLogin.jsp：\n\u0026lt;body\u0026gt; \u0026lt;% //每11秒刷新一次 response.setIntHeader(\u0026#34;Refresh\u0026#34;, 11); Date date = new Date(); DateFormat format = new SimpleDateFormat(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;); HttpSession session1 = request.getSession(); //在线人数 String times = \u0026#34;登陆超时，请重新登陆\u0026#34;; try{ if(session1.getAttribute(\u0026#34;userIP\u0026#34;) != null){ times = OnlineCountInfo.getOnlinePeople() + \u0026#34;\u0026#34;; } }catch(NullPointerException e){ System.out.println(\u0026#34;没有值\u0026#34;); } %\u0026gt; \u0026lt;h2\u0026gt;\u0026lt;%=format.format(date) %\u0026gt;\u0026lt;/h2\u0026gt; \u0026lt;h2\u0026gt;当前在线人数：\u0026lt;%=times %\u0026gt;\u0026lt;/h2\u0026gt;\u0026amp;nbsp; \u0026lt;h2\u0026gt;登陆成功\u0026lt;/h2\u0026gt; \u0026lt;/body\u0026gt; 3.使用Application存储不同账号（浏览器）登陆对象  Application存在于整个服务器阶段，除非重开服务器（Tomcat），否则不会刷新\n//获取application ServletContext application = this.getServletContext(); //为application赋值 application.setAttribute(name, value); //移除application application.removeAttribute(name); ","id":89,"section":"posts","summary":"1.使用Cookie记录页面浏览次数（通过按钮刷新的方式） \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=utf-8\u0026#34; pageEncoding=\u0026#34;utf-8\u0026#34;%\u0026gt; \u0026lt;% /** 功能：使用cookie记录页面的浏览次数： 步骤： 1.遍历所有coo","tags":[""],"title":"Cookie,Session及Application的使用","uri":"https://PI-KA-CHU.github.io/2018/10/cookiesession%E5%8F%8Aapplication%E7%9A%84%E4%BD%BF%E7%94%A8/","year":"2018"},{"content":"// 创建工作簿 HSSFWorkbook wb = new HSSFWorkbook(); // 创建工作表 HSSFSheet sheet = wb.createSheet(); // 设置工作表的索引行的宽度，15 * 256表示15个字符宽度 sheet.setColumnWidth(0, 15 * 256); sheet.setColumnWidth(1, 15 * 256); sheet.setColumnWidth(2, 15 * 256); sheet.setColumnWidth(3, 15 * 256); // 设置表格格式 HSSFCellStyle style = wb.createCellStyle(); // 设置字体 HSSFFont font = wb.createFont(); font.setFontName(\u0026#34;宋体\u0026#34;); font.setFontHeightInPoints((short) 12); // 设置单元格类型 style.setAlignment(HSSFCellStyle.ALIGN_CENTER); style.setVerticalAlignment(HSSFCellStyle.VERTICAL_CENTER); style.setFont(font); style.setWrapText(true); // 合并起止行0，0，起止列0，3单元格 sheet.addMergedRegion(new CellRangeAddress(0, 0, 0, 3)); // 创建行 HSSFRow row0 = sheet.createRow(0); row0.setHeight((short) 600); // 在行中创建单元格，并设置内容及格式 HSSFCell cell0 = row0.createCell(0); cell0.setCellValue(\u0026#34;七彩评价社团信息\u0026#34;); cell0.setCellStyle(style); // 通过获取的list数据在表格中进行填充数据 for (LeagueMemberDto stu : list) { HSSFRow rowX = sheet.createRow(num++); cell = rowX.createCell(0); cell.setCellValue(stu.getName()); cell.setCellStyle(style); } /** 若要为创建的工作簿进行下载，可以先下载到服务器（也可以不用），再由客户端下载 **/ // 文件下载的地址（服务器） String upLoadPath = request.getSession().getServletContext().getRealPath(\u0026#34;/\u0026#34;) + \u0026#34;/download/athleteFile/\u0026#34;; String filePath = upLoadPath + fileName; // 创建下载的文件夹 File dirFile = new File(upLoadPath); if (!dirFile.exists() || !dirFile.isDirectory()) { dirFile.mkdirs(); } // (在服务器) 生成文件 workbook.write(new FileOutputStream(filePath)); // 下载文件 FileUtil.download(filePath, fileName + \u0026#34;.xls\u0026#34;, request, response, false); EXCEL文件下载： excel文件下载时可以直接获取response的getOutputStream()进行文件流的输出，再设置response的相关头信息即可实现浏览器下载，代码如下：\nString fileName = \u0026#34;人员信息.xls\u0026#34;; // 更换文件名的编码格式，否则在浏览器无法正常显示 fileName = response.encodeURL(new String(fileName.getBytes(\u0026#34;utf-8\u0026#34;), \u0026#34;iso8859-1\u0026#34;)); response.reset(); // 清空输出流 response.setContentType(\u0026#34;application/vnd.ms-excel;charset=utf-8\\\u0026#34;\u0026#34;); response.setHeader(\u0026#34;Content-Disposition\u0026#34;, \u0026#34;attachment;filename=\u0026#34; + fileName); outputStream = response.getOutputStream(); workbook.write(outputStream); EXCEL文件导入 JAR包支持：\nERROR提醒：\n  前端在使用form进行input文件的上传时，需要为相应input设置name属性，否则无法正常上传\n  将input标签加上multiple属性表示可上传多个文件\n  小数字符串转化为整型：转换为Int类型String num =\u0026ldquo;1.00\u0026rdquo;; int abc =Double.valueOf(num).intValue();  前端： 使用jquery实现同一个form中，不同按钮使用不用servlet进行处理，代码如下：\n\u0026lt;form action=\u0026#34;../ExcelImportServlet\u0026#34; method=\u0026#34;post\u0026#34; enctype=\u0026#34;multipart/form-data\u0026#34; id=\u0026#34;handle\u0026#34; name=\u0026#34;handle\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;input-group\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;custom-file\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;file\u0026#34; class=\u0026#34;custom-file-input\u0026#34; id=\u0026#34;inputGroupFile04\u0026#34; name=\u0026#34;file\u0026#34; multiple\u0026gt; \u0026lt;label class=\u0026#34;custom-file-label\u0026#34; for=\u0026#34;inputGroupFile04\u0026#34;\u0026gt;Choose file\u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;input-group-append\u0026#34;\u0026gt; \u0026lt;button class=\u0026#34;btn btn-outline-secondary\u0026#34; onclick=\u0026#34;$.handler.uploadExcel()\u0026#34;\u0026gt;upload\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;input-group-append\u0026#34;\u0026gt; \u0026lt;button class=\u0026#34;btn btn-outline-secondary\u0026#34; onclick=\u0026#34;$.handler.download()\u0026#34;\u0026gt;download\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; $(function() { $.handler = { download: function() { $(\u0026#34;#handle\u0026#34;).attr(\u0026#34;action\u0026#34;,\u0026#34;../ExcelDownloadServlet\u0026#34;); }, uploadExcel: function() { $(\u0026#34;#handle\u0026#34;).attr({ action : \u0026#34;../ExcelImportServlet\u0026#34;, method : \u0026#34;post\u0026#34;, enctype: \u0026#34;multipart/form-data\u0026#34; }); } } }) \u0026lt;/script\u0026gt; 调用：\u0026lt;button class=\u0026#34;btn btn-outline-secondary\u0026#34; onclick=\u0026#34;$.handler.uploadExcel()\u0026#34;\u0026gt;upload\u0026lt;/button\u0026gt; 后端： excel文件导入时前端可以使用form表单上传文件（将input标签加上multiple属性表示可上传多个文件），同时需要将form表单属性设置为： enctype=\u0026ldquo;multipart/form-data\u0026rdquo;，上传后使用Apache文件上传组件处理文件上传，代码如下：\npublic static int importExcel(HttpServletRequest request) { // 使用Apache文件上传组件处理文件上传步骤： // 为解析类提供配置信息 DiskFileItemFactory factory = new DiskFileItemFactory(); // 创建解析类的实例 ServletFileUpload sfu = new ServletFileUpload(factory); // 设置单个上传的文件大小 sfu.setFileSizeMax(1024 * 1024 * 5); //设置总上传文件的文件大小 sfu.setSizeMax(1024 * 1024 * 50); try { List\u0026lt;UserDto\u0026gt; userLists = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;FileItem\u0026gt; fileItems = sfu.parseRequest(request); System.out.println(\u0026#34;FileItem的数目为：\u0026#34; + fileItems.size()); for (FileItem item : fileItems) { if (item.isFormField()) { } else { String fileName = item.getName(); String fileType = fileName.substring(fileName.lastIndexOf(\u0026#34;.\u0026#34;)); System.out.println(\u0026#34;文件名后缀为：\u0026#34; + fileName); if (fileType.equals(\u0026#34;.xls\u0026#34;) || fileType.equals(\u0026#34;.xlsx\u0026#34;)) { InputStream is = item.getInputStream(); HSSFWorkbook workbook = new HSSFWorkbook(is); HSSFSheet sheet; HSSFRow row; System.out.println(\u0026#34;工作簿的表数为：\u0026#34; + workbook.getNumberOfSheets()); // 遍历工作表 for (int i = 0; i \u0026lt; workbook.getNumberOfSheets(); i++) { sheet = workbook.getSheetAt(i); System.out.println(\u0026#34;工作表的行数为：\u0026#34; + sheet.getLastRowNum()); // 遍历行 for (int j = sheet.getFirstRowNum() + 1; j \u0026lt;= sheet.getLastRowNum(); j++) { UserDto user = new UserDto(); row = sheet.getRow(j); user.setId(Double.valueOf(row.getCell(0).toString()).intValue()); user.setName(row.getCell(1).toString().trim()); user.setSex(row.getCell(2).toString().trim()); user.setEmail(row.getCell(3).toString().trim()); user.setTelephone(row.getCell(4).toString().trim()); userLists.add(user); } // 将读取的数据导入数据库 DBBean.insertUserExcelToDB(userLists); } } else { return -1; } } } } catch (FileUploadException e) { logger.debug(e.getMessage(), e); return 0; } catch (IOException e) { logger.debug(e.getMessage(), e); return 0; } return 1; } ","id":90,"section":"posts","summary":"// 创建工作簿 HSSFWorkbook wb = new HSSFWorkbook(); // 创建工作表 HSSFSheet sheet = wb.createSheet(); // 设置工作表的索引行的宽度，15 * 256表示15个字符宽度 sheet.setColumnWidth(0, 15 * 256); sheet.setColumnWidth(1, 15 * 256); sheet.setColumnWidth(2, 15 * 256); sheet.setColumnWidth(3, 15 * 256); // 设置表","tags":[""],"title":"JAVA Excel表格学习笔记","uri":"https://PI-KA-CHU.github.io/2018/09/java-excel%E8%A1%A8%E6%A0%BC%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","year":"2018"},{"content":"暗网简介：  暗网（英语：Dark web）是存在于黑暗网络上的万维网内容，只能_用特殊软件、特殊授权、或对计算机做特殊设置_才能访问，使用一般的浏览器和搜索引擎找不到暗网的内容。暗网的服务器地址和数据传输通常是匿名、匿踪的。与此相对，一般常用的互联网由于可追踪其真实地理位置和通信进行人的身份被称为“明网”（英语：Clearnet）。  使用步骤： 下载tor浏览器： http://www.theonionrouter.com/projects/torbrowser.html.en 如果打不开的话可以在github上下载： https://github.com/TheTorProject/gettorbrowser/releases\n安装成功后点击配置： 正常情况是获取内置网桥进行连接，选择内置网桥中的meek-azure，或者从torproject.org获取网桥\n网络提速： 经本人测试连接成功后是可以正常使用tor浏览器的，但因为网桥连接不稳定，通常打开页面会需要很久的时间，经过网上查找发现可以用代理服务器的方法使用tor，（若不能正常启动，需设置系统代理，代理脚本不是固定的）使用国外免费代理服务器，按上图设置（选择网桥的方式则勾选网桥，选择代理则勾选代理）后可以有很大的提速。\n系统代理设置：\n成功页面：\n附免费国外代理服务：\n http://www.data5u.com/free/gwgn/index.shtml http://free-proxy.cz/zh/proxylist/country/US/all/ping/all   此笔记仅用于学习记录，请勿用于不法用途\n ","id":91,"section":"posts","summary":"暗网简介： 暗网（英语：Dark web）是存在于黑暗网络上的万维网内容，只能_用特殊软件、特殊授权、或对计算机做特殊设置_才能访问，使用一般的","tags":["实用工具"],"title":"暗网学习-初入暗网（一）","uri":"https://PI-KA-CHU.github.io/2018/09/%E6%9A%97%E7%BD%91%E5%AD%A6%E4%B9%A0-%E5%88%9D%E5%85%A5%E6%9A%97%E7%BD%91%E4%B8%80/","year":"2018"}],"tags":[{"title":"docker","uri":"https://PI-KA-CHU.github.io/tags/docker/"},{"title":"log","uri":"https://PI-KA-CHU.github.io/tags/log/"},{"title":"mysql","uri":"https://PI-KA-CHU.github.io/tags/mysql/"},{"title":"redis","uri":"https://PI-KA-CHU.github.io/tags/redis/"},{"title":"spring","uri":"https://PI-KA-CHU.github.io/tags/spring/"},{"title":"spring-boot","uri":"https://PI-KA-CHU.github.io/tags/spring-boot/"},{"title":"多线程","uri":"https://PI-KA-CHU.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"title":"大数据","uri":"https://PI-KA-CHU.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"title":"安全","uri":"https://PI-KA-CHU.github.io/tags/%E5%AE%89%E5%85%A8/"},{"title":"实用工具","uri":"https://PI-KA-CHU.github.io/tags/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/"},{"title":"开发工具","uri":"https://PI-KA-CHU.github.io/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"title":"操作系统","uri":"https://PI-KA-CHU.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"title":"疑难杂症","uri":"https://PI-KA-CHU.github.io/tags/%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87/"},{"title":"部署工具","uri":"https://PI-KA-CHU.github.io/tags/%E9%83%A8%E7%BD%B2%E5%B7%A5%E5%85%B7/"}]}